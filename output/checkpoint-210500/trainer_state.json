{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.997294603445821,
  "eval_steps": 500,
  "global_step": 210500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0007119464616260857,
      "grad_norm": 0.4662109315395355,
      "learning_rate": 0.0001,
      "loss": 2.0889,
      "step": 50
    },
    {
      "epoch": 0.0014238929232521715,
      "grad_norm": 2.0783979892730713,
      "learning_rate": 0.0002,
      "loss": 1.3916,
      "step": 100
    },
    {
      "epoch": 0.0021358393848782574,
      "grad_norm": 0.810705840587616,
      "learning_rate": 0.0001999525143644048,
      "loss": 0.9003,
      "step": 150
    },
    {
      "epoch": 0.002847785846504343,
      "grad_norm": 1.4010056257247925,
      "learning_rate": 0.00019990502872880954,
      "loss": 0.8804,
      "step": 200
    },
    {
      "epoch": 0.0035597323081304286,
      "grad_norm": 1.4423211812973022,
      "learning_rate": 0.00019985754309321432,
      "loss": 0.8896,
      "step": 250
    },
    {
      "epoch": 0.004271678769756515,
      "grad_norm": 1.118407130241394,
      "learning_rate": 0.00019981005745761907,
      "loss": 0.7696,
      "step": 300
    },
    {
      "epoch": 0.0049836252313826,
      "grad_norm": 0.6415151953697205,
      "learning_rate": 0.00019976257182202384,
      "loss": 0.82,
      "step": 350
    },
    {
      "epoch": 0.005695571693008686,
      "grad_norm": 0.8087588548660278,
      "learning_rate": 0.00019971508618642862,
      "loss": 0.8228,
      "step": 400
    },
    {
      "epoch": 0.006407518154634771,
      "grad_norm": 0.9505290389060974,
      "learning_rate": 0.00019966760055083337,
      "loss": 0.7366,
      "step": 450
    },
    {
      "epoch": 0.007119464616260857,
      "grad_norm": 0.585881233215332,
      "learning_rate": 0.00019962011491523815,
      "loss": 0.8006,
      "step": 500
    },
    {
      "epoch": 0.007831411077886943,
      "grad_norm": 0.9155104160308838,
      "learning_rate": 0.0001995726292796429,
      "loss": 0.7617,
      "step": 550
    },
    {
      "epoch": 0.00854335753951303,
      "grad_norm": 1.1511977910995483,
      "learning_rate": 0.00019952514364404768,
      "loss": 0.7547,
      "step": 600
    },
    {
      "epoch": 0.009255304001139114,
      "grad_norm": 0.7824767231941223,
      "learning_rate": 0.00019947765800845245,
      "loss": 0.756,
      "step": 650
    },
    {
      "epoch": 0.0099672504627652,
      "grad_norm": 0.5592254996299744,
      "learning_rate": 0.0001994301723728572,
      "loss": 0.7809,
      "step": 700
    },
    {
      "epoch": 0.010679196924391286,
      "grad_norm": 1.5539606809616089,
      "learning_rate": 0.00019938268673726198,
      "loss": 0.7359,
      "step": 750
    },
    {
      "epoch": 0.011391143386017372,
      "grad_norm": 0.5708678364753723,
      "learning_rate": 0.00019933520110166673,
      "loss": 0.696,
      "step": 800
    },
    {
      "epoch": 0.012103089847643458,
      "grad_norm": 0.6240519881248474,
      "learning_rate": 0.0001992877154660715,
      "loss": 0.7733,
      "step": 850
    },
    {
      "epoch": 0.012815036309269542,
      "grad_norm": 1.4090445041656494,
      "learning_rate": 0.0001992402298304763,
      "loss": 0.7728,
      "step": 900
    },
    {
      "epoch": 0.013526982770895628,
      "grad_norm": 0.4395677447319031,
      "learning_rate": 0.00019919274419488107,
      "loss": 0.7226,
      "step": 950
    },
    {
      "epoch": 0.014238929232521715,
      "grad_norm": 0.5064687132835388,
      "learning_rate": 0.00019914525855928582,
      "loss": 0.7312,
      "step": 1000
    },
    {
      "epoch": 0.0149508756941478,
      "grad_norm": 0.49892619252204895,
      "learning_rate": 0.0001990977729236906,
      "loss": 0.7077,
      "step": 1050
    },
    {
      "epoch": 0.015662822155773887,
      "grad_norm": 1.17630136013031,
      "learning_rate": 0.00019905028728809537,
      "loss": 0.711,
      "step": 1100
    },
    {
      "epoch": 0.016374768617399973,
      "grad_norm": 0.8871251940727234,
      "learning_rate": 0.00019900280165250015,
      "loss": 0.7873,
      "step": 1150
    },
    {
      "epoch": 0.01708671507902606,
      "grad_norm": 0.5925207734107971,
      "learning_rate": 0.0001989553160169049,
      "loss": 0.7186,
      "step": 1200
    },
    {
      "epoch": 0.01779866154065214,
      "grad_norm": 0.6800721287727356,
      "learning_rate": 0.00019890783038130968,
      "loss": 0.7424,
      "step": 1250
    },
    {
      "epoch": 0.018510608002278228,
      "grad_norm": 0.5514938235282898,
      "learning_rate": 0.00019886034474571443,
      "loss": 0.7195,
      "step": 1300
    },
    {
      "epoch": 0.019222554463904314,
      "grad_norm": 0.5589170455932617,
      "learning_rate": 0.0001988128591101192,
      "loss": 0.78,
      "step": 1350
    },
    {
      "epoch": 0.0199345009255304,
      "grad_norm": 0.8585851192474365,
      "learning_rate": 0.00019876537347452398,
      "loss": 0.7516,
      "step": 1400
    },
    {
      "epoch": 0.020646447387156486,
      "grad_norm": 0.6393819451332092,
      "learning_rate": 0.00019871788783892873,
      "loss": 0.7763,
      "step": 1450
    },
    {
      "epoch": 0.021358393848782572,
      "grad_norm": 0.7929326891899109,
      "learning_rate": 0.0001986704022033335,
      "loss": 0.7438,
      "step": 1500
    },
    {
      "epoch": 0.022070340310408658,
      "grad_norm": 0.44529393315315247,
      "learning_rate": 0.00019862291656773826,
      "loss": 0.7053,
      "step": 1550
    },
    {
      "epoch": 0.022782286772034744,
      "grad_norm": 0.6121500134468079,
      "learning_rate": 0.00019857543093214304,
      "loss": 0.7049,
      "step": 1600
    },
    {
      "epoch": 0.02349423323366083,
      "grad_norm": 0.5606988072395325,
      "learning_rate": 0.00019852794529654782,
      "loss": 0.6974,
      "step": 1650
    },
    {
      "epoch": 0.024206179695286916,
      "grad_norm": 0.8857629299163818,
      "learning_rate": 0.00019848045966095257,
      "loss": 0.7648,
      "step": 1700
    },
    {
      "epoch": 0.024918126156913,
      "grad_norm": 0.9423251748085022,
      "learning_rate": 0.00019843297402535735,
      "loss": 0.7203,
      "step": 1750
    },
    {
      "epoch": 0.025630072618539085,
      "grad_norm": 0.3749793767929077,
      "learning_rate": 0.0001983854883897621,
      "loss": 0.7088,
      "step": 1800
    },
    {
      "epoch": 0.02634201908016517,
      "grad_norm": 0.8474676012992859,
      "learning_rate": 0.00019833800275416687,
      "loss": 0.682,
      "step": 1850
    },
    {
      "epoch": 0.027053965541791257,
      "grad_norm": 0.6499496698379517,
      "learning_rate": 0.00019829051711857165,
      "loss": 0.7684,
      "step": 1900
    },
    {
      "epoch": 0.027765912003417343,
      "grad_norm": 0.44813019037246704,
      "learning_rate": 0.0001982430314829764,
      "loss": 0.7513,
      "step": 1950
    },
    {
      "epoch": 0.02847785846504343,
      "grad_norm": 0.8961121439933777,
      "learning_rate": 0.00019819554584738118,
      "loss": 0.7618,
      "step": 2000
    },
    {
      "epoch": 0.029189804926669515,
      "grad_norm": 0.7726924419403076,
      "learning_rate": 0.00019814806021178593,
      "loss": 0.6797,
      "step": 2050
    },
    {
      "epoch": 0.0299017513882956,
      "grad_norm": 0.5274971127510071,
      "learning_rate": 0.0001981005745761907,
      "loss": 0.7349,
      "step": 2100
    },
    {
      "epoch": 0.030613697849921687,
      "grad_norm": 0.5519452095031738,
      "learning_rate": 0.00019805308894059548,
      "loss": 0.7125,
      "step": 2150
    },
    {
      "epoch": 0.03132564431154777,
      "grad_norm": 1.1728527545928955,
      "learning_rate": 0.00019800560330500024,
      "loss": 0.7048,
      "step": 2200
    },
    {
      "epoch": 0.032037590773173856,
      "grad_norm": 0.44137105345726013,
      "learning_rate": 0.000197958117669405,
      "loss": 0.7232,
      "step": 2250
    },
    {
      "epoch": 0.032749537234799946,
      "grad_norm": 0.8850932121276855,
      "learning_rate": 0.00019791063203380976,
      "loss": 0.7052,
      "step": 2300
    },
    {
      "epoch": 0.03346148369642603,
      "grad_norm": 0.3644295036792755,
      "learning_rate": 0.00019786314639821454,
      "loss": 0.7139,
      "step": 2350
    },
    {
      "epoch": 0.03417343015805212,
      "grad_norm": 1.1426713466644287,
      "learning_rate": 0.00019781566076261932,
      "loss": 0.715,
      "step": 2400
    },
    {
      "epoch": 0.0348853766196782,
      "grad_norm": 0.544600248336792,
      "learning_rate": 0.00019776817512702407,
      "loss": 0.702,
      "step": 2450
    },
    {
      "epoch": 0.03559732308130428,
      "grad_norm": 0.6748408079147339,
      "learning_rate": 0.00019772068949142885,
      "loss": 0.6686,
      "step": 2500
    },
    {
      "epoch": 0.03630926954293037,
      "grad_norm": 0.4423487186431885,
      "learning_rate": 0.0001976732038558336,
      "loss": 0.6867,
      "step": 2550
    },
    {
      "epoch": 0.037021216004556455,
      "grad_norm": 0.7080407738685608,
      "learning_rate": 0.00019762571822023837,
      "loss": 0.732,
      "step": 2600
    },
    {
      "epoch": 0.037733162466182545,
      "grad_norm": 0.31992340087890625,
      "learning_rate": 0.00019757823258464315,
      "loss": 0.7027,
      "step": 2650
    },
    {
      "epoch": 0.03844510892780863,
      "grad_norm": 0.45096948742866516,
      "learning_rate": 0.00019753074694904793,
      "loss": 0.6901,
      "step": 2700
    },
    {
      "epoch": 0.03915705538943472,
      "grad_norm": 0.5331617593765259,
      "learning_rate": 0.0001974832613134527,
      "loss": 0.755,
      "step": 2750
    },
    {
      "epoch": 0.0398690018510608,
      "grad_norm": 0.8468232750892639,
      "learning_rate": 0.00019743577567785746,
      "loss": 0.7431,
      "step": 2800
    },
    {
      "epoch": 0.04058094831268689,
      "grad_norm": 0.7852275967597961,
      "learning_rate": 0.00019738829004226224,
      "loss": 0.683,
      "step": 2850
    },
    {
      "epoch": 0.04129289477431297,
      "grad_norm": 0.6966323852539062,
      "learning_rate": 0.000197340804406667,
      "loss": 0.675,
      "step": 2900
    },
    {
      "epoch": 0.042004841235939054,
      "grad_norm": 0.8339259028434753,
      "learning_rate": 0.00019729331877107176,
      "loss": 0.674,
      "step": 2950
    },
    {
      "epoch": 0.042716787697565144,
      "grad_norm": 0.523601233959198,
      "learning_rate": 0.00019724583313547654,
      "loss": 0.7278,
      "step": 3000
    },
    {
      "epoch": 0.043428734159191226,
      "grad_norm": 0.5586069822311401,
      "learning_rate": 0.0001971983474998813,
      "loss": 0.6951,
      "step": 3050
    },
    {
      "epoch": 0.044140680620817316,
      "grad_norm": 0.6161527037620544,
      "learning_rate": 0.00019715086186428607,
      "loss": 0.7007,
      "step": 3100
    },
    {
      "epoch": 0.0448526270824434,
      "grad_norm": 0.47202742099761963,
      "learning_rate": 0.00019710337622869085,
      "loss": 0.7143,
      "step": 3150
    },
    {
      "epoch": 0.04556457354406949,
      "grad_norm": 0.5672162771224976,
      "learning_rate": 0.0001970558905930956,
      "loss": 0.6994,
      "step": 3200
    },
    {
      "epoch": 0.04627652000569557,
      "grad_norm": 0.45998501777648926,
      "learning_rate": 0.00019700840495750038,
      "loss": 0.7364,
      "step": 3250
    },
    {
      "epoch": 0.04698846646732166,
      "grad_norm": 0.3553861081600189,
      "learning_rate": 0.00019696091932190513,
      "loss": 0.6522,
      "step": 3300
    },
    {
      "epoch": 0.04770041292894774,
      "grad_norm": 0.4985118806362152,
      "learning_rate": 0.0001969134336863099,
      "loss": 0.7161,
      "step": 3350
    },
    {
      "epoch": 0.04841235939057383,
      "grad_norm": 0.49380290508270264,
      "learning_rate": 0.00019686594805071468,
      "loss": 0.7362,
      "step": 3400
    },
    {
      "epoch": 0.049124305852199915,
      "grad_norm": 0.49287867546081543,
      "learning_rate": 0.00019681846241511943,
      "loss": 0.6899,
      "step": 3450
    },
    {
      "epoch": 0.049836252313826,
      "grad_norm": 0.44767189025878906,
      "learning_rate": 0.0001967709767795242,
      "loss": 0.6996,
      "step": 3500
    },
    {
      "epoch": 0.05054819877545209,
      "grad_norm": 0.4863317012786865,
      "learning_rate": 0.00019672349114392896,
      "loss": 0.6972,
      "step": 3550
    },
    {
      "epoch": 0.05126014523707817,
      "grad_norm": 0.35835927724838257,
      "learning_rate": 0.00019667600550833374,
      "loss": 0.7122,
      "step": 3600
    },
    {
      "epoch": 0.05197209169870426,
      "grad_norm": 0.4981730878353119,
      "learning_rate": 0.00019662851987273852,
      "loss": 0.7503,
      "step": 3650
    },
    {
      "epoch": 0.05268403816033034,
      "grad_norm": 0.8259324431419373,
      "learning_rate": 0.00019658103423714327,
      "loss": 0.6744,
      "step": 3700
    },
    {
      "epoch": 0.05339598462195643,
      "grad_norm": 0.7307945489883423,
      "learning_rate": 0.00019653354860154804,
      "loss": 0.7521,
      "step": 3750
    },
    {
      "epoch": 0.054107931083582514,
      "grad_norm": 0.9237150549888611,
      "learning_rate": 0.0001964860629659528,
      "loss": 0.7775,
      "step": 3800
    },
    {
      "epoch": 0.0548198775452086,
      "grad_norm": 0.5096402168273926,
      "learning_rate": 0.00019643857733035757,
      "loss": 0.7885,
      "step": 3850
    },
    {
      "epoch": 0.055531824006834686,
      "grad_norm": 0.5888190269470215,
      "learning_rate": 0.00019639109169476235,
      "loss": 0.6303,
      "step": 3900
    },
    {
      "epoch": 0.05624377046846077,
      "grad_norm": 0.4871295988559723,
      "learning_rate": 0.0001963436060591671,
      "loss": 0.6964,
      "step": 3950
    },
    {
      "epoch": 0.05695571693008686,
      "grad_norm": 0.48180490732192993,
      "learning_rate": 0.00019629612042357188,
      "loss": 0.6858,
      "step": 4000
    },
    {
      "epoch": 0.05766766339171294,
      "grad_norm": 0.7341374158859253,
      "learning_rate": 0.00019624863478797663,
      "loss": 0.6725,
      "step": 4050
    },
    {
      "epoch": 0.05837960985333903,
      "grad_norm": 0.9766613841056824,
      "learning_rate": 0.0001962011491523814,
      "loss": 0.6675,
      "step": 4100
    },
    {
      "epoch": 0.05909155631496511,
      "grad_norm": 0.8052583336830139,
      "learning_rate": 0.00019615366351678618,
      "loss": 0.7346,
      "step": 4150
    },
    {
      "epoch": 0.0598035027765912,
      "grad_norm": 0.9619312286376953,
      "learning_rate": 0.00019610617788119093,
      "loss": 0.6673,
      "step": 4200
    },
    {
      "epoch": 0.060515449238217285,
      "grad_norm": 0.8578758835792542,
      "learning_rate": 0.0001960586922455957,
      "loss": 0.6816,
      "step": 4250
    },
    {
      "epoch": 0.061227395699843375,
      "grad_norm": 0.7369866967201233,
      "learning_rate": 0.0001960112066100005,
      "loss": 0.7306,
      "step": 4300
    },
    {
      "epoch": 0.06193934216146946,
      "grad_norm": 0.6480921506881714,
      "learning_rate": 0.00019596372097440524,
      "loss": 0.7467,
      "step": 4350
    },
    {
      "epoch": 0.06265128862309555,
      "grad_norm": 0.6069624423980713,
      "learning_rate": 0.00019591623533881002,
      "loss": 0.6609,
      "step": 4400
    },
    {
      "epoch": 0.06336323508472162,
      "grad_norm": 0.8978185653686523,
      "learning_rate": 0.0001958687497032148,
      "loss": 0.7163,
      "step": 4450
    },
    {
      "epoch": 0.06407518154634771,
      "grad_norm": 0.6166532635688782,
      "learning_rate": 0.00019582126406761957,
      "loss": 0.7559,
      "step": 4500
    },
    {
      "epoch": 0.0647871280079738,
      "grad_norm": 0.7387000322341919,
      "learning_rate": 0.00019577377843202432,
      "loss": 0.7176,
      "step": 4550
    },
    {
      "epoch": 0.06549907446959989,
      "grad_norm": 0.6487991809844971,
      "learning_rate": 0.0001957262927964291,
      "loss": 0.6567,
      "step": 4600
    },
    {
      "epoch": 0.06621102093122597,
      "grad_norm": 0.6061031818389893,
      "learning_rate": 0.00019567880716083388,
      "loss": 0.6945,
      "step": 4650
    },
    {
      "epoch": 0.06692296739285206,
      "grad_norm": 0.5449630618095398,
      "learning_rate": 0.00019563132152523863,
      "loss": 0.6391,
      "step": 4700
    },
    {
      "epoch": 0.06763491385447815,
      "grad_norm": 0.42971062660217285,
      "learning_rate": 0.0001955838358896434,
      "loss": 0.6878,
      "step": 4750
    },
    {
      "epoch": 0.06834686031610424,
      "grad_norm": 0.5285096168518066,
      "learning_rate": 0.00019553635025404816,
      "loss": 0.7447,
      "step": 4800
    },
    {
      "epoch": 0.06905880677773031,
      "grad_norm": 0.5713834762573242,
      "learning_rate": 0.00019548886461845293,
      "loss": 0.7157,
      "step": 4850
    },
    {
      "epoch": 0.0697707532393564,
      "grad_norm": 0.6133925914764404,
      "learning_rate": 0.0001954413789828577,
      "loss": 0.6952,
      "step": 4900
    },
    {
      "epoch": 0.07048269970098249,
      "grad_norm": 0.6245903968811035,
      "learning_rate": 0.00019539389334726246,
      "loss": 0.6835,
      "step": 4950
    },
    {
      "epoch": 0.07119464616260857,
      "grad_norm": 0.537019670009613,
      "learning_rate": 0.00019534640771166724,
      "loss": 0.6842,
      "step": 5000
    },
    {
      "epoch": 0.07190659262423466,
      "grad_norm": 0.4502958655357361,
      "learning_rate": 0.000195298922076072,
      "loss": 0.6742,
      "step": 5050
    },
    {
      "epoch": 0.07261853908586074,
      "grad_norm": 0.47497907280921936,
      "learning_rate": 0.00019525143644047677,
      "loss": 0.6686,
      "step": 5100
    },
    {
      "epoch": 0.07333048554748683,
      "grad_norm": 0.7555782794952393,
      "learning_rate": 0.00019520395080488155,
      "loss": 0.6987,
      "step": 5150
    },
    {
      "epoch": 0.07404243200911291,
      "grad_norm": 0.6344314217567444,
      "learning_rate": 0.0001951564651692863,
      "loss": 0.7424,
      "step": 5200
    },
    {
      "epoch": 0.074754378470739,
      "grad_norm": 0.4931880831718445,
      "learning_rate": 0.00019510897953369107,
      "loss": 0.7308,
      "step": 5250
    },
    {
      "epoch": 0.07546632493236509,
      "grad_norm": 0.4543268382549286,
      "learning_rate": 0.00019506149389809582,
      "loss": 0.7449,
      "step": 5300
    },
    {
      "epoch": 0.07617827139399118,
      "grad_norm": 0.36828458309173584,
      "learning_rate": 0.0001950140082625006,
      "loss": 0.6578,
      "step": 5350
    },
    {
      "epoch": 0.07689021785561725,
      "grad_norm": 0.4573463499546051,
      "learning_rate": 0.00019496652262690538,
      "loss": 0.7367,
      "step": 5400
    },
    {
      "epoch": 0.07760216431724334,
      "grad_norm": 0.7267215847969055,
      "learning_rate": 0.00019491903699131013,
      "loss": 0.7127,
      "step": 5450
    },
    {
      "epoch": 0.07831411077886943,
      "grad_norm": 0.4614969491958618,
      "learning_rate": 0.0001948715513557149,
      "loss": 0.6964,
      "step": 5500
    },
    {
      "epoch": 0.07902605724049551,
      "grad_norm": 0.3561732769012451,
      "learning_rate": 0.00019482406572011966,
      "loss": 0.7553,
      "step": 5550
    },
    {
      "epoch": 0.0797380037021216,
      "grad_norm": 0.5233297348022461,
      "learning_rate": 0.00019477658008452444,
      "loss": 0.6824,
      "step": 5600
    },
    {
      "epoch": 0.08044995016374769,
      "grad_norm": 0.6948227882385254,
      "learning_rate": 0.0001947290944489292,
      "loss": 0.7721,
      "step": 5650
    },
    {
      "epoch": 0.08116189662537378,
      "grad_norm": 0.4559673070907593,
      "learning_rate": 0.00019468160881333396,
      "loss": 0.7356,
      "step": 5700
    },
    {
      "epoch": 0.08187384308699985,
      "grad_norm": 0.7700779438018799,
      "learning_rate": 0.00019463412317773874,
      "loss": 0.7285,
      "step": 5750
    },
    {
      "epoch": 0.08258578954862594,
      "grad_norm": 0.3959493041038513,
      "learning_rate": 0.0001945866375421435,
      "loss": 0.7299,
      "step": 5800
    },
    {
      "epoch": 0.08329773601025203,
      "grad_norm": 0.5573490858078003,
      "learning_rate": 0.00019453915190654827,
      "loss": 0.708,
      "step": 5850
    },
    {
      "epoch": 0.08400968247187811,
      "grad_norm": 0.5645168423652649,
      "learning_rate": 0.00019449166627095305,
      "loss": 0.6626,
      "step": 5900
    },
    {
      "epoch": 0.0847216289335042,
      "grad_norm": 0.44044560194015503,
      "learning_rate": 0.0001944441806353578,
      "loss": 0.6954,
      "step": 5950
    },
    {
      "epoch": 0.08543357539513029,
      "grad_norm": 0.7016535401344299,
      "learning_rate": 0.00019439669499976257,
      "loss": 0.6718,
      "step": 6000
    },
    {
      "epoch": 0.08614552185675638,
      "grad_norm": 0.41936609148979187,
      "learning_rate": 0.00019434920936416735,
      "loss": 0.7134,
      "step": 6050
    },
    {
      "epoch": 0.08685746831838245,
      "grad_norm": 0.6575160026550293,
      "learning_rate": 0.0001943017237285721,
      "loss": 0.7231,
      "step": 6100
    },
    {
      "epoch": 0.08756941478000854,
      "grad_norm": 0.6923287510871887,
      "learning_rate": 0.00019425423809297688,
      "loss": 0.7118,
      "step": 6150
    },
    {
      "epoch": 0.08828136124163463,
      "grad_norm": 0.3608802556991577,
      "learning_rate": 0.00019420675245738166,
      "loss": 0.6556,
      "step": 6200
    },
    {
      "epoch": 0.08899330770326072,
      "grad_norm": 0.49045491218566895,
      "learning_rate": 0.00019415926682178644,
      "loss": 0.6404,
      "step": 6250
    },
    {
      "epoch": 0.0897052541648868,
      "grad_norm": 0.5386914014816284,
      "learning_rate": 0.00019411178118619119,
      "loss": 0.702,
      "step": 6300
    },
    {
      "epoch": 0.09041720062651289,
      "grad_norm": 0.5859946608543396,
      "learning_rate": 0.00019406429555059596,
      "loss": 0.6889,
      "step": 6350
    },
    {
      "epoch": 0.09112914708813898,
      "grad_norm": 0.5204946398735046,
      "learning_rate": 0.00019401680991500074,
      "loss": 0.7004,
      "step": 6400
    },
    {
      "epoch": 0.09184109354976505,
      "grad_norm": 0.46367770433425903,
      "learning_rate": 0.0001939693242794055,
      "loss": 0.6521,
      "step": 6450
    },
    {
      "epoch": 0.09255304001139114,
      "grad_norm": 0.4543396830558777,
      "learning_rate": 0.00019392183864381027,
      "loss": 0.7173,
      "step": 6500
    },
    {
      "epoch": 0.09326498647301723,
      "grad_norm": 0.7840442061424255,
      "learning_rate": 0.00019387435300821502,
      "loss": 0.7058,
      "step": 6550
    },
    {
      "epoch": 0.09397693293464332,
      "grad_norm": 0.41895341873168945,
      "learning_rate": 0.0001938268673726198,
      "loss": 0.7164,
      "step": 6600
    },
    {
      "epoch": 0.0946888793962694,
      "grad_norm": 0.4375690817832947,
      "learning_rate": 0.00019377938173702458,
      "loss": 0.7082,
      "step": 6650
    },
    {
      "epoch": 0.09540082585789549,
      "grad_norm": 0.45913165807724,
      "learning_rate": 0.00019373189610142933,
      "loss": 0.7121,
      "step": 6700
    },
    {
      "epoch": 0.09611277231952157,
      "grad_norm": 0.37611088156700134,
      "learning_rate": 0.000193685360178546,
      "loss": 0.7188,
      "step": 6750
    },
    {
      "epoch": 0.09682471878114766,
      "grad_norm": 0.5824044346809387,
      "learning_rate": 0.00019363787454295078,
      "loss": 0.6205,
      "step": 6800
    },
    {
      "epoch": 0.09753666524277374,
      "grad_norm": 0.4285948872566223,
      "learning_rate": 0.00019359038890735553,
      "loss": 0.7292,
      "step": 6850
    },
    {
      "epoch": 0.09824861170439983,
      "grad_norm": 0.5465938448905945,
      "learning_rate": 0.0001935429032717603,
      "loss": 0.7167,
      "step": 6900
    },
    {
      "epoch": 0.09896055816602592,
      "grad_norm": 0.5359683036804199,
      "learning_rate": 0.0001934954176361651,
      "loss": 0.7049,
      "step": 6950
    },
    {
      "epoch": 0.099672504627652,
      "grad_norm": 0.4726085364818573,
      "learning_rate": 0.00019344793200056984,
      "loss": 0.69,
      "step": 7000
    },
    {
      "epoch": 0.10038445108927808,
      "grad_norm": 0.4319663941860199,
      "learning_rate": 0.00019340044636497462,
      "loss": 0.7503,
      "step": 7050
    },
    {
      "epoch": 0.10109639755090417,
      "grad_norm": 0.39279812574386597,
      "learning_rate": 0.00019335296072937937,
      "loss": 0.708,
      "step": 7100
    },
    {
      "epoch": 0.10180834401253026,
      "grad_norm": 0.6290025115013123,
      "learning_rate": 0.00019330547509378414,
      "loss": 0.6903,
      "step": 7150
    },
    {
      "epoch": 0.10252029047415634,
      "grad_norm": 0.6003757119178772,
      "learning_rate": 0.00019325798945818892,
      "loss": 0.686,
      "step": 7200
    },
    {
      "epoch": 0.10323223693578243,
      "grad_norm": 0.4517073631286621,
      "learning_rate": 0.00019321050382259367,
      "loss": 0.668,
      "step": 7250
    },
    {
      "epoch": 0.10394418339740852,
      "grad_norm": 0.8771567940711975,
      "learning_rate": 0.00019316301818699845,
      "loss": 0.6525,
      "step": 7300
    },
    {
      "epoch": 0.1046561298590346,
      "grad_norm": 0.36901238560676575,
      "learning_rate": 0.0001931155325514032,
      "loss": 0.7062,
      "step": 7350
    },
    {
      "epoch": 0.10536807632066068,
      "grad_norm": 0.4600578844547272,
      "learning_rate": 0.00019306804691580798,
      "loss": 0.7377,
      "step": 7400
    },
    {
      "epoch": 0.10608002278228677,
      "grad_norm": 0.40208351612091064,
      "learning_rate": 0.00019302056128021276,
      "loss": 0.6447,
      "step": 7450
    },
    {
      "epoch": 0.10679196924391286,
      "grad_norm": 0.5099172592163086,
      "learning_rate": 0.0001929730756446175,
      "loss": 0.7081,
      "step": 7500
    },
    {
      "epoch": 0.10750391570553894,
      "grad_norm": 0.9746870398521423,
      "learning_rate": 0.00019292559000902228,
      "loss": 0.7458,
      "step": 7550
    },
    {
      "epoch": 0.10821586216716503,
      "grad_norm": 0.4724650979042053,
      "learning_rate": 0.00019287810437342703,
      "loss": 0.7444,
      "step": 7600
    },
    {
      "epoch": 0.10892780862879112,
      "grad_norm": 0.5653396248817444,
      "learning_rate": 0.0001928306187378318,
      "loss": 0.6985,
      "step": 7650
    },
    {
      "epoch": 0.1096397550904172,
      "grad_norm": 0.6812823414802551,
      "learning_rate": 0.0001927831331022366,
      "loss": 0.6836,
      "step": 7700
    },
    {
      "epoch": 0.11035170155204328,
      "grad_norm": 0.3255240321159363,
      "learning_rate": 0.00019273564746664134,
      "loss": 0.6966,
      "step": 7750
    },
    {
      "epoch": 0.11106364801366937,
      "grad_norm": 0.7691791653633118,
      "learning_rate": 0.00019268816183104612,
      "loss": 0.6296,
      "step": 7800
    },
    {
      "epoch": 0.11177559447529546,
      "grad_norm": 0.8415160179138184,
      "learning_rate": 0.00019264067619545087,
      "loss": 0.6484,
      "step": 7850
    },
    {
      "epoch": 0.11248754093692154,
      "grad_norm": 0.2714729905128479,
      "learning_rate": 0.00019259319055985565,
      "loss": 0.6758,
      "step": 7900
    },
    {
      "epoch": 0.11319948739854763,
      "grad_norm": 0.40790167450904846,
      "learning_rate": 0.00019254570492426042,
      "loss": 0.7417,
      "step": 7950
    },
    {
      "epoch": 0.11391143386017372,
      "grad_norm": 0.40004897117614746,
      "learning_rate": 0.00019249821928866517,
      "loss": 0.7597,
      "step": 8000
    },
    {
      "epoch": 0.1146233803217998,
      "grad_norm": 0.46127668023109436,
      "learning_rate": 0.00019245073365306995,
      "loss": 0.6373,
      "step": 8050
    },
    {
      "epoch": 0.11533532678342588,
      "grad_norm": 0.45349830389022827,
      "learning_rate": 0.0001924032480174747,
      "loss": 0.6955,
      "step": 8100
    },
    {
      "epoch": 0.11604727324505197,
      "grad_norm": 0.4830804467201233,
      "learning_rate": 0.00019235576238187948,
      "loss": 0.6625,
      "step": 8150
    },
    {
      "epoch": 0.11675921970667806,
      "grad_norm": 0.7604774236679077,
      "learning_rate": 0.00019230827674628426,
      "loss": 0.6603,
      "step": 8200
    },
    {
      "epoch": 0.11747116616830415,
      "grad_norm": 0.4507305324077606,
      "learning_rate": 0.000192260791110689,
      "loss": 0.763,
      "step": 8250
    },
    {
      "epoch": 0.11818311262993023,
      "grad_norm": 0.3846606910228729,
      "learning_rate": 0.00019221330547509379,
      "loss": 0.726,
      "step": 8300
    },
    {
      "epoch": 0.11889505909155632,
      "grad_norm": 0.39435678720474243,
      "learning_rate": 0.00019216581983949856,
      "loss": 0.6895,
      "step": 8350
    },
    {
      "epoch": 0.1196070055531824,
      "grad_norm": 0.49928000569343567,
      "learning_rate": 0.00019211833420390334,
      "loss": 0.7327,
      "step": 8400
    },
    {
      "epoch": 0.12031895201480848,
      "grad_norm": 0.4502124488353729,
      "learning_rate": 0.0001920708485683081,
      "loss": 0.6781,
      "step": 8450
    },
    {
      "epoch": 0.12103089847643457,
      "grad_norm": 0.4895519018173218,
      "learning_rate": 0.00019202336293271287,
      "loss": 0.7096,
      "step": 8500
    },
    {
      "epoch": 0.12174284493806066,
      "grad_norm": 0.5177880525588989,
      "learning_rate": 0.00019197587729711765,
      "loss": 0.7283,
      "step": 8550
    },
    {
      "epoch": 0.12245479139968675,
      "grad_norm": 0.6024084091186523,
      "learning_rate": 0.0001919293413742343,
      "loss": 0.7308,
      "step": 8600
    },
    {
      "epoch": 0.12316673786131282,
      "grad_norm": 0.4800964891910553,
      "learning_rate": 0.00019188185573863905,
      "loss": 0.6942,
      "step": 8650
    },
    {
      "epoch": 0.12387868432293891,
      "grad_norm": 0.3546399772167206,
      "learning_rate": 0.00019183437010304383,
      "loss": 0.672,
      "step": 8700
    },
    {
      "epoch": 0.124590630784565,
      "grad_norm": 0.7654172778129578,
      "learning_rate": 0.0001917868844674486,
      "loss": 0.6783,
      "step": 8750
    },
    {
      "epoch": 0.1253025772461911,
      "grad_norm": 0.5014586448669434,
      "learning_rate": 0.00019173939883185335,
      "loss": 0.667,
      "step": 8800
    },
    {
      "epoch": 0.12601452370781718,
      "grad_norm": 0.7384951710700989,
      "learning_rate": 0.00019169191319625813,
      "loss": 0.7478,
      "step": 8850
    },
    {
      "epoch": 0.12672647016944324,
      "grad_norm": 0.7167573571205139,
      "learning_rate": 0.0001916444275606629,
      "loss": 0.7322,
      "step": 8900
    },
    {
      "epoch": 0.12743841663106933,
      "grad_norm": 0.5049604177474976,
      "learning_rate": 0.0001915969419250677,
      "loss": 0.6877,
      "step": 8950
    },
    {
      "epoch": 0.12815036309269542,
      "grad_norm": 0.4349973797798157,
      "learning_rate": 0.00019154945628947244,
      "loss": 0.6435,
      "step": 9000
    },
    {
      "epoch": 0.1288623095543215,
      "grad_norm": 0.4939323365688324,
      "learning_rate": 0.00019150197065387722,
      "loss": 0.6963,
      "step": 9050
    },
    {
      "epoch": 0.1295742560159476,
      "grad_norm": 0.6483086943626404,
      "learning_rate": 0.000191454485018282,
      "loss": 0.734,
      "step": 9100
    },
    {
      "epoch": 0.1302862024775737,
      "grad_norm": 0.3657320737838745,
      "learning_rate": 0.00019140699938268674,
      "loss": 0.6726,
      "step": 9150
    },
    {
      "epoch": 0.13099814893919978,
      "grad_norm": 0.48090600967407227,
      "learning_rate": 0.00019135951374709152,
      "loss": 0.7127,
      "step": 9200
    },
    {
      "epoch": 0.13171009540082587,
      "grad_norm": 0.3313867449760437,
      "learning_rate": 0.0001913120281114963,
      "loss": 0.7546,
      "step": 9250
    },
    {
      "epoch": 0.13242204186245193,
      "grad_norm": 0.7718214392662048,
      "learning_rate": 0.00019126454247590105,
      "loss": 0.77,
      "step": 9300
    },
    {
      "epoch": 0.13313398832407802,
      "grad_norm": 0.8705127835273743,
      "learning_rate": 0.00019121705684030583,
      "loss": 0.737,
      "step": 9350
    },
    {
      "epoch": 0.1338459347857041,
      "grad_norm": 0.47264111042022705,
      "learning_rate": 0.00019116957120471058,
      "loss": 0.7467,
      "step": 9400
    },
    {
      "epoch": 0.1345578812473302,
      "grad_norm": 0.7625200152397156,
      "learning_rate": 0.00019112208556911536,
      "loss": 0.6628,
      "step": 9450
    },
    {
      "epoch": 0.1352698277089563,
      "grad_norm": 0.43054401874542236,
      "learning_rate": 0.00019107459993352013,
      "loss": 0.6468,
      "step": 9500
    },
    {
      "epoch": 0.13598177417058238,
      "grad_norm": 0.694355309009552,
      "learning_rate": 0.00019102711429792488,
      "loss": 0.7513,
      "step": 9550
    },
    {
      "epoch": 0.13669372063220847,
      "grad_norm": 0.5550425052642822,
      "learning_rate": 0.00019097962866232966,
      "loss": 0.6687,
      "step": 9600
    },
    {
      "epoch": 0.13740566709383453,
      "grad_norm": 0.48079320788383484,
      "learning_rate": 0.0001909321430267344,
      "loss": 0.656,
      "step": 9650
    },
    {
      "epoch": 0.13811761355546062,
      "grad_norm": 0.44573649764060974,
      "learning_rate": 0.0001908846573911392,
      "loss": 0.6634,
      "step": 9700
    },
    {
      "epoch": 0.1388295600170867,
      "grad_norm": 0.45440688729286194,
      "learning_rate": 0.00019083717175554397,
      "loss": 0.6744,
      "step": 9750
    },
    {
      "epoch": 0.1395415064787128,
      "grad_norm": 0.530576765537262,
      "learning_rate": 0.00019078968611994872,
      "loss": 0.6426,
      "step": 9800
    },
    {
      "epoch": 0.1402534529403389,
      "grad_norm": 0.6273830533027649,
      "learning_rate": 0.0001907422004843535,
      "loss": 0.6771,
      "step": 9850
    },
    {
      "epoch": 0.14096539940196498,
      "grad_norm": 0.487541526556015,
      "learning_rate": 0.00019069471484875825,
      "loss": 0.6756,
      "step": 9900
    },
    {
      "epoch": 0.14167734586359107,
      "grad_norm": 0.5881896018981934,
      "learning_rate": 0.00019064722921316302,
      "loss": 0.7443,
      "step": 9950
    },
    {
      "epoch": 0.14238929232521713,
      "grad_norm": 0.4691188633441925,
      "learning_rate": 0.0001905997435775678,
      "loss": 0.7369,
      "step": 10000
    },
    {
      "epoch": 0.14310123878684322,
      "grad_norm": 0.48362234234809875,
      "learning_rate": 0.00019055225794197255,
      "loss": 0.7187,
      "step": 10050
    },
    {
      "epoch": 0.1438131852484693,
      "grad_norm": 0.7114773988723755,
      "learning_rate": 0.00019050477230637733,
      "loss": 0.682,
      "step": 10100
    },
    {
      "epoch": 0.1445251317100954,
      "grad_norm": 0.46073395013809204,
      "learning_rate": 0.00019045728667078208,
      "loss": 0.6998,
      "step": 10150
    },
    {
      "epoch": 0.1452370781717215,
      "grad_norm": 0.7717083692550659,
      "learning_rate": 0.00019040980103518686,
      "loss": 0.6951,
      "step": 10200
    },
    {
      "epoch": 0.14594902463334758,
      "grad_norm": 0.7157528400421143,
      "learning_rate": 0.00019036231539959163,
      "loss": 0.6982,
      "step": 10250
    },
    {
      "epoch": 0.14666097109497367,
      "grad_norm": 0.6716151833534241,
      "learning_rate": 0.00019031482976399639,
      "loss": 0.6698,
      "step": 10300
    },
    {
      "epoch": 0.14737291755659973,
      "grad_norm": 0.796192467212677,
      "learning_rate": 0.00019026734412840116,
      "loss": 0.7315,
      "step": 10350
    },
    {
      "epoch": 0.14808486401822582,
      "grad_norm": 0.5830238461494446,
      "learning_rate": 0.0001902198584928059,
      "loss": 0.7065,
      "step": 10400
    },
    {
      "epoch": 0.1487968104798519,
      "grad_norm": 0.5832176208496094,
      "learning_rate": 0.0001901723728572107,
      "loss": 0.711,
      "step": 10450
    },
    {
      "epoch": 0.149508756941478,
      "grad_norm": 0.3852265179157257,
      "learning_rate": 0.00019012488722161547,
      "loss": 0.7067,
      "step": 10500
    },
    {
      "epoch": 0.1502207034031041,
      "grad_norm": 0.607343852519989,
      "learning_rate": 0.00019007740158602022,
      "loss": 0.7177,
      "step": 10550
    },
    {
      "epoch": 0.15093264986473018,
      "grad_norm": 0.44329699873924255,
      "learning_rate": 0.000190029915950425,
      "loss": 0.6999,
      "step": 10600
    },
    {
      "epoch": 0.15164459632635627,
      "grad_norm": 0.7200364470481873,
      "learning_rate": 0.00018998243031482977,
      "loss": 0.6784,
      "step": 10650
    },
    {
      "epoch": 0.15235654278798236,
      "grad_norm": 0.4712580442428589,
      "learning_rate": 0.00018993494467923455,
      "loss": 0.7064,
      "step": 10700
    },
    {
      "epoch": 0.15306848924960842,
      "grad_norm": 0.8364105820655823,
      "learning_rate": 0.0001898874590436393,
      "loss": 0.625,
      "step": 10750
    },
    {
      "epoch": 0.1537804357112345,
      "grad_norm": 0.8134909868240356,
      "learning_rate": 0.00018983997340804408,
      "loss": 0.6512,
      "step": 10800
    },
    {
      "epoch": 0.1544923821728606,
      "grad_norm": 0.35703423619270325,
      "learning_rate": 0.00018979248777244886,
      "loss": 0.6915,
      "step": 10850
    },
    {
      "epoch": 0.1552043286344867,
      "grad_norm": 0.48514363169670105,
      "learning_rate": 0.0001897450021368536,
      "loss": 0.7251,
      "step": 10900
    },
    {
      "epoch": 0.15591627509611278,
      "grad_norm": 0.6510757803916931,
      "learning_rate": 0.00018969751650125839,
      "loss": 0.6452,
      "step": 10950
    },
    {
      "epoch": 0.15662822155773887,
      "grad_norm": 0.3404597342014313,
      "learning_rate": 0.00018965003086566316,
      "loss": 0.699,
      "step": 11000
    },
    {
      "epoch": 0.15734016801936496,
      "grad_norm": 0.73976731300354,
      "learning_rate": 0.00018960254523006791,
      "loss": 0.6769,
      "step": 11050
    },
    {
      "epoch": 0.15805211448099102,
      "grad_norm": 0.4916416108608246,
      "learning_rate": 0.0001895550595944727,
      "loss": 0.6822,
      "step": 11100
    },
    {
      "epoch": 0.1587640609426171,
      "grad_norm": 0.5822714567184448,
      "learning_rate": 0.00018950757395887744,
      "loss": 0.6659,
      "step": 11150
    },
    {
      "epoch": 0.1594760074042432,
      "grad_norm": 0.6327516436576843,
      "learning_rate": 0.00018946008832328222,
      "loss": 0.7062,
      "step": 11200
    },
    {
      "epoch": 0.1601879538658693,
      "grad_norm": 0.45477771759033203,
      "learning_rate": 0.000189412602687687,
      "loss": 0.6526,
      "step": 11250
    },
    {
      "epoch": 0.16089990032749538,
      "grad_norm": 0.4797494113445282,
      "learning_rate": 0.00018936511705209175,
      "loss": 0.6987,
      "step": 11300
    },
    {
      "epoch": 0.16161184678912147,
      "grad_norm": 0.41163110733032227,
      "learning_rate": 0.00018931763141649653,
      "loss": 0.6606,
      "step": 11350
    },
    {
      "epoch": 0.16232379325074756,
      "grad_norm": 0.4798876643180847,
      "learning_rate": 0.00018927014578090128,
      "loss": 0.6378,
      "step": 11400
    },
    {
      "epoch": 0.16303573971237362,
      "grad_norm": 0.9591719508171082,
      "learning_rate": 0.00018922266014530605,
      "loss": 0.6729,
      "step": 11450
    },
    {
      "epoch": 0.1637476861739997,
      "grad_norm": 0.46692219376564026,
      "learning_rate": 0.00018917517450971083,
      "loss": 0.7341,
      "step": 11500
    },
    {
      "epoch": 0.1644596326356258,
      "grad_norm": 0.6616337299346924,
      "learning_rate": 0.00018912768887411558,
      "loss": 0.7194,
      "step": 11550
    },
    {
      "epoch": 0.16517157909725189,
      "grad_norm": 0.5274822115898132,
      "learning_rate": 0.00018908020323852036,
      "loss": 0.6618,
      "step": 11600
    },
    {
      "epoch": 0.16588352555887798,
      "grad_norm": 0.4653695523738861,
      "learning_rate": 0.0001890327176029251,
      "loss": 0.6606,
      "step": 11650
    },
    {
      "epoch": 0.16659547202050407,
      "grad_norm": 0.46570539474487305,
      "learning_rate": 0.0001889852319673299,
      "loss": 0.6632,
      "step": 11700
    },
    {
      "epoch": 0.16730741848213015,
      "grad_norm": 0.47176867723464966,
      "learning_rate": 0.00018893774633173466,
      "loss": 0.7037,
      "step": 11750
    },
    {
      "epoch": 0.16801936494375622,
      "grad_norm": 0.7003355622291565,
      "learning_rate": 0.00018889026069613942,
      "loss": 0.6357,
      "step": 11800
    },
    {
      "epoch": 0.1687313114053823,
      "grad_norm": 0.6379736065864563,
      "learning_rate": 0.0001888427750605442,
      "loss": 0.737,
      "step": 11850
    },
    {
      "epoch": 0.1694432578670084,
      "grad_norm": 0.49320584535598755,
      "learning_rate": 0.00018879528942494894,
      "loss": 0.6804,
      "step": 11900
    },
    {
      "epoch": 0.17015520432863449,
      "grad_norm": 0.44460248947143555,
      "learning_rate": 0.00018874780378935372,
      "loss": 0.7183,
      "step": 11950
    },
    {
      "epoch": 0.17086715079026057,
      "grad_norm": 0.61611008644104,
      "learning_rate": 0.0001887003181537585,
      "loss": 0.6604,
      "step": 12000
    },
    {
      "epoch": 0.17157909725188666,
      "grad_norm": 0.49293091893196106,
      "learning_rate": 0.00018865283251816325,
      "loss": 0.7222,
      "step": 12050
    },
    {
      "epoch": 0.17229104371351275,
      "grad_norm": 0.7209308743476868,
      "learning_rate": 0.00018860534688256803,
      "loss": 0.6856,
      "step": 12100
    },
    {
      "epoch": 0.17300299017513884,
      "grad_norm": 0.4631013572216034,
      "learning_rate": 0.00018855786124697278,
      "loss": 0.7405,
      "step": 12150
    },
    {
      "epoch": 0.1737149366367649,
      "grad_norm": 0.3978773355484009,
      "learning_rate": 0.00018851037561137755,
      "loss": 0.6494,
      "step": 12200
    },
    {
      "epoch": 0.174426883098391,
      "grad_norm": 0.4440232813358307,
      "learning_rate": 0.00018846288997578233,
      "loss": 0.7196,
      "step": 12250
    },
    {
      "epoch": 0.17513882956001708,
      "grad_norm": 0.2756289541721344,
      "learning_rate": 0.00018841540434018708,
      "loss": 0.6895,
      "step": 12300
    },
    {
      "epoch": 0.17585077602164317,
      "grad_norm": 0.33338943123817444,
      "learning_rate": 0.00018836791870459186,
      "loss": 0.7134,
      "step": 12350
    },
    {
      "epoch": 0.17656272248326926,
      "grad_norm": 0.4857466518878937,
      "learning_rate": 0.00018832043306899664,
      "loss": 0.6512,
      "step": 12400
    },
    {
      "epoch": 0.17727466894489535,
      "grad_norm": 0.5097000002861023,
      "learning_rate": 0.00018827389714611332,
      "loss": 0.6952,
      "step": 12450
    },
    {
      "epoch": 0.17798661540652144,
      "grad_norm": 0.3834129869937897,
      "learning_rate": 0.00018822641151051807,
      "loss": 0.6604,
      "step": 12500
    },
    {
      "epoch": 0.1786985618681475,
      "grad_norm": 0.7883620858192444,
      "learning_rate": 0.00018817892587492285,
      "loss": 0.716,
      "step": 12550
    },
    {
      "epoch": 0.1794105083297736,
      "grad_norm": 0.5230913758277893,
      "learning_rate": 0.0001881314402393276,
      "loss": 0.68,
      "step": 12600
    },
    {
      "epoch": 0.18012245479139968,
      "grad_norm": 0.45608413219451904,
      "learning_rate": 0.00018808395460373237,
      "loss": 0.6572,
      "step": 12650
    },
    {
      "epoch": 0.18083440125302577,
      "grad_norm": 0.37235724925994873,
      "learning_rate": 0.00018803646896813715,
      "loss": 0.6782,
      "step": 12700
    },
    {
      "epoch": 0.18154634771465186,
      "grad_norm": 0.7388754487037659,
      "learning_rate": 0.0001879889833325419,
      "loss": 0.7052,
      "step": 12750
    },
    {
      "epoch": 0.18225829417627795,
      "grad_norm": 0.3938698172569275,
      "learning_rate": 0.00018794149769694668,
      "loss": 0.6611,
      "step": 12800
    },
    {
      "epoch": 0.18297024063790404,
      "grad_norm": 0.5230003595352173,
      "learning_rate": 0.00018789401206135143,
      "loss": 0.6999,
      "step": 12850
    },
    {
      "epoch": 0.1836821870995301,
      "grad_norm": 0.43881022930145264,
      "learning_rate": 0.0001878465264257562,
      "loss": 0.6825,
      "step": 12900
    },
    {
      "epoch": 0.1843941335611562,
      "grad_norm": 0.4190365970134735,
      "learning_rate": 0.00018779904079016099,
      "loss": 0.6637,
      "step": 12950
    },
    {
      "epoch": 0.18510608002278228,
      "grad_norm": 0.29311689734458923,
      "learning_rate": 0.00018775155515456576,
      "loss": 0.6866,
      "step": 13000
    },
    {
      "epoch": 0.18581802648440837,
      "grad_norm": 0.41103047132492065,
      "learning_rate": 0.00018770406951897054,
      "loss": 0.6892,
      "step": 13050
    },
    {
      "epoch": 0.18652997294603446,
      "grad_norm": 0.5701935291290283,
      "learning_rate": 0.0001876565838833753,
      "loss": 0.7118,
      "step": 13100
    },
    {
      "epoch": 0.18724191940766055,
      "grad_norm": 0.4999297559261322,
      "learning_rate": 0.00018760909824778007,
      "loss": 0.6873,
      "step": 13150
    },
    {
      "epoch": 0.18795386586928664,
      "grad_norm": 0.456923246383667,
      "learning_rate": 0.00018756161261218485,
      "loss": 0.7236,
      "step": 13200
    },
    {
      "epoch": 0.1886658123309127,
      "grad_norm": 0.29772427678108215,
      "learning_rate": 0.0001875141269765896,
      "loss": 0.7068,
      "step": 13250
    },
    {
      "epoch": 0.1893777587925388,
      "grad_norm": 0.4567866921424866,
      "learning_rate": 0.00018746664134099437,
      "loss": 0.7216,
      "step": 13300
    },
    {
      "epoch": 0.19008970525416488,
      "grad_norm": 0.5054633021354675,
      "learning_rate": 0.00018741915570539912,
      "loss": 0.6812,
      "step": 13350
    },
    {
      "epoch": 0.19080165171579097,
      "grad_norm": 0.6466734409332275,
      "learning_rate": 0.0001873716700698039,
      "loss": 0.6966,
      "step": 13400
    },
    {
      "epoch": 0.19151359817741706,
      "grad_norm": 0.49479514360427856,
      "learning_rate": 0.00018732418443420868,
      "loss": 0.7607,
      "step": 13450
    },
    {
      "epoch": 0.19222554463904315,
      "grad_norm": 0.4364739954471588,
      "learning_rate": 0.00018727669879861343,
      "loss": 0.6597,
      "step": 13500
    },
    {
      "epoch": 0.19293749110066924,
      "grad_norm": 0.44745737314224243,
      "learning_rate": 0.0001872292131630182,
      "loss": 0.708,
      "step": 13550
    },
    {
      "epoch": 0.19364943756229533,
      "grad_norm": 0.6304471492767334,
      "learning_rate": 0.00018718172752742296,
      "loss": 0.6898,
      "step": 13600
    },
    {
      "epoch": 0.1943613840239214,
      "grad_norm": 0.49632924795150757,
      "learning_rate": 0.00018713424189182774,
      "loss": 0.7074,
      "step": 13650
    },
    {
      "epoch": 0.19507333048554748,
      "grad_norm": 0.5219523906707764,
      "learning_rate": 0.00018708675625623251,
      "loss": 0.6736,
      "step": 13700
    },
    {
      "epoch": 0.19578527694717357,
      "grad_norm": 0.5658223628997803,
      "learning_rate": 0.00018703927062063726,
      "loss": 0.7219,
      "step": 13750
    },
    {
      "epoch": 0.19649722340879966,
      "grad_norm": 0.4058087468147278,
      "learning_rate": 0.00018699178498504204,
      "loss": 0.6796,
      "step": 13800
    },
    {
      "epoch": 0.19720916987042575,
      "grad_norm": 0.7904816269874573,
      "learning_rate": 0.0001869442993494468,
      "loss": 0.684,
      "step": 13850
    },
    {
      "epoch": 0.19792111633205184,
      "grad_norm": 0.3134657144546509,
      "learning_rate": 0.00018689681371385157,
      "loss": 0.624,
      "step": 13900
    },
    {
      "epoch": 0.19863306279367793,
      "grad_norm": 0.3776400685310364,
      "learning_rate": 0.00018684932807825635,
      "loss": 0.6588,
      "step": 13950
    },
    {
      "epoch": 0.199345009255304,
      "grad_norm": 0.47208791971206665,
      "learning_rate": 0.0001868018424426611,
      "loss": 0.6859,
      "step": 14000
    },
    {
      "epoch": 0.20005695571693008,
      "grad_norm": 0.6984130144119263,
      "learning_rate": 0.00018675435680706588,
      "loss": 0.6838,
      "step": 14050
    },
    {
      "epoch": 0.20076890217855617,
      "grad_norm": 0.7986543774604797,
      "learning_rate": 0.00018670687117147063,
      "loss": 0.706,
      "step": 14100
    },
    {
      "epoch": 0.20148084864018226,
      "grad_norm": 0.6237590909004211,
      "learning_rate": 0.0001866593855358754,
      "loss": 0.6878,
      "step": 14150
    },
    {
      "epoch": 0.20219279510180835,
      "grad_norm": 0.6273373365402222,
      "learning_rate": 0.00018661189990028018,
      "loss": 0.6855,
      "step": 14200
    },
    {
      "epoch": 0.20290474156343444,
      "grad_norm": 0.604033350944519,
      "learning_rate": 0.00018656441426468493,
      "loss": 0.7049,
      "step": 14250
    },
    {
      "epoch": 0.20361668802506053,
      "grad_norm": 0.5453717708587646,
      "learning_rate": 0.0001865169286290897,
      "loss": 0.6697,
      "step": 14300
    },
    {
      "epoch": 0.2043286344866866,
      "grad_norm": 0.3115394413471222,
      "learning_rate": 0.00018646944299349446,
      "loss": 0.6401,
      "step": 14350
    },
    {
      "epoch": 0.20504058094831268,
      "grad_norm": 0.41460850834846497,
      "learning_rate": 0.00018642195735789924,
      "loss": 0.6985,
      "step": 14400
    },
    {
      "epoch": 0.20575252740993877,
      "grad_norm": 0.7311303019523621,
      "learning_rate": 0.00018637447172230402,
      "loss": 0.7445,
      "step": 14450
    },
    {
      "epoch": 0.20646447387156486,
      "grad_norm": 0.3782998323440552,
      "learning_rate": 0.00018632698608670877,
      "loss": 0.709,
      "step": 14500
    },
    {
      "epoch": 0.20717642033319095,
      "grad_norm": 0.549152672290802,
      "learning_rate": 0.00018627950045111354,
      "loss": 0.6597,
      "step": 14550
    },
    {
      "epoch": 0.20788836679481704,
      "grad_norm": 0.4877627193927765,
      "learning_rate": 0.00018623201481551832,
      "loss": 0.7201,
      "step": 14600
    },
    {
      "epoch": 0.20860031325644313,
      "grad_norm": 0.6465471386909485,
      "learning_rate": 0.00018618452917992307,
      "loss": 0.6991,
      "step": 14650
    },
    {
      "epoch": 0.2093122597180692,
      "grad_norm": 0.28309252858161926,
      "learning_rate": 0.00018613704354432785,
      "loss": 0.6732,
      "step": 14700
    },
    {
      "epoch": 0.21002420617969528,
      "grad_norm": 0.44818463921546936,
      "learning_rate": 0.00018608955790873263,
      "loss": 0.672,
      "step": 14750
    },
    {
      "epoch": 0.21073615264132137,
      "grad_norm": 0.308315634727478,
      "learning_rate": 0.0001860420722731374,
      "loss": 0.7056,
      "step": 14800
    },
    {
      "epoch": 0.21144809910294746,
      "grad_norm": 0.42353707551956177,
      "learning_rate": 0.00018599458663754215,
      "loss": 0.691,
      "step": 14850
    },
    {
      "epoch": 0.21216004556457355,
      "grad_norm": 0.509606659412384,
      "learning_rate": 0.00018594710100194693,
      "loss": 0.6298,
      "step": 14900
    },
    {
      "epoch": 0.21287199202619964,
      "grad_norm": 0.4060191512107849,
      "learning_rate": 0.0001858996153663517,
      "loss": 0.6778,
      "step": 14950
    },
    {
      "epoch": 0.21358393848782573,
      "grad_norm": 0.7057895064353943,
      "learning_rate": 0.00018585212973075646,
      "loss": 0.7323,
      "step": 15000
    },
    {
      "epoch": 0.21429588494945181,
      "grad_norm": 0.27572157979011536,
      "learning_rate": 0.00018580464409516124,
      "loss": 0.6381,
      "step": 15050
    },
    {
      "epoch": 0.21500783141107788,
      "grad_norm": 0.46923843026161194,
      "learning_rate": 0.000185757158459566,
      "loss": 0.7349,
      "step": 15100
    },
    {
      "epoch": 0.21571977787270397,
      "grad_norm": 0.4077560007572174,
      "learning_rate": 0.00018570967282397077,
      "loss": 0.6417,
      "step": 15150
    },
    {
      "epoch": 0.21643172433433006,
      "grad_norm": 0.3283257484436035,
      "learning_rate": 0.00018566218718837554,
      "loss": 0.6714,
      "step": 15200
    },
    {
      "epoch": 0.21714367079595615,
      "grad_norm": 0.5237439870834351,
      "learning_rate": 0.0001856147015527803,
      "loss": 0.6755,
      "step": 15250
    },
    {
      "epoch": 0.21785561725758223,
      "grad_norm": 0.38953250646591187,
      "learning_rate": 0.00018556721591718507,
      "loss": 0.6838,
      "step": 15300
    },
    {
      "epoch": 0.21856756371920832,
      "grad_norm": 0.5483748912811279,
      "learning_rate": 0.00018551973028158982,
      "loss": 0.67,
      "step": 15350
    },
    {
      "epoch": 0.2192795101808344,
      "grad_norm": 0.4352509677410126,
      "learning_rate": 0.0001854722446459946,
      "loss": 0.6667,
      "step": 15400
    },
    {
      "epoch": 0.21999145664246048,
      "grad_norm": 0.3311760127544403,
      "learning_rate": 0.00018542475901039938,
      "loss": 0.7306,
      "step": 15450
    },
    {
      "epoch": 0.22070340310408657,
      "grad_norm": 0.5866081714630127,
      "learning_rate": 0.00018537727337480413,
      "loss": 0.6749,
      "step": 15500
    },
    {
      "epoch": 0.22141534956571265,
      "grad_norm": 0.31051284074783325,
      "learning_rate": 0.0001853297877392089,
      "loss": 0.6919,
      "step": 15550
    },
    {
      "epoch": 0.22212729602733874,
      "grad_norm": 0.5353036522865295,
      "learning_rate": 0.00018528230210361366,
      "loss": 0.7,
      "step": 15600
    },
    {
      "epoch": 0.22283924248896483,
      "grad_norm": 0.5870132446289062,
      "learning_rate": 0.00018523481646801843,
      "loss": 0.7045,
      "step": 15650
    },
    {
      "epoch": 0.22355118895059092,
      "grad_norm": 0.5271260738372803,
      "learning_rate": 0.0001851873308324232,
      "loss": 0.6818,
      "step": 15700
    },
    {
      "epoch": 0.224263135412217,
      "grad_norm": 0.39142462611198425,
      "learning_rate": 0.00018513984519682796,
      "loss": 0.6908,
      "step": 15750
    },
    {
      "epoch": 0.22497508187384307,
      "grad_norm": 0.41151347756385803,
      "learning_rate": 0.00018509235956123274,
      "loss": 0.699,
      "step": 15800
    },
    {
      "epoch": 0.22568702833546916,
      "grad_norm": 0.4803644120693207,
      "learning_rate": 0.0001850448739256375,
      "loss": 0.6774,
      "step": 15850
    },
    {
      "epoch": 0.22639897479709525,
      "grad_norm": 0.696892499923706,
      "learning_rate": 0.00018499738829004227,
      "loss": 0.6585,
      "step": 15900
    },
    {
      "epoch": 0.22711092125872134,
      "grad_norm": 0.38917919993400574,
      "learning_rate": 0.00018494990265444705,
      "loss": 0.6797,
      "step": 15950
    },
    {
      "epoch": 0.22782286772034743,
      "grad_norm": 0.3891746699810028,
      "learning_rate": 0.00018490336673156372,
      "loss": 0.7143,
      "step": 16000
    },
    {
      "epoch": 0.22853481418197352,
      "grad_norm": 0.5741848349571228,
      "learning_rate": 0.00018485588109596848,
      "loss": 0.7156,
      "step": 16050
    },
    {
      "epoch": 0.2292467606435996,
      "grad_norm": 0.4243764281272888,
      "learning_rate": 0.00018480839546037325,
      "loss": 0.7239,
      "step": 16100
    },
    {
      "epoch": 0.22995870710522567,
      "grad_norm": 0.6491216421127319,
      "learning_rate": 0.000184760909824778,
      "loss": 0.6979,
      "step": 16150
    },
    {
      "epoch": 0.23067065356685176,
      "grad_norm": 0.7729814052581787,
      "learning_rate": 0.00018471342418918278,
      "loss": 0.6398,
      "step": 16200
    },
    {
      "epoch": 0.23138260002847785,
      "grad_norm": 0.43786096572875977,
      "learning_rate": 0.00018466593855358756,
      "loss": 0.6861,
      "step": 16250
    },
    {
      "epoch": 0.23209454649010394,
      "grad_norm": 0.42879191040992737,
      "learning_rate": 0.0001846184529179923,
      "loss": 0.6566,
      "step": 16300
    },
    {
      "epoch": 0.23280649295173003,
      "grad_norm": 0.6534227728843689,
      "learning_rate": 0.0001845709672823971,
      "loss": 0.6181,
      "step": 16350
    },
    {
      "epoch": 0.23351843941335612,
      "grad_norm": 0.5771490335464478,
      "learning_rate": 0.00018452348164680184,
      "loss": 0.7232,
      "step": 16400
    },
    {
      "epoch": 0.2342303858749822,
      "grad_norm": 0.6190285086631775,
      "learning_rate": 0.00018447599601120661,
      "loss": 0.6592,
      "step": 16450
    },
    {
      "epoch": 0.2349423323366083,
      "grad_norm": 0.7551806569099426,
      "learning_rate": 0.0001844285103756114,
      "loss": 0.7727,
      "step": 16500
    },
    {
      "epoch": 0.23565427879823436,
      "grad_norm": 0.7028316855430603,
      "learning_rate": 0.00018438102474001614,
      "loss": 0.691,
      "step": 16550
    },
    {
      "epoch": 0.23636622525986045,
      "grad_norm": 0.28612416982650757,
      "learning_rate": 0.00018433353910442092,
      "loss": 0.6703,
      "step": 16600
    },
    {
      "epoch": 0.23707817172148654,
      "grad_norm": 0.40149444341659546,
      "learning_rate": 0.00018428605346882567,
      "loss": 0.7095,
      "step": 16650
    },
    {
      "epoch": 0.23779011818311263,
      "grad_norm": 0.4482235610485077,
      "learning_rate": 0.00018423856783323045,
      "loss": 0.6655,
      "step": 16700
    },
    {
      "epoch": 0.23850206464473872,
      "grad_norm": 0.3703096807003021,
      "learning_rate": 0.00018419108219763523,
      "loss": 0.6799,
      "step": 16750
    },
    {
      "epoch": 0.2392140111063648,
      "grad_norm": 0.5712965726852417,
      "learning_rate": 0.00018414359656203998,
      "loss": 0.7051,
      "step": 16800
    },
    {
      "epoch": 0.2399259575679909,
      "grad_norm": 0.27843335270881653,
      "learning_rate": 0.00018409611092644475,
      "loss": 0.7304,
      "step": 16850
    },
    {
      "epoch": 0.24063790402961696,
      "grad_norm": 0.45866069197654724,
      "learning_rate": 0.00018404862529084953,
      "loss": 0.658,
      "step": 16900
    },
    {
      "epoch": 0.24134985049124305,
      "grad_norm": 0.6504546999931335,
      "learning_rate": 0.00018400113965525428,
      "loss": 0.6567,
      "step": 16950
    },
    {
      "epoch": 0.24206179695286914,
      "grad_norm": 0.653417706489563,
      "learning_rate": 0.00018395365401965906,
      "loss": 0.705,
      "step": 17000
    },
    {
      "epoch": 0.24277374341449523,
      "grad_norm": 0.48669204115867615,
      "learning_rate": 0.00018390616838406384,
      "loss": 0.7627,
      "step": 17050
    },
    {
      "epoch": 0.24348568987612132,
      "grad_norm": 0.5619635581970215,
      "learning_rate": 0.00018385868274846862,
      "loss": 0.7293,
      "step": 17100
    },
    {
      "epoch": 0.2441976363377474,
      "grad_norm": 0.4476282298564911,
      "learning_rate": 0.00018381119711287337,
      "loss": 0.7091,
      "step": 17150
    },
    {
      "epoch": 0.2449095827993735,
      "grad_norm": 0.5138367414474487,
      "learning_rate": 0.00018376371147727814,
      "loss": 0.6928,
      "step": 17200
    },
    {
      "epoch": 0.24562152926099956,
      "grad_norm": 0.4676365256309509,
      "learning_rate": 0.00018371622584168292,
      "loss": 0.6924,
      "step": 17250
    },
    {
      "epoch": 0.24633347572262565,
      "grad_norm": 0.6087572574615479,
      "learning_rate": 0.00018366874020608767,
      "loss": 0.6214,
      "step": 17300
    },
    {
      "epoch": 0.24704542218425174,
      "grad_norm": 0.4886969029903412,
      "learning_rate": 0.00018362125457049245,
      "loss": 0.7393,
      "step": 17350
    },
    {
      "epoch": 0.24775736864587783,
      "grad_norm": 0.4632793664932251,
      "learning_rate": 0.0001835737689348972,
      "loss": 0.7109,
      "step": 17400
    },
    {
      "epoch": 0.24846931510750392,
      "grad_norm": 0.43322744965553284,
      "learning_rate": 0.00018352628329930198,
      "loss": 0.6868,
      "step": 17450
    },
    {
      "epoch": 0.24918126156913,
      "grad_norm": 0.551471471786499,
      "learning_rate": 0.00018347879766370675,
      "loss": 0.6456,
      "step": 17500
    },
    {
      "epoch": 0.2498932080307561,
      "grad_norm": 0.5743646621704102,
      "learning_rate": 0.0001834313120281115,
      "loss": 0.6525,
      "step": 17550
    },
    {
      "epoch": 0.2506051544923822,
      "grad_norm": 0.4019286334514618,
      "learning_rate": 0.00018338382639251628,
      "loss": 0.7756,
      "step": 17600
    },
    {
      "epoch": 0.2513171009540083,
      "grad_norm": 0.40814879536628723,
      "learning_rate": 0.00018333634075692103,
      "loss": 0.651,
      "step": 17650
    },
    {
      "epoch": 0.25202904741563437,
      "grad_norm": 0.2988874912261963,
      "learning_rate": 0.0001832888551213258,
      "loss": 0.7038,
      "step": 17700
    },
    {
      "epoch": 0.25274099387726046,
      "grad_norm": 0.3843196928501129,
      "learning_rate": 0.0001832413694857306,
      "loss": 0.6772,
      "step": 17750
    },
    {
      "epoch": 0.2534529403388865,
      "grad_norm": 0.8815444111824036,
      "learning_rate": 0.00018319388385013534,
      "loss": 0.6889,
      "step": 17800
    },
    {
      "epoch": 0.2541648868005126,
      "grad_norm": 0.45768070220947266,
      "learning_rate": 0.00018314639821454012,
      "loss": 0.6169,
      "step": 17850
    },
    {
      "epoch": 0.25487683326213867,
      "grad_norm": 0.6381509304046631,
      "learning_rate": 0.00018309891257894487,
      "loss": 0.6672,
      "step": 17900
    },
    {
      "epoch": 0.25558877972376476,
      "grad_norm": 0.715805172920227,
      "learning_rate": 0.00018305142694334964,
      "loss": 0.6727,
      "step": 17950
    },
    {
      "epoch": 0.25630072618539085,
      "grad_norm": 0.6175369024276733,
      "learning_rate": 0.00018300394130775442,
      "loss": 0.5977,
      "step": 18000
    },
    {
      "epoch": 0.25701267264701694,
      "grad_norm": 0.4592760503292084,
      "learning_rate": 0.00018295645567215917,
      "loss": 0.6976,
      "step": 18050
    },
    {
      "epoch": 0.257724619108643,
      "grad_norm": 0.4429938495159149,
      "learning_rate": 0.00018290897003656395,
      "loss": 0.6761,
      "step": 18100
    },
    {
      "epoch": 0.2584365655702691,
      "grad_norm": 0.47039249539375305,
      "learning_rate": 0.0001828614844009687,
      "loss": 0.6918,
      "step": 18150
    },
    {
      "epoch": 0.2591485120318952,
      "grad_norm": 0.5192654728889465,
      "learning_rate": 0.00018281399876537348,
      "loss": 0.6339,
      "step": 18200
    },
    {
      "epoch": 0.2598604584935213,
      "grad_norm": 0.41772544384002686,
      "learning_rate": 0.00018276651312977826,
      "loss": 0.6909,
      "step": 18250
    },
    {
      "epoch": 0.2605724049551474,
      "grad_norm": 0.6619644165039062,
      "learning_rate": 0.000182719027494183,
      "loss": 0.6846,
      "step": 18300
    },
    {
      "epoch": 0.2612843514167735,
      "grad_norm": 0.4401281177997589,
      "learning_rate": 0.00018267154185858778,
      "loss": 0.6409,
      "step": 18350
    },
    {
      "epoch": 0.26199629787839956,
      "grad_norm": 0.6464541554450989,
      "learning_rate": 0.00018262405622299253,
      "loss": 0.7007,
      "step": 18400
    },
    {
      "epoch": 0.26270824434002565,
      "grad_norm": 0.41400739550590515,
      "learning_rate": 0.0001825765705873973,
      "loss": 0.7032,
      "step": 18450
    },
    {
      "epoch": 0.26342019080165174,
      "grad_norm": 0.2790846526622772,
      "learning_rate": 0.0001825290849518021,
      "loss": 0.6905,
      "step": 18500
    },
    {
      "epoch": 0.2641321372632778,
      "grad_norm": 0.3709394931793213,
      "learning_rate": 0.00018248159931620684,
      "loss": 0.6477,
      "step": 18550
    },
    {
      "epoch": 0.26484408372490387,
      "grad_norm": 0.522350013256073,
      "learning_rate": 0.00018243411368061162,
      "loss": 0.6844,
      "step": 18600
    },
    {
      "epoch": 0.26555603018652996,
      "grad_norm": 0.6687082648277283,
      "learning_rate": 0.0001823866280450164,
      "loss": 0.6863,
      "step": 18650
    },
    {
      "epoch": 0.26626797664815605,
      "grad_norm": 0.5376429557800293,
      "learning_rate": 0.00018233914240942117,
      "loss": 0.6432,
      "step": 18700
    },
    {
      "epoch": 0.26697992310978214,
      "grad_norm": 0.8377213478088379,
      "learning_rate": 0.00018229165677382592,
      "loss": 0.6954,
      "step": 18750
    },
    {
      "epoch": 0.2676918695714082,
      "grad_norm": 0.47426262497901917,
      "learning_rate": 0.0001822441711382307,
      "loss": 0.6857,
      "step": 18800
    },
    {
      "epoch": 0.2684038160330343,
      "grad_norm": 0.44115543365478516,
      "learning_rate": 0.00018219668550263548,
      "loss": 0.6385,
      "step": 18850
    },
    {
      "epoch": 0.2691157624946604,
      "grad_norm": 0.3798832595348358,
      "learning_rate": 0.00018214919986704023,
      "loss": 0.6802,
      "step": 18900
    },
    {
      "epoch": 0.2698277089562865,
      "grad_norm": 0.5850785374641418,
      "learning_rate": 0.00018210266394415688,
      "loss": 0.6673,
      "step": 18950
    },
    {
      "epoch": 0.2705396554179126,
      "grad_norm": 0.516544759273529,
      "learning_rate": 0.00018205517830856166,
      "loss": 0.6688,
      "step": 19000
    },
    {
      "epoch": 0.2712516018795387,
      "grad_norm": 0.47167083621025085,
      "learning_rate": 0.00018200769267296644,
      "loss": 0.6384,
      "step": 19050
    },
    {
      "epoch": 0.27196354834116476,
      "grad_norm": 0.6933400630950928,
      "learning_rate": 0.0001819602070373712,
      "loss": 0.6891,
      "step": 19100
    },
    {
      "epoch": 0.27267549480279085,
      "grad_norm": 0.6356776356697083,
      "learning_rate": 0.00018191272140177597,
      "loss": 0.6177,
      "step": 19150
    },
    {
      "epoch": 0.27338744126441694,
      "grad_norm": 0.5194046497344971,
      "learning_rate": 0.00018186523576618074,
      "loss": 0.6634,
      "step": 19200
    },
    {
      "epoch": 0.274099387726043,
      "grad_norm": 0.30321362614631653,
      "learning_rate": 0.00018181775013058552,
      "loss": 0.6893,
      "step": 19250
    },
    {
      "epoch": 0.27481133418766907,
      "grad_norm": 0.8568271398544312,
      "learning_rate": 0.00018177026449499027,
      "loss": 0.674,
      "step": 19300
    },
    {
      "epoch": 0.27552328064929515,
      "grad_norm": 0.4723086953163147,
      "learning_rate": 0.00018172277885939505,
      "loss": 0.6865,
      "step": 19350
    },
    {
      "epoch": 0.27623522711092124,
      "grad_norm": 0.6000940799713135,
      "learning_rate": 0.00018167529322379983,
      "loss": 0.702,
      "step": 19400
    },
    {
      "epoch": 0.27694717357254733,
      "grad_norm": 0.37829115986824036,
      "learning_rate": 0.00018162780758820458,
      "loss": 0.6862,
      "step": 19450
    },
    {
      "epoch": 0.2776591200341734,
      "grad_norm": 0.4011131227016449,
      "learning_rate": 0.00018158032195260935,
      "loss": 0.7245,
      "step": 19500
    },
    {
      "epoch": 0.2783710664957995,
      "grad_norm": 0.5295931100845337,
      "learning_rate": 0.00018153283631701413,
      "loss": 0.6834,
      "step": 19550
    },
    {
      "epoch": 0.2790830129574256,
      "grad_norm": 0.4819171726703644,
      "learning_rate": 0.00018148535068141888,
      "loss": 0.7655,
      "step": 19600
    },
    {
      "epoch": 0.2797949594190517,
      "grad_norm": 0.4770902395248413,
      "learning_rate": 0.00018143786504582366,
      "loss": 0.6891,
      "step": 19650
    },
    {
      "epoch": 0.2805069058806778,
      "grad_norm": 0.5151127576828003,
      "learning_rate": 0.0001813903794102284,
      "loss": 0.6886,
      "step": 19700
    },
    {
      "epoch": 0.28121885234230387,
      "grad_norm": 0.49227824807167053,
      "learning_rate": 0.0001813428937746332,
      "loss": 0.6696,
      "step": 19750
    },
    {
      "epoch": 0.28193079880392996,
      "grad_norm": 0.5398520827293396,
      "learning_rate": 0.00018129540813903797,
      "loss": 0.6912,
      "step": 19800
    },
    {
      "epoch": 0.28264274526555605,
      "grad_norm": 0.681896984577179,
      "learning_rate": 0.00018124792250344272,
      "loss": 0.6873,
      "step": 19850
    },
    {
      "epoch": 0.28335469172718214,
      "grad_norm": 0.9437415599822998,
      "learning_rate": 0.0001812004368678475,
      "loss": 0.6743,
      "step": 19900
    },
    {
      "epoch": 0.28406663818880823,
      "grad_norm": 0.5834245085716248,
      "learning_rate": 0.00018115295123225224,
      "loss": 0.6007,
      "step": 19950
    },
    {
      "epoch": 0.28477858465043426,
      "grad_norm": 0.7138566374778748,
      "learning_rate": 0.00018110546559665702,
      "loss": 0.6647,
      "step": 20000
    },
    {
      "epoch": 0.28549053111206035,
      "grad_norm": 0.6226940751075745,
      "learning_rate": 0.0001810579799610618,
      "loss": 0.6196,
      "step": 20050
    },
    {
      "epoch": 0.28620247757368644,
      "grad_norm": 0.45102494955062866,
      "learning_rate": 0.00018101049432546655,
      "loss": 0.6499,
      "step": 20100
    },
    {
      "epoch": 0.28691442403531253,
      "grad_norm": 0.6201118230819702,
      "learning_rate": 0.00018096300868987133,
      "loss": 0.659,
      "step": 20150
    },
    {
      "epoch": 0.2876263704969386,
      "grad_norm": 0.5991831421852112,
      "learning_rate": 0.00018091552305427608,
      "loss": 0.7191,
      "step": 20200
    },
    {
      "epoch": 0.2883383169585647,
      "grad_norm": 1.4246684312820435,
      "learning_rate": 0.00018086803741868086,
      "loss": 0.6909,
      "step": 20250
    },
    {
      "epoch": 0.2890502634201908,
      "grad_norm": 0.6339738965034485,
      "learning_rate": 0.00018082055178308563,
      "loss": 0.6195,
      "step": 20300
    },
    {
      "epoch": 0.2897622098818169,
      "grad_norm": 0.4584163725376129,
      "learning_rate": 0.00018077306614749038,
      "loss": 0.6639,
      "step": 20350
    },
    {
      "epoch": 0.290474156343443,
      "grad_norm": 0.47242558002471924,
      "learning_rate": 0.00018072558051189516,
      "loss": 0.7008,
      "step": 20400
    },
    {
      "epoch": 0.29118610280506907,
      "grad_norm": 0.8131493330001831,
      "learning_rate": 0.0001806780948762999,
      "loss": 0.6716,
      "step": 20450
    },
    {
      "epoch": 0.29189804926669516,
      "grad_norm": 0.6835311651229858,
      "learning_rate": 0.0001806306092407047,
      "loss": 0.7278,
      "step": 20500
    },
    {
      "epoch": 0.29260999572832125,
      "grad_norm": 0.5756648778915405,
      "learning_rate": 0.00018058312360510947,
      "loss": 0.7414,
      "step": 20550
    },
    {
      "epoch": 0.29332194218994734,
      "grad_norm": 0.36731642484664917,
      "learning_rate": 0.00018053563796951422,
      "loss": 0.6538,
      "step": 20600
    },
    {
      "epoch": 0.2940338886515734,
      "grad_norm": 0.577545702457428,
      "learning_rate": 0.000180488152333919,
      "loss": 0.7002,
      "step": 20650
    },
    {
      "epoch": 0.29474583511319946,
      "grad_norm": 0.3989127576351166,
      "learning_rate": 0.00018044066669832375,
      "loss": 0.7061,
      "step": 20700
    },
    {
      "epoch": 0.29545778157482555,
      "grad_norm": 0.7379161715507507,
      "learning_rate": 0.00018039318106272852,
      "loss": 0.6546,
      "step": 20750
    },
    {
      "epoch": 0.29616972803645164,
      "grad_norm": 0.6605389714241028,
      "learning_rate": 0.0001803456954271333,
      "loss": 0.7326,
      "step": 20800
    },
    {
      "epoch": 0.29688167449807773,
      "grad_norm": 0.4827878177165985,
      "learning_rate": 0.00018029820979153805,
      "loss": 0.7061,
      "step": 20850
    },
    {
      "epoch": 0.2975936209597038,
      "grad_norm": 0.3394770622253418,
      "learning_rate": 0.00018025072415594283,
      "loss": 0.6925,
      "step": 20900
    },
    {
      "epoch": 0.2983055674213299,
      "grad_norm": 0.5736395120620728,
      "learning_rate": 0.0001802032385203476,
      "loss": 0.7083,
      "step": 20950
    },
    {
      "epoch": 0.299017513882956,
      "grad_norm": 0.4580910801887512,
      "learning_rate": 0.00018015575288475238,
      "loss": 0.6875,
      "step": 21000
    },
    {
      "epoch": 0.2997294603445821,
      "grad_norm": 0.7596296668052673,
      "learning_rate": 0.00018010826724915713,
      "loss": 0.7358,
      "step": 21050
    },
    {
      "epoch": 0.3004414068062082,
      "grad_norm": 0.4404641091823578,
      "learning_rate": 0.0001800607816135619,
      "loss": 0.614,
      "step": 21100
    },
    {
      "epoch": 0.30115335326783427,
      "grad_norm": 0.45313653349876404,
      "learning_rate": 0.0001800132959779667,
      "loss": 0.6786,
      "step": 21150
    },
    {
      "epoch": 0.30186529972946036,
      "grad_norm": 0.6372213959693909,
      "learning_rate": 0.00017996581034237144,
      "loss": 0.6564,
      "step": 21200
    },
    {
      "epoch": 0.30257724619108645,
      "grad_norm": 0.43552741408348083,
      "learning_rate": 0.00017991832470677622,
      "loss": 0.6855,
      "step": 21250
    },
    {
      "epoch": 0.30328919265271254,
      "grad_norm": 0.49189332127571106,
      "learning_rate": 0.000179870839071181,
      "loss": 0.6982,
      "step": 21300
    },
    {
      "epoch": 0.3040011391143386,
      "grad_norm": 0.5557447075843811,
      "learning_rate": 0.00017982335343558575,
      "loss": 0.6726,
      "step": 21350
    },
    {
      "epoch": 0.3047130855759647,
      "grad_norm": 0.5212144255638123,
      "learning_rate": 0.00017977586779999052,
      "loss": 0.6362,
      "step": 21400
    },
    {
      "epoch": 0.30542503203759075,
      "grad_norm": 0.4989528954029083,
      "learning_rate": 0.00017972838216439527,
      "loss": 0.6566,
      "step": 21450
    },
    {
      "epoch": 0.30613697849921684,
      "grad_norm": 0.3800361752510071,
      "learning_rate": 0.00017968089652880005,
      "loss": 0.6789,
      "step": 21500
    },
    {
      "epoch": 0.30684892496084293,
      "grad_norm": 0.34524503350257874,
      "learning_rate": 0.00017963341089320483,
      "loss": 0.6234,
      "step": 21550
    },
    {
      "epoch": 0.307560871422469,
      "grad_norm": 0.4711267352104187,
      "learning_rate": 0.00017958592525760958,
      "loss": 0.6729,
      "step": 21600
    },
    {
      "epoch": 0.3082728178840951,
      "grad_norm": 0.46167871356010437,
      "learning_rate": 0.00017953843962201436,
      "loss": 0.6875,
      "step": 21650
    },
    {
      "epoch": 0.3089847643457212,
      "grad_norm": 0.5116742253303528,
      "learning_rate": 0.0001794909539864191,
      "loss": 0.7162,
      "step": 21700
    },
    {
      "epoch": 0.3096967108073473,
      "grad_norm": 0.5350607633590698,
      "learning_rate": 0.00017944346835082389,
      "loss": 0.6702,
      "step": 21750
    },
    {
      "epoch": 0.3104086572689734,
      "grad_norm": 0.48638689517974854,
      "learning_rate": 0.00017939598271522866,
      "loss": 0.704,
      "step": 21800
    },
    {
      "epoch": 0.31112060373059947,
      "grad_norm": 0.6561754941940308,
      "learning_rate": 0.00017934849707963341,
      "loss": 0.6656,
      "step": 21850
    },
    {
      "epoch": 0.31183255019222555,
      "grad_norm": 1.2250155210494995,
      "learning_rate": 0.0001793010114440382,
      "loss": 0.6904,
      "step": 21900
    },
    {
      "epoch": 0.31254449665385164,
      "grad_norm": 0.7260218262672424,
      "learning_rate": 0.00017925352580844294,
      "loss": 0.6466,
      "step": 21950
    },
    {
      "epoch": 0.31325644311547773,
      "grad_norm": 0.38013747334480286,
      "learning_rate": 0.00017920604017284772,
      "loss": 0.664,
      "step": 22000
    },
    {
      "epoch": 0.3139683895771038,
      "grad_norm": 0.5994366407394409,
      "learning_rate": 0.0001791585545372525,
      "loss": 0.716,
      "step": 22050
    },
    {
      "epoch": 0.3146803360387299,
      "grad_norm": 0.5244066119194031,
      "learning_rate": 0.00017911106890165725,
      "loss": 0.7061,
      "step": 22100
    },
    {
      "epoch": 0.31539228250035595,
      "grad_norm": 0.6918956637382507,
      "learning_rate": 0.00017906358326606203,
      "loss": 0.681,
      "step": 22150
    },
    {
      "epoch": 0.31610422896198204,
      "grad_norm": 0.8906545042991638,
      "learning_rate": 0.00017901609763046678,
      "loss": 0.6748,
      "step": 22200
    },
    {
      "epoch": 0.3168161754236081,
      "grad_norm": 0.481733500957489,
      "learning_rate": 0.00017896861199487155,
      "loss": 0.695,
      "step": 22250
    },
    {
      "epoch": 0.3175281218852342,
      "grad_norm": 0.4698660373687744,
      "learning_rate": 0.00017892112635927633,
      "loss": 0.6891,
      "step": 22300
    },
    {
      "epoch": 0.3182400683468603,
      "grad_norm": 0.4760347306728363,
      "learning_rate": 0.00017887364072368108,
      "loss": 0.6555,
      "step": 22350
    },
    {
      "epoch": 0.3189520148084864,
      "grad_norm": 0.4234459698200226,
      "learning_rate": 0.00017882615508808586,
      "loss": 0.6623,
      "step": 22400
    },
    {
      "epoch": 0.3196639612701125,
      "grad_norm": 0.6320436596870422,
      "learning_rate": 0.0001787786694524906,
      "loss": 0.7219,
      "step": 22450
    },
    {
      "epoch": 0.3203759077317386,
      "grad_norm": 0.4503026604652405,
      "learning_rate": 0.0001787311838168954,
      "loss": 0.7071,
      "step": 22500
    },
    {
      "epoch": 0.32108785419336466,
      "grad_norm": 0.4140237271785736,
      "learning_rate": 0.00017868369818130016,
      "loss": 0.6947,
      "step": 22550
    },
    {
      "epoch": 0.32179980065499075,
      "grad_norm": 0.42801085114479065,
      "learning_rate": 0.00017863621254570492,
      "loss": 0.697,
      "step": 22600
    },
    {
      "epoch": 0.32251174711661684,
      "grad_norm": 0.3973206877708435,
      "learning_rate": 0.0001785887269101097,
      "loss": 0.6166,
      "step": 22650
    },
    {
      "epoch": 0.32322369357824293,
      "grad_norm": 0.42642027139663696,
      "learning_rate": 0.00017854124127451447,
      "loss": 0.6598,
      "step": 22700
    },
    {
      "epoch": 0.323935640039869,
      "grad_norm": 0.859296441078186,
      "learning_rate": 0.00017849375563891925,
      "loss": 0.7458,
      "step": 22750
    },
    {
      "epoch": 0.3246475865014951,
      "grad_norm": 0.3066715598106384,
      "learning_rate": 0.000178446270003324,
      "loss": 0.7124,
      "step": 22800
    },
    {
      "epoch": 0.3253595329631212,
      "grad_norm": 0.4826236367225647,
      "learning_rate": 0.00017839878436772878,
      "loss": 0.6877,
      "step": 22850
    },
    {
      "epoch": 0.32607147942474723,
      "grad_norm": 0.42270976305007935,
      "learning_rate": 0.00017835129873213355,
      "loss": 0.7363,
      "step": 22900
    },
    {
      "epoch": 0.3267834258863733,
      "grad_norm": 0.4723404049873352,
      "learning_rate": 0.0001783038130965383,
      "loss": 0.6806,
      "step": 22950
    },
    {
      "epoch": 0.3274953723479994,
      "grad_norm": 0.5869237184524536,
      "learning_rate": 0.00017825632746094308,
      "loss": 0.6659,
      "step": 23000
    },
    {
      "epoch": 0.3282073188096255,
      "grad_norm": 0.5023677349090576,
      "learning_rate": 0.00017820884182534786,
      "loss": 0.728,
      "step": 23050
    },
    {
      "epoch": 0.3289192652712516,
      "grad_norm": 0.35767805576324463,
      "learning_rate": 0.0001781613561897526,
      "loss": 0.6148,
      "step": 23100
    },
    {
      "epoch": 0.3296312117328777,
      "grad_norm": 0.44000479578971863,
      "learning_rate": 0.0001781138705541574,
      "loss": 0.6589,
      "step": 23150
    },
    {
      "epoch": 0.33034315819450377,
      "grad_norm": 0.4204387962818146,
      "learning_rate": 0.00017806638491856214,
      "loss": 0.6735,
      "step": 23200
    },
    {
      "epoch": 0.33105510465612986,
      "grad_norm": 0.4730833172798157,
      "learning_rate": 0.00017801889928296692,
      "loss": 0.6679,
      "step": 23250
    },
    {
      "epoch": 0.33176705111775595,
      "grad_norm": 0.7453460097312927,
      "learning_rate": 0.0001779723633600836,
      "loss": 0.7162,
      "step": 23300
    },
    {
      "epoch": 0.33247899757938204,
      "grad_norm": 0.6202524900436401,
      "learning_rate": 0.00017792487772448837,
      "loss": 0.7063,
      "step": 23350
    },
    {
      "epoch": 0.33319094404100813,
      "grad_norm": 1.2839281558990479,
      "learning_rate": 0.00017787739208889312,
      "loss": 0.7047,
      "step": 23400
    },
    {
      "epoch": 0.3339028905026342,
      "grad_norm": 0.6207451820373535,
      "learning_rate": 0.0001778299064532979,
      "loss": 0.7503,
      "step": 23450
    },
    {
      "epoch": 0.3346148369642603,
      "grad_norm": 0.37251535058021545,
      "learning_rate": 0.00017778242081770265,
      "loss": 0.6798,
      "step": 23500
    },
    {
      "epoch": 0.3353267834258864,
      "grad_norm": 0.29692041873931885,
      "learning_rate": 0.00017773493518210743,
      "loss": 0.7279,
      "step": 23550
    },
    {
      "epoch": 0.33603872988751243,
      "grad_norm": 0.5539446473121643,
      "learning_rate": 0.0001776874495465122,
      "loss": 0.6904,
      "step": 23600
    },
    {
      "epoch": 0.3367506763491385,
      "grad_norm": 0.4667747914791107,
      "learning_rate": 0.00017763996391091696,
      "loss": 0.6208,
      "step": 23650
    },
    {
      "epoch": 0.3374626228107646,
      "grad_norm": 0.5555427074432373,
      "learning_rate": 0.00017759247827532173,
      "loss": 0.6983,
      "step": 23700
    },
    {
      "epoch": 0.3381745692723907,
      "grad_norm": 0.5624998807907104,
      "learning_rate": 0.00017754499263972649,
      "loss": 0.625,
      "step": 23750
    },
    {
      "epoch": 0.3388865157340168,
      "grad_norm": 0.4272076487541199,
      "learning_rate": 0.00017749750700413126,
      "loss": 0.6827,
      "step": 23800
    },
    {
      "epoch": 0.3395984621956429,
      "grad_norm": 0.4133290648460388,
      "learning_rate": 0.00017745002136853604,
      "loss": 0.6882,
      "step": 23850
    },
    {
      "epoch": 0.34031040865726897,
      "grad_norm": 0.3193897008895874,
      "learning_rate": 0.0001774025357329408,
      "loss": 0.7288,
      "step": 23900
    },
    {
      "epoch": 0.34102235511889506,
      "grad_norm": 0.45887991786003113,
      "learning_rate": 0.00017735599981005747,
      "loss": 0.6501,
      "step": 23950
    },
    {
      "epoch": 0.34173430158052115,
      "grad_norm": 0.5700291395187378,
      "learning_rate": 0.00017730851417446225,
      "loss": 0.6666,
      "step": 24000
    },
    {
      "epoch": 0.34244624804214724,
      "grad_norm": 0.6893838047981262,
      "learning_rate": 0.000177261028538867,
      "loss": 0.6921,
      "step": 24050
    },
    {
      "epoch": 0.34315819450377333,
      "grad_norm": 0.4576588273048401,
      "learning_rate": 0.00017721354290327178,
      "loss": 0.7005,
      "step": 24100
    },
    {
      "epoch": 0.3438701409653994,
      "grad_norm": 0.5025978684425354,
      "learning_rate": 0.00017716605726767655,
      "loss": 0.7259,
      "step": 24150
    },
    {
      "epoch": 0.3445820874270255,
      "grad_norm": 0.40607622265815735,
      "learning_rate": 0.0001771185716320813,
      "loss": 0.6869,
      "step": 24200
    },
    {
      "epoch": 0.3452940338886516,
      "grad_norm": 0.51070636510849,
      "learning_rate": 0.00017707108599648608,
      "loss": 0.7196,
      "step": 24250
    },
    {
      "epoch": 0.3460059803502777,
      "grad_norm": 0.4758355915546417,
      "learning_rate": 0.00017702360036089083,
      "loss": 0.6392,
      "step": 24300
    },
    {
      "epoch": 0.3467179268119037,
      "grad_norm": 0.30354321002960205,
      "learning_rate": 0.0001769761147252956,
      "loss": 0.6103,
      "step": 24350
    },
    {
      "epoch": 0.3474298732735298,
      "grad_norm": 0.9817876219749451,
      "learning_rate": 0.0001769286290897004,
      "loss": 0.7042,
      "step": 24400
    },
    {
      "epoch": 0.3481418197351559,
      "grad_norm": 0.4009197950363159,
      "learning_rate": 0.00017688114345410514,
      "loss": 0.6407,
      "step": 24450
    },
    {
      "epoch": 0.348853766196782,
      "grad_norm": 0.6697973608970642,
      "learning_rate": 0.00017683365781850992,
      "loss": 0.6857,
      "step": 24500
    },
    {
      "epoch": 0.3495657126584081,
      "grad_norm": 0.6171991229057312,
      "learning_rate": 0.00017678617218291467,
      "loss": 0.6739,
      "step": 24550
    },
    {
      "epoch": 0.35027765912003417,
      "grad_norm": 0.5300775766372681,
      "learning_rate": 0.00017673868654731944,
      "loss": 0.6718,
      "step": 24600
    },
    {
      "epoch": 0.35098960558166026,
      "grad_norm": 0.5420801639556885,
      "learning_rate": 0.00017669120091172422,
      "loss": 0.6737,
      "step": 24650
    },
    {
      "epoch": 0.35170155204328635,
      "grad_norm": 0.4644017219543457,
      "learning_rate": 0.00017664371527612897,
      "loss": 0.6952,
      "step": 24700
    },
    {
      "epoch": 0.35241349850491244,
      "grad_norm": 0.9816707968711853,
      "learning_rate": 0.00017659622964053375,
      "loss": 0.6428,
      "step": 24750
    },
    {
      "epoch": 0.3531254449665385,
      "grad_norm": 0.4243690073490143,
      "learning_rate": 0.0001765487440049385,
      "loss": 0.7222,
      "step": 24800
    },
    {
      "epoch": 0.3538373914281646,
      "grad_norm": 0.5580642819404602,
      "learning_rate": 0.00017650125836934328,
      "loss": 0.6829,
      "step": 24850
    },
    {
      "epoch": 0.3545493378897907,
      "grad_norm": 0.5216699838638306,
      "learning_rate": 0.00017645377273374805,
      "loss": 0.6827,
      "step": 24900
    },
    {
      "epoch": 0.3552612843514168,
      "grad_norm": 0.38392388820648193,
      "learning_rate": 0.0001764062870981528,
      "loss": 0.669,
      "step": 24950
    },
    {
      "epoch": 0.3559732308130429,
      "grad_norm": 0.6404458284378052,
      "learning_rate": 0.00017635880146255758,
      "loss": 0.6797,
      "step": 25000
    },
    {
      "epoch": 0.3566851772746689,
      "grad_norm": 0.6057348847389221,
      "learning_rate": 0.00017631131582696233,
      "loss": 0.6925,
      "step": 25050
    },
    {
      "epoch": 0.357397123736295,
      "grad_norm": 0.6433330178260803,
      "learning_rate": 0.0001762638301913671,
      "loss": 0.6846,
      "step": 25100
    },
    {
      "epoch": 0.3581090701979211,
      "grad_norm": 0.5746502876281738,
      "learning_rate": 0.0001762163445557719,
      "loss": 0.6678,
      "step": 25150
    },
    {
      "epoch": 0.3588210166595472,
      "grad_norm": 0.5633167028427124,
      "learning_rate": 0.00017616885892017664,
      "loss": 0.7049,
      "step": 25200
    },
    {
      "epoch": 0.3595329631211733,
      "grad_norm": 0.6341020464897156,
      "learning_rate": 0.00017612137328458142,
      "loss": 0.7136,
      "step": 25250
    },
    {
      "epoch": 0.36024490958279937,
      "grad_norm": 0.4083213210105896,
      "learning_rate": 0.00017607388764898617,
      "loss": 0.675,
      "step": 25300
    },
    {
      "epoch": 0.36095685604442546,
      "grad_norm": 1.0429779291152954,
      "learning_rate": 0.00017602640201339095,
      "loss": 0.676,
      "step": 25350
    },
    {
      "epoch": 0.36166880250605155,
      "grad_norm": 0.5658376812934875,
      "learning_rate": 0.00017597891637779572,
      "loss": 0.6755,
      "step": 25400
    },
    {
      "epoch": 0.36238074896767763,
      "grad_norm": 0.47311508655548096,
      "learning_rate": 0.0001759314307422005,
      "loss": 0.6471,
      "step": 25450
    },
    {
      "epoch": 0.3630926954293037,
      "grad_norm": 0.4728989005088806,
      "learning_rate": 0.00017588394510660525,
      "loss": 0.6554,
      "step": 25500
    },
    {
      "epoch": 0.3638046418909298,
      "grad_norm": 0.8142309784889221,
      "learning_rate": 0.00017583645947101003,
      "loss": 0.7141,
      "step": 25550
    },
    {
      "epoch": 0.3645165883525559,
      "grad_norm": 0.4905613362789154,
      "learning_rate": 0.0001757889738354148,
      "loss": 0.6562,
      "step": 25600
    },
    {
      "epoch": 0.365228534814182,
      "grad_norm": 0.3728856146335602,
      "learning_rate": 0.00017574148819981958,
      "loss": 0.6827,
      "step": 25650
    },
    {
      "epoch": 0.3659404812758081,
      "grad_norm": 0.5128461122512817,
      "learning_rate": 0.00017569400256422433,
      "loss": 0.6691,
      "step": 25700
    },
    {
      "epoch": 0.3666524277374342,
      "grad_norm": 0.44016268849372864,
      "learning_rate": 0.0001756465169286291,
      "loss": 0.6605,
      "step": 25750
    },
    {
      "epoch": 0.3673643741990602,
      "grad_norm": 0.2970273196697235,
      "learning_rate": 0.00017559903129303386,
      "loss": 0.7017,
      "step": 25800
    },
    {
      "epoch": 0.3680763206606863,
      "grad_norm": 0.48948121070861816,
      "learning_rate": 0.00017555154565743864,
      "loss": 0.6969,
      "step": 25850
    },
    {
      "epoch": 0.3687882671223124,
      "grad_norm": 0.578896701335907,
      "learning_rate": 0.00017550406002184342,
      "loss": 0.5981,
      "step": 25900
    },
    {
      "epoch": 0.3695002135839385,
      "grad_norm": 0.3758697509765625,
      "learning_rate": 0.00017545657438624817,
      "loss": 0.6717,
      "step": 25950
    },
    {
      "epoch": 0.37021216004556456,
      "grad_norm": 0.5706399083137512,
      "learning_rate": 0.00017540908875065295,
      "loss": 0.7252,
      "step": 26000
    },
    {
      "epoch": 0.37092410650719065,
      "grad_norm": 0.48694750666618347,
      "learning_rate": 0.0001753616031150577,
      "loss": 0.6909,
      "step": 26050
    },
    {
      "epoch": 0.37163605296881674,
      "grad_norm": 0.6386743783950806,
      "learning_rate": 0.00017531411747946247,
      "loss": 0.6813,
      "step": 26100
    },
    {
      "epoch": 0.37234799943044283,
      "grad_norm": 0.45603981614112854,
      "learning_rate": 0.00017526663184386725,
      "loss": 0.6172,
      "step": 26150
    },
    {
      "epoch": 0.3730599458920689,
      "grad_norm": 0.5416699647903442,
      "learning_rate": 0.000175219146208272,
      "loss": 0.6519,
      "step": 26200
    },
    {
      "epoch": 0.373771892353695,
      "grad_norm": 0.47396281361579895,
      "learning_rate": 0.00017517166057267678,
      "loss": 0.6687,
      "step": 26250
    },
    {
      "epoch": 0.3744838388153211,
      "grad_norm": 0.41431617736816406,
      "learning_rate": 0.00017512417493708153,
      "loss": 0.7051,
      "step": 26300
    },
    {
      "epoch": 0.3751957852769472,
      "grad_norm": 0.39554157853126526,
      "learning_rate": 0.0001750766893014863,
      "loss": 0.7018,
      "step": 26350
    },
    {
      "epoch": 0.3759077317385733,
      "grad_norm": 0.5423676371574402,
      "learning_rate": 0.00017502920366589109,
      "loss": 0.6593,
      "step": 26400
    },
    {
      "epoch": 0.37661967820019937,
      "grad_norm": 0.47774937748908997,
      "learning_rate": 0.00017498171803029584,
      "loss": 0.6842,
      "step": 26450
    },
    {
      "epoch": 0.3773316246618254,
      "grad_norm": 0.2671537399291992,
      "learning_rate": 0.0001749342323947006,
      "loss": 0.6657,
      "step": 26500
    },
    {
      "epoch": 0.3780435711234515,
      "grad_norm": 0.32470938563346863,
      "learning_rate": 0.00017488674675910536,
      "loss": 0.641,
      "step": 26550
    },
    {
      "epoch": 0.3787555175850776,
      "grad_norm": 0.959954023361206,
      "learning_rate": 0.00017483926112351014,
      "loss": 0.735,
      "step": 26600
    },
    {
      "epoch": 0.3794674640467037,
      "grad_norm": 0.6958151459693909,
      "learning_rate": 0.00017479177548791492,
      "loss": 0.6419,
      "step": 26650
    },
    {
      "epoch": 0.38017941050832976,
      "grad_norm": 0.7249197363853455,
      "learning_rate": 0.00017474428985231967,
      "loss": 0.6308,
      "step": 26700
    },
    {
      "epoch": 0.38089135696995585,
      "grad_norm": 0.4213898777961731,
      "learning_rate": 0.00017469680421672445,
      "loss": 0.6851,
      "step": 26750
    },
    {
      "epoch": 0.38160330343158194,
      "grad_norm": 0.38500091433525085,
      "learning_rate": 0.0001746493185811292,
      "loss": 0.7058,
      "step": 26800
    },
    {
      "epoch": 0.38231524989320803,
      "grad_norm": 0.6286186575889587,
      "learning_rate": 0.00017460183294553398,
      "loss": 0.7076,
      "step": 26850
    },
    {
      "epoch": 0.3830271963548341,
      "grad_norm": 0.5713860988616943,
      "learning_rate": 0.00017455434730993875,
      "loss": 0.6615,
      "step": 26900
    },
    {
      "epoch": 0.3837391428164602,
      "grad_norm": 0.5961173176765442,
      "learning_rate": 0.0001745068616743435,
      "loss": 0.6687,
      "step": 26950
    },
    {
      "epoch": 0.3844510892780863,
      "grad_norm": 0.7984665036201477,
      "learning_rate": 0.00017445937603874828,
      "loss": 0.6443,
      "step": 27000
    },
    {
      "epoch": 0.3851630357397124,
      "grad_norm": 0.49747857451438904,
      "learning_rate": 0.00017441189040315303,
      "loss": 0.6823,
      "step": 27050
    },
    {
      "epoch": 0.3858749822013385,
      "grad_norm": 0.7397565841674805,
      "learning_rate": 0.0001743644047675578,
      "loss": 0.7067,
      "step": 27100
    },
    {
      "epoch": 0.38658692866296457,
      "grad_norm": 0.5509675741195679,
      "learning_rate": 0.0001743169191319626,
      "loss": 0.6927,
      "step": 27150
    },
    {
      "epoch": 0.38729887512459066,
      "grad_norm": 0.6285383701324463,
      "learning_rate": 0.00017426943349636736,
      "loss": 0.6261,
      "step": 27200
    },
    {
      "epoch": 0.3880108215862167,
      "grad_norm": 0.5582692623138428,
      "learning_rate": 0.00017422194786077211,
      "loss": 0.6946,
      "step": 27250
    },
    {
      "epoch": 0.3887227680478428,
      "grad_norm": 0.33479753136634827,
      "learning_rate": 0.0001741744622251769,
      "loss": 0.6749,
      "step": 27300
    },
    {
      "epoch": 0.38943471450946887,
      "grad_norm": 0.4040103852748871,
      "learning_rate": 0.00017412697658958167,
      "loss": 0.6512,
      "step": 27350
    },
    {
      "epoch": 0.39014666097109496,
      "grad_norm": 0.3852396011352539,
      "learning_rate": 0.00017407949095398645,
      "loss": 0.6565,
      "step": 27400
    },
    {
      "epoch": 0.39085860743272105,
      "grad_norm": 0.6498516201972961,
      "learning_rate": 0.0001740320053183912,
      "loss": 0.6213,
      "step": 27450
    },
    {
      "epoch": 0.39157055389434714,
      "grad_norm": 0.6533379554748535,
      "learning_rate": 0.00017398451968279598,
      "loss": 0.6985,
      "step": 27500
    },
    {
      "epoch": 0.39228250035597323,
      "grad_norm": 0.41278401017189026,
      "learning_rate": 0.00017393703404720073,
      "loss": 0.6793,
      "step": 27550
    },
    {
      "epoch": 0.3929944468175993,
      "grad_norm": 0.4931855797767639,
      "learning_rate": 0.0001738895484116055,
      "loss": 0.6382,
      "step": 27600
    },
    {
      "epoch": 0.3937063932792254,
      "grad_norm": 0.6899877786636353,
      "learning_rate": 0.00017384206277601028,
      "loss": 0.6877,
      "step": 27650
    },
    {
      "epoch": 0.3944183397408515,
      "grad_norm": 0.9682238101959229,
      "learning_rate": 0.00017379457714041503,
      "loss": 0.6627,
      "step": 27700
    },
    {
      "epoch": 0.3951302862024776,
      "grad_norm": 0.30784082412719727,
      "learning_rate": 0.0001737470915048198,
      "loss": 0.6572,
      "step": 27750
    },
    {
      "epoch": 0.3958422326641037,
      "grad_norm": 0.7177991271018982,
      "learning_rate": 0.00017369960586922456,
      "loss": 0.7179,
      "step": 27800
    },
    {
      "epoch": 0.39655417912572977,
      "grad_norm": 0.5005647540092468,
      "learning_rate": 0.00017365212023362934,
      "loss": 0.6987,
      "step": 27850
    },
    {
      "epoch": 0.39726612558735586,
      "grad_norm": 0.5427362322807312,
      "learning_rate": 0.00017360463459803412,
      "loss": 0.7141,
      "step": 27900
    },
    {
      "epoch": 0.3979780720489819,
      "grad_norm": 0.6593919396400452,
      "learning_rate": 0.0001735580986751508,
      "loss": 0.6554,
      "step": 27950
    },
    {
      "epoch": 0.398690018510608,
      "grad_norm": 0.44003796577453613,
      "learning_rate": 0.00017351061303955554,
      "loss": 0.6461,
      "step": 28000
    },
    {
      "epoch": 0.39940196497223407,
      "grad_norm": 0.38952675461769104,
      "learning_rate": 0.00017346312740396032,
      "loss": 0.6371,
      "step": 28050
    },
    {
      "epoch": 0.40011391143386016,
      "grad_norm": 0.4717070460319519,
      "learning_rate": 0.00017341564176836507,
      "loss": 0.6622,
      "step": 28100
    },
    {
      "epoch": 0.40082585789548625,
      "grad_norm": 0.6306685209274292,
      "learning_rate": 0.00017336815613276985,
      "loss": 0.6988,
      "step": 28150
    },
    {
      "epoch": 0.40153780435711234,
      "grad_norm": 0.3544526994228363,
      "learning_rate": 0.00017332067049717463,
      "loss": 0.6616,
      "step": 28200
    },
    {
      "epoch": 0.4022497508187384,
      "grad_norm": 0.6826235055923462,
      "learning_rate": 0.00017327318486157938,
      "loss": 0.6353,
      "step": 28250
    },
    {
      "epoch": 0.4029616972803645,
      "grad_norm": 0.6330058574676514,
      "learning_rate": 0.00017322569922598416,
      "loss": 0.6293,
      "step": 28300
    },
    {
      "epoch": 0.4036736437419906,
      "grad_norm": 0.34218883514404297,
      "learning_rate": 0.0001731782135903889,
      "loss": 0.6752,
      "step": 28350
    },
    {
      "epoch": 0.4043855902036167,
      "grad_norm": 0.48135265707969666,
      "learning_rate": 0.00017313072795479368,
      "loss": 0.6724,
      "step": 28400
    },
    {
      "epoch": 0.4050975366652428,
      "grad_norm": 0.4622671604156494,
      "learning_rate": 0.00017308324231919846,
      "loss": 0.6536,
      "step": 28450
    },
    {
      "epoch": 0.4058094831268689,
      "grad_norm": 0.33293360471725464,
      "learning_rate": 0.0001730357566836032,
      "loss": 0.637,
      "step": 28500
    },
    {
      "epoch": 0.40652142958849496,
      "grad_norm": 0.4509107768535614,
      "learning_rate": 0.000172988271048008,
      "loss": 0.693,
      "step": 28550
    },
    {
      "epoch": 0.40723337605012105,
      "grad_norm": 0.541266918182373,
      "learning_rate": 0.00017294078541241274,
      "loss": 0.6904,
      "step": 28600
    },
    {
      "epoch": 0.40794532251174714,
      "grad_norm": 0.5767783522605896,
      "learning_rate": 0.00017289329977681752,
      "loss": 0.6564,
      "step": 28650
    },
    {
      "epoch": 0.4086572689733732,
      "grad_norm": 0.482087641954422,
      "learning_rate": 0.0001728458141412223,
      "loss": 0.6442,
      "step": 28700
    },
    {
      "epoch": 0.40936921543499927,
      "grad_norm": 0.6336058378219604,
      "learning_rate": 0.00017279832850562705,
      "loss": 0.6762,
      "step": 28750
    },
    {
      "epoch": 0.41008116189662536,
      "grad_norm": 0.695407509803772,
      "learning_rate": 0.00017275084287003182,
      "loss": 0.682,
      "step": 28800
    },
    {
      "epoch": 0.41079310835825145,
      "grad_norm": 0.37485817074775696,
      "learning_rate": 0.0001727043069471485,
      "loss": 0.7193,
      "step": 28850
    },
    {
      "epoch": 0.41150505481987754,
      "grad_norm": 0.4355666935443878,
      "learning_rate": 0.00017265682131155328,
      "loss": 0.6536,
      "step": 28900
    },
    {
      "epoch": 0.4122170012815036,
      "grad_norm": 0.43831905722618103,
      "learning_rate": 0.00017260933567595803,
      "loss": 0.7183,
      "step": 28950
    },
    {
      "epoch": 0.4129289477431297,
      "grad_norm": 0.43436798453330994,
      "learning_rate": 0.0001725618500403628,
      "loss": 0.6761,
      "step": 29000
    },
    {
      "epoch": 0.4136408942047558,
      "grad_norm": 0.46046891808509827,
      "learning_rate": 0.00017251436440476756,
      "loss": 0.6691,
      "step": 29050
    },
    {
      "epoch": 0.4143528406663819,
      "grad_norm": 0.6302428245544434,
      "learning_rate": 0.00017246687876917234,
      "loss": 0.6702,
      "step": 29100
    },
    {
      "epoch": 0.415064787128008,
      "grad_norm": 0.38608628511428833,
      "learning_rate": 0.00017241939313357711,
      "loss": 0.6033,
      "step": 29150
    },
    {
      "epoch": 0.4157767335896341,
      "grad_norm": 0.36609792709350586,
      "learning_rate": 0.00017237190749798187,
      "loss": 0.6844,
      "step": 29200
    },
    {
      "epoch": 0.41648868005126016,
      "grad_norm": 0.7176139950752258,
      "learning_rate": 0.00017232442186238664,
      "loss": 0.7062,
      "step": 29250
    },
    {
      "epoch": 0.41720062651288625,
      "grad_norm": 0.5256023406982422,
      "learning_rate": 0.0001722769362267914,
      "loss": 0.6917,
      "step": 29300
    },
    {
      "epoch": 0.41791257297451234,
      "grad_norm": 0.3334902226924896,
      "learning_rate": 0.00017222945059119617,
      "loss": 0.5968,
      "step": 29350
    },
    {
      "epoch": 0.4186245194361384,
      "grad_norm": 0.7096837759017944,
      "learning_rate": 0.00017218196495560095,
      "loss": 0.7038,
      "step": 29400
    },
    {
      "epoch": 0.41933646589776447,
      "grad_norm": 0.3264211118221283,
      "learning_rate": 0.0001721344793200057,
      "loss": 0.6487,
      "step": 29450
    },
    {
      "epoch": 0.42004841235939055,
      "grad_norm": 0.5797348022460938,
      "learning_rate": 0.00017208699368441048,
      "loss": 0.6717,
      "step": 29500
    },
    {
      "epoch": 0.42076035882101664,
      "grad_norm": 0.30693674087524414,
      "learning_rate": 0.00017203950804881523,
      "loss": 0.637,
      "step": 29550
    },
    {
      "epoch": 0.42147230528264273,
      "grad_norm": 0.4921700656414032,
      "learning_rate": 0.00017199202241322,
      "loss": 0.6277,
      "step": 29600
    },
    {
      "epoch": 0.4221842517442688,
      "grad_norm": 0.423771470785141,
      "learning_rate": 0.00017194453677762478,
      "loss": 0.6564,
      "step": 29650
    },
    {
      "epoch": 0.4228961982058949,
      "grad_norm": 0.6901205778121948,
      "learning_rate": 0.00017189705114202953,
      "loss": 0.7306,
      "step": 29700
    },
    {
      "epoch": 0.423608144667521,
      "grad_norm": 0.4837847352027893,
      "learning_rate": 0.0001718495655064343,
      "loss": 0.6786,
      "step": 29750
    },
    {
      "epoch": 0.4243200911291471,
      "grad_norm": 0.4919523298740387,
      "learning_rate": 0.00017180207987083906,
      "loss": 0.6916,
      "step": 29800
    },
    {
      "epoch": 0.4250320375907732,
      "grad_norm": 0.6853720545768738,
      "learning_rate": 0.00017175459423524384,
      "loss": 0.6742,
      "step": 29850
    },
    {
      "epoch": 0.42574398405239927,
      "grad_norm": 0.6091758012771606,
      "learning_rate": 0.00017170710859964862,
      "loss": 0.6966,
      "step": 29900
    },
    {
      "epoch": 0.42645593051402536,
      "grad_norm": 0.5885192155838013,
      "learning_rate": 0.00017165962296405337,
      "loss": 0.7094,
      "step": 29950
    },
    {
      "epoch": 0.42716787697565145,
      "grad_norm": 0.68942791223526,
      "learning_rate": 0.00017161213732845814,
      "loss": 0.6696,
      "step": 30000
    },
    {
      "epoch": 0.42787982343727754,
      "grad_norm": 0.6660544276237488,
      "learning_rate": 0.00017156465169286292,
      "loss": 0.6444,
      "step": 30050
    },
    {
      "epoch": 0.42859176989890363,
      "grad_norm": 0.8744890093803406,
      "learning_rate": 0.0001715171660572677,
      "loss": 0.7277,
      "step": 30100
    },
    {
      "epoch": 0.42930371636052966,
      "grad_norm": 0.43409401178359985,
      "learning_rate": 0.00017146968042167245,
      "loss": 0.6221,
      "step": 30150
    },
    {
      "epoch": 0.43001566282215575,
      "grad_norm": 0.42262911796569824,
      "learning_rate": 0.00017142219478607723,
      "loss": 0.6446,
      "step": 30200
    },
    {
      "epoch": 0.43072760928378184,
      "grad_norm": 0.37972885370254517,
      "learning_rate": 0.000171374709150482,
      "loss": 0.6212,
      "step": 30250
    },
    {
      "epoch": 0.43143955574540793,
      "grad_norm": 0.2988380491733551,
      "learning_rate": 0.00017132722351488676,
      "loss": 0.6341,
      "step": 30300
    },
    {
      "epoch": 0.432151502207034,
      "grad_norm": 0.5312715172767639,
      "learning_rate": 0.00017127973787929153,
      "loss": 0.582,
      "step": 30350
    },
    {
      "epoch": 0.4328634486686601,
      "grad_norm": 0.502945601940155,
      "learning_rate": 0.0001712322522436963,
      "loss": 0.6153,
      "step": 30400
    },
    {
      "epoch": 0.4335753951302862,
      "grad_norm": 0.3537924587726593,
      "learning_rate": 0.00017118476660810106,
      "loss": 0.6499,
      "step": 30450
    },
    {
      "epoch": 0.4342873415919123,
      "grad_norm": 0.4999769628047943,
      "learning_rate": 0.00017113728097250584,
      "loss": 0.6727,
      "step": 30500
    },
    {
      "epoch": 0.4349992880535384,
      "grad_norm": 0.39527323842048645,
      "learning_rate": 0.0001710897953369106,
      "loss": 0.6603,
      "step": 30550
    },
    {
      "epoch": 0.43571123451516447,
      "grad_norm": 0.5232540965080261,
      "learning_rate": 0.00017104230970131537,
      "loss": 0.6606,
      "step": 30600
    },
    {
      "epoch": 0.43642318097679056,
      "grad_norm": 0.6786646842956543,
      "learning_rate": 0.00017099482406572014,
      "loss": 0.6954,
      "step": 30650
    },
    {
      "epoch": 0.43713512743841665,
      "grad_norm": 0.541454017162323,
      "learning_rate": 0.0001709473384301249,
      "loss": 0.6709,
      "step": 30700
    },
    {
      "epoch": 0.43784707390004274,
      "grad_norm": 0.5408956408500671,
      "learning_rate": 0.00017089985279452967,
      "loss": 0.6782,
      "step": 30750
    },
    {
      "epoch": 0.4385590203616688,
      "grad_norm": 0.6717507243156433,
      "learning_rate": 0.00017085236715893442,
      "loss": 0.6182,
      "step": 30800
    },
    {
      "epoch": 0.43927096682329486,
      "grad_norm": 0.4324497580528259,
      "learning_rate": 0.0001708048815233392,
      "loss": 0.657,
      "step": 30850
    },
    {
      "epoch": 0.43998291328492095,
      "grad_norm": 0.4514576494693756,
      "learning_rate": 0.00017075739588774398,
      "loss": 0.6498,
      "step": 30900
    },
    {
      "epoch": 0.44069485974654704,
      "grad_norm": 0.5749064683914185,
      "learning_rate": 0.00017070991025214873,
      "loss": 0.7156,
      "step": 30950
    },
    {
      "epoch": 0.44140680620817313,
      "grad_norm": 0.5116904973983765,
      "learning_rate": 0.0001706624246165535,
      "loss": 0.6111,
      "step": 31000
    },
    {
      "epoch": 0.4421187526697992,
      "grad_norm": 0.5028579235076904,
      "learning_rate": 0.00017061493898095826,
      "loss": 0.6888,
      "step": 31050
    },
    {
      "epoch": 0.4428306991314253,
      "grad_norm": 0.4397398829460144,
      "learning_rate": 0.00017056745334536303,
      "loss": 0.6734,
      "step": 31100
    },
    {
      "epoch": 0.4435426455930514,
      "grad_norm": 0.4504433572292328,
      "learning_rate": 0.0001705199677097678,
      "loss": 0.659,
      "step": 31150
    },
    {
      "epoch": 0.4442545920546775,
      "grad_norm": 0.501086950302124,
      "learning_rate": 0.00017047248207417256,
      "loss": 0.6234,
      "step": 31200
    },
    {
      "epoch": 0.4449665385163036,
      "grad_norm": 0.6333891749382019,
      "learning_rate": 0.00017042499643857734,
      "loss": 0.6634,
      "step": 31250
    },
    {
      "epoch": 0.44567848497792967,
      "grad_norm": 0.44695618748664856,
      "learning_rate": 0.0001703775108029821,
      "loss": 0.7036,
      "step": 31300
    },
    {
      "epoch": 0.44639043143955576,
      "grad_norm": 0.46077629923820496,
      "learning_rate": 0.00017033002516738687,
      "loss": 0.6119,
      "step": 31350
    },
    {
      "epoch": 0.44710237790118185,
      "grad_norm": 0.4480903446674347,
      "learning_rate": 0.00017028253953179165,
      "loss": 0.6511,
      "step": 31400
    },
    {
      "epoch": 0.44781432436280794,
      "grad_norm": 0.514434814453125,
      "learning_rate": 0.0001702350538961964,
      "loss": 0.6578,
      "step": 31450
    },
    {
      "epoch": 0.448526270824434,
      "grad_norm": 0.8427043557167053,
      "learning_rate": 0.00017018756826060117,
      "loss": 0.6288,
      "step": 31500
    },
    {
      "epoch": 0.4492382172860601,
      "grad_norm": 0.5326997637748718,
      "learning_rate": 0.00017014008262500593,
      "loss": 0.6641,
      "step": 31550
    },
    {
      "epoch": 0.44995016374768615,
      "grad_norm": 0.40653517842292786,
      "learning_rate": 0.0001700925969894107,
      "loss": 0.6067,
      "step": 31600
    },
    {
      "epoch": 0.45066211020931224,
      "grad_norm": 0.6669105291366577,
      "learning_rate": 0.00017004511135381548,
      "loss": 0.6827,
      "step": 31650
    },
    {
      "epoch": 0.45137405667093833,
      "grad_norm": 0.3982217013835907,
      "learning_rate": 0.00016999762571822023,
      "loss": 0.6612,
      "step": 31700
    },
    {
      "epoch": 0.4520860031325644,
      "grad_norm": 0.41852667927742004,
      "learning_rate": 0.000169950140082625,
      "loss": 0.6199,
      "step": 31750
    },
    {
      "epoch": 0.4527979495941905,
      "grad_norm": 0.33039283752441406,
      "learning_rate": 0.00016990265444702979,
      "loss": 0.6716,
      "step": 31800
    },
    {
      "epoch": 0.4535098960558166,
      "grad_norm": 0.6046603322029114,
      "learning_rate": 0.00016985516881143456,
      "loss": 0.6328,
      "step": 31850
    },
    {
      "epoch": 0.4542218425174427,
      "grad_norm": 0.502120316028595,
      "learning_rate": 0.00016980768317583931,
      "loss": 0.7015,
      "step": 31900
    },
    {
      "epoch": 0.4549337889790688,
      "grad_norm": 0.41812625527381897,
      "learning_rate": 0.0001697601975402441,
      "loss": 0.688,
      "step": 31950
    },
    {
      "epoch": 0.45564573544069487,
      "grad_norm": 0.44411277770996094,
      "learning_rate": 0.00016971271190464887,
      "loss": 0.69,
      "step": 32000
    },
    {
      "epoch": 0.45635768190232096,
      "grad_norm": 0.6935537457466125,
      "learning_rate": 0.00016966522626905362,
      "loss": 0.6833,
      "step": 32050
    },
    {
      "epoch": 0.45706962836394704,
      "grad_norm": 0.5551621913909912,
      "learning_rate": 0.0001696177406334584,
      "loss": 0.6837,
      "step": 32100
    },
    {
      "epoch": 0.45778157482557313,
      "grad_norm": 0.4274217486381531,
      "learning_rate": 0.00016957025499786318,
      "loss": 0.6499,
      "step": 32150
    },
    {
      "epoch": 0.4584935212871992,
      "grad_norm": 0.5988304018974304,
      "learning_rate": 0.00016952276936226793,
      "loss": 0.6175,
      "step": 32200
    },
    {
      "epoch": 0.4592054677488253,
      "grad_norm": 0.511957585811615,
      "learning_rate": 0.0001694752837266727,
      "loss": 0.707,
      "step": 32250
    },
    {
      "epoch": 0.45991741421045135,
      "grad_norm": 0.4136187732219696,
      "learning_rate": 0.00016942779809107745,
      "loss": 0.6805,
      "step": 32300
    },
    {
      "epoch": 0.46062936067207744,
      "grad_norm": 0.4045735001564026,
      "learning_rate": 0.00016938031245548223,
      "loss": 0.6988,
      "step": 32350
    },
    {
      "epoch": 0.4613413071337035,
      "grad_norm": 0.3411802649497986,
      "learning_rate": 0.000169332826819887,
      "loss": 0.6805,
      "step": 32400
    },
    {
      "epoch": 0.4620532535953296,
      "grad_norm": 0.517874002456665,
      "learning_rate": 0.00016928534118429176,
      "loss": 0.6846,
      "step": 32450
    },
    {
      "epoch": 0.4627652000569557,
      "grad_norm": 0.660116970539093,
      "learning_rate": 0.00016923785554869654,
      "loss": 0.6575,
      "step": 32500
    },
    {
      "epoch": 0.4634771465185818,
      "grad_norm": 0.3547523021697998,
      "learning_rate": 0.0001691903699131013,
      "loss": 0.6763,
      "step": 32550
    },
    {
      "epoch": 0.4641890929802079,
      "grad_norm": 0.44331565499305725,
      "learning_rate": 0.00016914288427750607,
      "loss": 0.6594,
      "step": 32600
    },
    {
      "epoch": 0.464901039441834,
      "grad_norm": 0.6276004910469055,
      "learning_rate": 0.00016909539864191084,
      "loss": 0.7024,
      "step": 32650
    },
    {
      "epoch": 0.46561298590346006,
      "grad_norm": 0.5058104991912842,
      "learning_rate": 0.0001690479130063156,
      "loss": 0.6843,
      "step": 32700
    },
    {
      "epoch": 0.46632493236508615,
      "grad_norm": 0.7661280035972595,
      "learning_rate": 0.00016900042737072037,
      "loss": 0.6838,
      "step": 32750
    },
    {
      "epoch": 0.46703687882671224,
      "grad_norm": 0.43551355600357056,
      "learning_rate": 0.00016895294173512512,
      "loss": 0.6448,
      "step": 32800
    },
    {
      "epoch": 0.46774882528833833,
      "grad_norm": 0.6254216432571411,
      "learning_rate": 0.0001689054560995299,
      "loss": 0.6393,
      "step": 32850
    },
    {
      "epoch": 0.4684607717499644,
      "grad_norm": 0.6858239769935608,
      "learning_rate": 0.00016885892017664658,
      "loss": 0.6927,
      "step": 32900
    },
    {
      "epoch": 0.4691727182115905,
      "grad_norm": 0.5634241700172424,
      "learning_rate": 0.00016881143454105136,
      "loss": 0.7043,
      "step": 32950
    },
    {
      "epoch": 0.4698846646732166,
      "grad_norm": 0.43883833289146423,
      "learning_rate": 0.0001687639489054561,
      "loss": 0.6646,
      "step": 33000
    },
    {
      "epoch": 0.47059661113484264,
      "grad_norm": 0.5439863204956055,
      "learning_rate": 0.00016871646326986088,
      "loss": 0.6552,
      "step": 33050
    },
    {
      "epoch": 0.4713085575964687,
      "grad_norm": 0.5522611737251282,
      "learning_rate": 0.00016866897763426563,
      "loss": 0.6529,
      "step": 33100
    },
    {
      "epoch": 0.4720205040580948,
      "grad_norm": 0.4797861576080322,
      "learning_rate": 0.0001686214919986704,
      "loss": 0.6108,
      "step": 33150
    },
    {
      "epoch": 0.4727324505197209,
      "grad_norm": 0.4495415985584259,
      "learning_rate": 0.0001685740063630752,
      "loss": 0.7024,
      "step": 33200
    },
    {
      "epoch": 0.473444396981347,
      "grad_norm": 0.3539580702781677,
      "learning_rate": 0.00016852652072747994,
      "loss": 0.7441,
      "step": 33250
    },
    {
      "epoch": 0.4741563434429731,
      "grad_norm": 0.4850756525993347,
      "learning_rate": 0.00016847903509188472,
      "loss": 0.6264,
      "step": 33300
    },
    {
      "epoch": 0.4748682899045992,
      "grad_norm": 0.6282451748847961,
      "learning_rate": 0.00016843154945628947,
      "loss": 0.6158,
      "step": 33350
    },
    {
      "epoch": 0.47558023636622526,
      "grad_norm": 0.5658183693885803,
      "learning_rate": 0.00016838406382069425,
      "loss": 0.6929,
      "step": 33400
    },
    {
      "epoch": 0.47629218282785135,
      "grad_norm": 0.42175695300102234,
      "learning_rate": 0.00016833657818509902,
      "loss": 0.62,
      "step": 33450
    },
    {
      "epoch": 0.47700412928947744,
      "grad_norm": 0.4382644295692444,
      "learning_rate": 0.00016828909254950377,
      "loss": 0.6214,
      "step": 33500
    },
    {
      "epoch": 0.47771607575110353,
      "grad_norm": 0.3849340081214905,
      "learning_rate": 0.00016824160691390855,
      "loss": 0.6796,
      "step": 33550
    },
    {
      "epoch": 0.4784280222127296,
      "grad_norm": 0.8069576025009155,
      "learning_rate": 0.0001681941212783133,
      "loss": 0.6718,
      "step": 33600
    },
    {
      "epoch": 0.4791399686743557,
      "grad_norm": 0.4972994029521942,
      "learning_rate": 0.00016814663564271808,
      "loss": 0.6479,
      "step": 33650
    },
    {
      "epoch": 0.4798519151359818,
      "grad_norm": 0.5744326710700989,
      "learning_rate": 0.00016809915000712286,
      "loss": 0.7,
      "step": 33700
    },
    {
      "epoch": 0.48056386159760783,
      "grad_norm": 0.684194803237915,
      "learning_rate": 0.0001680516643715276,
      "loss": 0.6961,
      "step": 33750
    },
    {
      "epoch": 0.4812758080592339,
      "grad_norm": 0.4007270336151123,
      "learning_rate": 0.00016800417873593239,
      "loss": 0.6456,
      "step": 33800
    },
    {
      "epoch": 0.48198775452086,
      "grad_norm": 0.5711613893508911,
      "learning_rate": 0.00016795669310033714,
      "loss": 0.6819,
      "step": 33850
    },
    {
      "epoch": 0.4826997009824861,
      "grad_norm": 0.6477279663085938,
      "learning_rate": 0.00016790920746474191,
      "loss": 0.6802,
      "step": 33900
    },
    {
      "epoch": 0.4834116474441122,
      "grad_norm": 0.5112445950508118,
      "learning_rate": 0.0001678617218291467,
      "loss": 0.7009,
      "step": 33950
    },
    {
      "epoch": 0.4841235939057383,
      "grad_norm": 0.3361683189868927,
      "learning_rate": 0.00016781423619355144,
      "loss": 0.6901,
      "step": 34000
    },
    {
      "epoch": 0.48483554036736437,
      "grad_norm": 0.48025572299957275,
      "learning_rate": 0.00016776675055795622,
      "loss": 0.6413,
      "step": 34050
    },
    {
      "epoch": 0.48554748682899046,
      "grad_norm": 0.6408604383468628,
      "learning_rate": 0.000167719264922361,
      "loss": 0.7078,
      "step": 34100
    },
    {
      "epoch": 0.48625943329061655,
      "grad_norm": 0.6315706372261047,
      "learning_rate": 0.00016767177928676577,
      "loss": 0.6633,
      "step": 34150
    },
    {
      "epoch": 0.48697137975224264,
      "grad_norm": 0.4316770136356354,
      "learning_rate": 0.00016762429365117055,
      "loss": 0.7218,
      "step": 34200
    },
    {
      "epoch": 0.48768332621386873,
      "grad_norm": 0.4580797851085663,
      "learning_rate": 0.0001675768080155753,
      "loss": 0.6883,
      "step": 34250
    },
    {
      "epoch": 0.4883952726754948,
      "grad_norm": 0.6028767228126526,
      "learning_rate": 0.00016752932237998008,
      "loss": 0.648,
      "step": 34300
    },
    {
      "epoch": 0.4891072191371209,
      "grad_norm": 0.8825786709785461,
      "learning_rate": 0.00016748183674438483,
      "loss": 0.6971,
      "step": 34350
    },
    {
      "epoch": 0.489819165598747,
      "grad_norm": 0.47819894552230835,
      "learning_rate": 0.0001674343511087896,
      "loss": 0.6885,
      "step": 34400
    },
    {
      "epoch": 0.4905311120603731,
      "grad_norm": 0.603828489780426,
      "learning_rate": 0.00016738686547319439,
      "loss": 0.6467,
      "step": 34450
    },
    {
      "epoch": 0.4912430585219991,
      "grad_norm": 0.4696390926837921,
      "learning_rate": 0.00016733937983759914,
      "loss": 0.7307,
      "step": 34500
    },
    {
      "epoch": 0.4919550049836252,
      "grad_norm": 0.5226401090621948,
      "learning_rate": 0.00016729189420200391,
      "loss": 0.6553,
      "step": 34550
    },
    {
      "epoch": 0.4926669514452513,
      "grad_norm": 0.6403241157531738,
      "learning_rate": 0.00016724440856640866,
      "loss": 0.7171,
      "step": 34600
    },
    {
      "epoch": 0.4933788979068774,
      "grad_norm": 0.5430486798286438,
      "learning_rate": 0.00016719692293081344,
      "loss": 0.6672,
      "step": 34650
    },
    {
      "epoch": 0.4940908443685035,
      "grad_norm": 0.31177863478660583,
      "learning_rate": 0.00016714943729521822,
      "loss": 0.6745,
      "step": 34700
    },
    {
      "epoch": 0.49480279083012957,
      "grad_norm": 0.47431662678718567,
      "learning_rate": 0.00016710195165962297,
      "loss": 0.6676,
      "step": 34750
    },
    {
      "epoch": 0.49551473729175566,
      "grad_norm": 0.46096163988113403,
      "learning_rate": 0.00016705446602402775,
      "loss": 0.69,
      "step": 34800
    },
    {
      "epoch": 0.49622668375338175,
      "grad_norm": 0.5237134099006653,
      "learning_rate": 0.0001670069803884325,
      "loss": 0.6739,
      "step": 34850
    },
    {
      "epoch": 0.49693863021500784,
      "grad_norm": 0.6905804872512817,
      "learning_rate": 0.00016695949475283728,
      "loss": 0.656,
      "step": 34900
    },
    {
      "epoch": 0.4976505766766339,
      "grad_norm": 0.51341313123703,
      "learning_rate": 0.00016691200911724205,
      "loss": 0.6789,
      "step": 34950
    },
    {
      "epoch": 0.49836252313826,
      "grad_norm": 0.4826664626598358,
      "learning_rate": 0.0001668645234816468,
      "loss": 0.7169,
      "step": 35000
    },
    {
      "epoch": 0.4990744695998861,
      "grad_norm": 0.8408471941947937,
      "learning_rate": 0.00016681703784605158,
      "loss": 0.6744,
      "step": 35050
    },
    {
      "epoch": 0.4997864160615122,
      "grad_norm": 0.5215728282928467,
      "learning_rate": 0.00016677050192316826,
      "loss": 0.7361,
      "step": 35100
    },
    {
      "epoch": 0.5004983625231383,
      "grad_norm": 0.4175398647785187,
      "learning_rate": 0.000166723016287573,
      "loss": 0.6957,
      "step": 35150
    },
    {
      "epoch": 0.5012103089847644,
      "grad_norm": 0.6240365505218506,
      "learning_rate": 0.0001666755306519778,
      "loss": 0.6904,
      "step": 35200
    },
    {
      "epoch": 0.5019222554463905,
      "grad_norm": 0.41373395919799805,
      "learning_rate": 0.00016662804501638257,
      "loss": 0.6854,
      "step": 35250
    },
    {
      "epoch": 0.5026342019080166,
      "grad_norm": 0.7640188932418823,
      "learning_rate": 0.00016658055938078732,
      "loss": 0.7567,
      "step": 35300
    },
    {
      "epoch": 0.5033461483696426,
      "grad_norm": 0.3872212767601013,
      "learning_rate": 0.0001665330737451921,
      "loss": 0.658,
      "step": 35350
    },
    {
      "epoch": 0.5040580948312687,
      "grad_norm": 0.4201212227344513,
      "learning_rate": 0.00016648558810959685,
      "loss": 0.665,
      "step": 35400
    },
    {
      "epoch": 0.5047700412928948,
      "grad_norm": 0.5318402051925659,
      "learning_rate": 0.00016643810247400162,
      "loss": 0.6904,
      "step": 35450
    },
    {
      "epoch": 0.5054819877545209,
      "grad_norm": 0.49608275294303894,
      "learning_rate": 0.0001663906168384064,
      "loss": 0.7027,
      "step": 35500
    },
    {
      "epoch": 0.506193934216147,
      "grad_norm": 0.7011944055557251,
      "learning_rate": 0.00016634313120281115,
      "loss": 0.6208,
      "step": 35550
    },
    {
      "epoch": 0.506905880677773,
      "grad_norm": 0.5838314294815063,
      "learning_rate": 0.00016629564556721593,
      "loss": 0.6565,
      "step": 35600
    },
    {
      "epoch": 0.5076178271393991,
      "grad_norm": 0.45324423909187317,
      "learning_rate": 0.00016624815993162068,
      "loss": 0.6883,
      "step": 35650
    },
    {
      "epoch": 0.5083297736010252,
      "grad_norm": 0.4064430594444275,
      "learning_rate": 0.00016620067429602546,
      "loss": 0.6375,
      "step": 35700
    },
    {
      "epoch": 0.5090417200626512,
      "grad_norm": 0.4112638235092163,
      "learning_rate": 0.00016615318866043023,
      "loss": 0.7342,
      "step": 35750
    },
    {
      "epoch": 0.5097536665242773,
      "grad_norm": 0.5444554686546326,
      "learning_rate": 0.00016610570302483498,
      "loss": 0.6521,
      "step": 35800
    },
    {
      "epoch": 0.5104656129859034,
      "grad_norm": 0.5913863182067871,
      "learning_rate": 0.00016605821738923976,
      "loss": 0.5891,
      "step": 35850
    },
    {
      "epoch": 0.5111775594475295,
      "grad_norm": 1.0620644092559814,
      "learning_rate": 0.0001660107317536445,
      "loss": 0.6696,
      "step": 35900
    },
    {
      "epoch": 0.5118895059091556,
      "grad_norm": 0.36830323934555054,
      "learning_rate": 0.0001659632461180493,
      "loss": 0.699,
      "step": 35950
    },
    {
      "epoch": 0.5126014523707817,
      "grad_norm": 0.3914502263069153,
      "learning_rate": 0.00016591576048245407,
      "loss": 0.5975,
      "step": 36000
    },
    {
      "epoch": 0.5133133988324078,
      "grad_norm": 0.5689184069633484,
      "learning_rate": 0.00016586827484685882,
      "loss": 0.6921,
      "step": 36050
    },
    {
      "epoch": 0.5140253452940339,
      "grad_norm": 0.2584925591945648,
      "learning_rate": 0.0001658207892112636,
      "loss": 0.6662,
      "step": 36100
    },
    {
      "epoch": 0.51473729175566,
      "grad_norm": 0.5393460988998413,
      "learning_rate": 0.00016577330357566835,
      "loss": 0.6936,
      "step": 36150
    },
    {
      "epoch": 0.515449238217286,
      "grad_norm": 0.6728947758674622,
      "learning_rate": 0.00016572581794007312,
      "loss": 0.6961,
      "step": 36200
    },
    {
      "epoch": 0.5161611846789121,
      "grad_norm": 0.7632102370262146,
      "learning_rate": 0.0001656783323044779,
      "loss": 0.6699,
      "step": 36250
    },
    {
      "epoch": 0.5168731311405382,
      "grad_norm": 0.6089598536491394,
      "learning_rate": 0.00016563084666888268,
      "loss": 0.6715,
      "step": 36300
    },
    {
      "epoch": 0.5175850776021643,
      "grad_norm": 0.4236952066421509,
      "learning_rate": 0.00016558336103328743,
      "loss": 0.6972,
      "step": 36350
    },
    {
      "epoch": 0.5182970240637904,
      "grad_norm": 0.39742428064346313,
      "learning_rate": 0.0001655358753976922,
      "loss": 0.6461,
      "step": 36400
    },
    {
      "epoch": 0.5190089705254165,
      "grad_norm": 0.44298267364501953,
      "learning_rate": 0.00016548838976209699,
      "loss": 0.7009,
      "step": 36450
    },
    {
      "epoch": 0.5197209169870426,
      "grad_norm": 0.6023497581481934,
      "learning_rate": 0.00016544090412650176,
      "loss": 0.6768,
      "step": 36500
    },
    {
      "epoch": 0.5204328634486687,
      "grad_norm": 0.5291181802749634,
      "learning_rate": 0.0001653934184909065,
      "loss": 0.6954,
      "step": 36550
    },
    {
      "epoch": 0.5211448099102948,
      "grad_norm": 0.4782496392726898,
      "learning_rate": 0.0001653459328553113,
      "loss": 0.7305,
      "step": 36600
    },
    {
      "epoch": 0.5218567563719209,
      "grad_norm": 0.4489971101284027,
      "learning_rate": 0.00016529844721971604,
      "loss": 0.6384,
      "step": 36650
    },
    {
      "epoch": 0.522568702833547,
      "grad_norm": 0.455275297164917,
      "learning_rate": 0.00016525096158412082,
      "loss": 0.6783,
      "step": 36700
    },
    {
      "epoch": 0.523280649295173,
      "grad_norm": 0.42303943634033203,
      "learning_rate": 0.0001652034759485256,
      "loss": 0.67,
      "step": 36750
    },
    {
      "epoch": 0.5239925957567991,
      "grad_norm": 0.47778376936912537,
      "learning_rate": 0.00016515599031293035,
      "loss": 0.709,
      "step": 36800
    },
    {
      "epoch": 0.5247045422184252,
      "grad_norm": 0.5132131576538086,
      "learning_rate": 0.00016510850467733512,
      "loss": 0.6682,
      "step": 36850
    },
    {
      "epoch": 0.5254164886800513,
      "grad_norm": 0.7255716323852539,
      "learning_rate": 0.00016506101904173988,
      "loss": 0.651,
      "step": 36900
    },
    {
      "epoch": 0.5261284351416774,
      "grad_norm": 0.29349783062934875,
      "learning_rate": 0.00016501353340614465,
      "loss": 0.6342,
      "step": 36950
    },
    {
      "epoch": 0.5268403816033035,
      "grad_norm": 0.506917417049408,
      "learning_rate": 0.00016496604777054943,
      "loss": 0.6472,
      "step": 37000
    },
    {
      "epoch": 0.5275523280649295,
      "grad_norm": 0.4343845546245575,
      "learning_rate": 0.00016491856213495418,
      "loss": 0.6623,
      "step": 37050
    },
    {
      "epoch": 0.5282642745265556,
      "grad_norm": 0.7428778409957886,
      "learning_rate": 0.00016487107649935896,
      "loss": 0.6747,
      "step": 37100
    },
    {
      "epoch": 0.5289762209881816,
      "grad_norm": 0.5931469202041626,
      "learning_rate": 0.0001648235908637637,
      "loss": 0.6324,
      "step": 37150
    },
    {
      "epoch": 0.5296881674498077,
      "grad_norm": 0.3914090394973755,
      "learning_rate": 0.0001647761052281685,
      "loss": 0.6429,
      "step": 37200
    },
    {
      "epoch": 0.5304001139114338,
      "grad_norm": 0.7476955056190491,
      "learning_rate": 0.00016472861959257326,
      "loss": 0.6858,
      "step": 37250
    },
    {
      "epoch": 0.5311120603730599,
      "grad_norm": 0.4714812636375427,
      "learning_rate": 0.00016468113395697802,
      "loss": 0.6467,
      "step": 37300
    },
    {
      "epoch": 0.531824006834686,
      "grad_norm": 0.4076942205429077,
      "learning_rate": 0.0001646336483213828,
      "loss": 0.6927,
      "step": 37350
    },
    {
      "epoch": 0.5325359532963121,
      "grad_norm": 0.5088539123535156,
      "learning_rate": 0.00016458616268578754,
      "loss": 0.5966,
      "step": 37400
    },
    {
      "epoch": 0.5332478997579382,
      "grad_norm": 0.46488434076309204,
      "learning_rate": 0.00016453867705019232,
      "loss": 0.6433,
      "step": 37450
    },
    {
      "epoch": 0.5339598462195643,
      "grad_norm": 0.5106256008148193,
      "learning_rate": 0.0001644911914145971,
      "loss": 0.683,
      "step": 37500
    },
    {
      "epoch": 0.5346717926811904,
      "grad_norm": 0.44839292764663696,
      "learning_rate": 0.00016444370577900185,
      "loss": 0.6335,
      "step": 37550
    },
    {
      "epoch": 0.5353837391428165,
      "grad_norm": 0.5184427499771118,
      "learning_rate": 0.00016439622014340663,
      "loss": 0.629,
      "step": 37600
    },
    {
      "epoch": 0.5360956856044425,
      "grad_norm": 0.8199726343154907,
      "learning_rate": 0.00016434873450781138,
      "loss": 0.6572,
      "step": 37650
    },
    {
      "epoch": 0.5368076320660686,
      "grad_norm": 0.49629920721054077,
      "learning_rate": 0.00016430124887221615,
      "loss": 0.6464,
      "step": 37700
    },
    {
      "epoch": 0.5375195785276947,
      "grad_norm": 0.49614155292510986,
      "learning_rate": 0.00016425376323662093,
      "loss": 0.6726,
      "step": 37750
    },
    {
      "epoch": 0.5382315249893208,
      "grad_norm": 0.3948458433151245,
      "learning_rate": 0.00016420627760102568,
      "loss": 0.6318,
      "step": 37800
    },
    {
      "epoch": 0.5389434714509469,
      "grad_norm": 0.7814825773239136,
      "learning_rate": 0.00016415974167814236,
      "loss": 0.7032,
      "step": 37850
    },
    {
      "epoch": 0.539655417912573,
      "grad_norm": 0.3547521233558655,
      "learning_rate": 0.00016411225604254714,
      "loss": 0.6754,
      "step": 37900
    },
    {
      "epoch": 0.5403673643741991,
      "grad_norm": 0.3444863557815552,
      "learning_rate": 0.0001640647704069519,
      "loss": 0.6961,
      "step": 37950
    },
    {
      "epoch": 0.5410793108358252,
      "grad_norm": 0.5503460764884949,
      "learning_rate": 0.00016401728477135667,
      "loss": 0.6423,
      "step": 38000
    },
    {
      "epoch": 0.5417912572974513,
      "grad_norm": 0.4034751355648041,
      "learning_rate": 0.00016396979913576145,
      "loss": 0.6726,
      "step": 38050
    },
    {
      "epoch": 0.5425032037590773,
      "grad_norm": 0.6095737218856812,
      "learning_rate": 0.0001639223135001662,
      "loss": 0.6832,
      "step": 38100
    },
    {
      "epoch": 0.5432151502207034,
      "grad_norm": 0.6016215085983276,
      "learning_rate": 0.00016387482786457097,
      "loss": 0.6471,
      "step": 38150
    },
    {
      "epoch": 0.5439270966823295,
      "grad_norm": 0.4177979826927185,
      "learning_rate": 0.00016382734222897572,
      "loss": 0.7325,
      "step": 38200
    },
    {
      "epoch": 0.5446390431439556,
      "grad_norm": 0.5720232129096985,
      "learning_rate": 0.0001637798565933805,
      "loss": 0.629,
      "step": 38250
    },
    {
      "epoch": 0.5453509896055817,
      "grad_norm": 0.46797996759414673,
      "learning_rate": 0.00016373237095778528,
      "loss": 0.678,
      "step": 38300
    },
    {
      "epoch": 0.5460629360672078,
      "grad_norm": 0.36959967017173767,
      "learning_rate": 0.00016368488532219003,
      "loss": 0.6124,
      "step": 38350
    },
    {
      "epoch": 0.5467748825288339,
      "grad_norm": 0.4796397387981415,
      "learning_rate": 0.0001636373996865948,
      "loss": 0.6998,
      "step": 38400
    },
    {
      "epoch": 0.54748682899046,
      "grad_norm": 0.4049106240272522,
      "learning_rate": 0.00016358991405099956,
      "loss": 0.6214,
      "step": 38450
    },
    {
      "epoch": 0.548198775452086,
      "grad_norm": 0.6862742304801941,
      "learning_rate": 0.00016354242841540434,
      "loss": 0.6665,
      "step": 38500
    },
    {
      "epoch": 0.548910721913712,
      "grad_norm": 0.7961316704750061,
      "learning_rate": 0.0001634949427798091,
      "loss": 0.6851,
      "step": 38550
    },
    {
      "epoch": 0.5496226683753381,
      "grad_norm": 0.45350685715675354,
      "learning_rate": 0.0001634474571442139,
      "loss": 0.7099,
      "step": 38600
    },
    {
      "epoch": 0.5503346148369642,
      "grad_norm": 0.4786728322505951,
      "learning_rate": 0.00016339997150861864,
      "loss": 0.6574,
      "step": 38650
    },
    {
      "epoch": 0.5510465612985903,
      "grad_norm": 0.863484263420105,
      "learning_rate": 0.00016335248587302342,
      "loss": 0.7019,
      "step": 38700
    },
    {
      "epoch": 0.5517585077602164,
      "grad_norm": 0.5704702734947205,
      "learning_rate": 0.0001633050002374282,
      "loss": 0.6185,
      "step": 38750
    },
    {
      "epoch": 0.5524704542218425,
      "grad_norm": 0.52175372838974,
      "learning_rate": 0.00016325751460183297,
      "loss": 0.6398,
      "step": 38800
    },
    {
      "epoch": 0.5531824006834686,
      "grad_norm": 0.5696172118186951,
      "learning_rate": 0.00016321002896623772,
      "loss": 0.7223,
      "step": 38850
    },
    {
      "epoch": 0.5538943471450947,
      "grad_norm": 0.5967522859573364,
      "learning_rate": 0.0001631625433306425,
      "loss": 0.6575,
      "step": 38900
    },
    {
      "epoch": 0.5546062936067208,
      "grad_norm": 0.6648288369178772,
      "learning_rate": 0.00016311505769504725,
      "loss": 0.6946,
      "step": 38950
    },
    {
      "epoch": 0.5553182400683468,
      "grad_norm": 0.9983428716659546,
      "learning_rate": 0.00016306757205945203,
      "loss": 0.6608,
      "step": 39000
    },
    {
      "epoch": 0.5560301865299729,
      "grad_norm": 0.5027548670768738,
      "learning_rate": 0.0001630200864238568,
      "loss": 0.6723,
      "step": 39050
    },
    {
      "epoch": 0.556742132991599,
      "grad_norm": 0.5353224873542786,
      "learning_rate": 0.00016297260078826156,
      "loss": 0.6771,
      "step": 39100
    },
    {
      "epoch": 0.5574540794532251,
      "grad_norm": 0.4915717542171478,
      "learning_rate": 0.00016292511515266634,
      "loss": 0.6973,
      "step": 39150
    },
    {
      "epoch": 0.5581660259148512,
      "grad_norm": 0.5130100846290588,
      "learning_rate": 0.00016287762951707109,
      "loss": 0.6399,
      "step": 39200
    },
    {
      "epoch": 0.5588779723764773,
      "grad_norm": 0.6462712287902832,
      "learning_rate": 0.00016283014388147586,
      "loss": 0.6579,
      "step": 39250
    },
    {
      "epoch": 0.5595899188381034,
      "grad_norm": 0.6382865309715271,
      "learning_rate": 0.00016278265824588064,
      "loss": 0.6209,
      "step": 39300
    },
    {
      "epoch": 0.5603018652997295,
      "grad_norm": 0.47597426176071167,
      "learning_rate": 0.0001627351726102854,
      "loss": 0.7113,
      "step": 39350
    },
    {
      "epoch": 0.5610138117613556,
      "grad_norm": 0.4746643006801605,
      "learning_rate": 0.00016268768697469017,
      "loss": 0.6837,
      "step": 39400
    },
    {
      "epoch": 0.5617257582229817,
      "grad_norm": 0.5347773432731628,
      "learning_rate": 0.00016264020133909492,
      "loss": 0.6607,
      "step": 39450
    },
    {
      "epoch": 0.5624377046846077,
      "grad_norm": 0.3563390374183655,
      "learning_rate": 0.0001625927157034997,
      "loss": 0.6469,
      "step": 39500
    },
    {
      "epoch": 0.5631496511462338,
      "grad_norm": 0.29718509316444397,
      "learning_rate": 0.00016254523006790448,
      "loss": 0.6167,
      "step": 39550
    },
    {
      "epoch": 0.5638615976078599,
      "grad_norm": 0.594212532043457,
      "learning_rate": 0.00016249774443230923,
      "loss": 0.6788,
      "step": 39600
    },
    {
      "epoch": 0.564573544069486,
      "grad_norm": 0.4469192326068878,
      "learning_rate": 0.000162450258796714,
      "loss": 0.6849,
      "step": 39650
    },
    {
      "epoch": 0.5652854905311121,
      "grad_norm": 0.44163286685943604,
      "learning_rate": 0.00016240277316111875,
      "loss": 0.665,
      "step": 39700
    },
    {
      "epoch": 0.5659974369927382,
      "grad_norm": 0.570176899433136,
      "learning_rate": 0.00016235528752552353,
      "loss": 0.6477,
      "step": 39750
    },
    {
      "epoch": 0.5667093834543643,
      "grad_norm": 0.3669861853122711,
      "learning_rate": 0.0001623078018899283,
      "loss": 0.6308,
      "step": 39800
    },
    {
      "epoch": 0.5674213299159904,
      "grad_norm": 0.4907342493534088,
      "learning_rate": 0.00016226031625433306,
      "loss": 0.6814,
      "step": 39850
    },
    {
      "epoch": 0.5681332763776165,
      "grad_norm": 0.34395554661750793,
      "learning_rate": 0.00016221283061873784,
      "loss": 0.6392,
      "step": 39900
    },
    {
      "epoch": 0.5688452228392424,
      "grad_norm": 0.5701097249984741,
      "learning_rate": 0.0001621653449831426,
      "loss": 0.6904,
      "step": 39950
    },
    {
      "epoch": 0.5695571693008685,
      "grad_norm": 0.4318050444126129,
      "learning_rate": 0.00016211785934754737,
      "loss": 0.627,
      "step": 40000
    },
    {
      "epoch": 0.5702691157624946,
      "grad_norm": 0.545208215713501,
      "learning_rate": 0.00016207037371195214,
      "loss": 0.6472,
      "step": 40050
    },
    {
      "epoch": 0.5709810622241207,
      "grad_norm": 0.7253319621086121,
      "learning_rate": 0.0001620228880763569,
      "loss": 0.6879,
      "step": 40100
    },
    {
      "epoch": 0.5716930086857468,
      "grad_norm": 0.4444308578968048,
      "learning_rate": 0.00016197540244076167,
      "loss": 0.614,
      "step": 40150
    },
    {
      "epoch": 0.5724049551473729,
      "grad_norm": 0.5792410373687744,
      "learning_rate": 0.00016192791680516645,
      "loss": 0.6958,
      "step": 40200
    },
    {
      "epoch": 0.573116901608999,
      "grad_norm": 0.55196613073349,
      "learning_rate": 0.0001618804311695712,
      "loss": 0.6728,
      "step": 40250
    },
    {
      "epoch": 0.5738288480706251,
      "grad_norm": 1.0691859722137451,
      "learning_rate": 0.00016183294553397598,
      "loss": 0.6657,
      "step": 40300
    },
    {
      "epoch": 0.5745407945322512,
      "grad_norm": 0.6424625515937805,
      "learning_rate": 0.00016178545989838075,
      "loss": 0.7136,
      "step": 40350
    },
    {
      "epoch": 0.5752527409938772,
      "grad_norm": 0.6013586521148682,
      "learning_rate": 0.00016173797426278553,
      "loss": 0.7257,
      "step": 40400
    },
    {
      "epoch": 0.5759646874555033,
      "grad_norm": 0.4763856828212738,
      "learning_rate": 0.00016169048862719028,
      "loss": 0.6786,
      "step": 40450
    },
    {
      "epoch": 0.5766766339171294,
      "grad_norm": 0.5468772053718567,
      "learning_rate": 0.00016164300299159506,
      "loss": 0.6982,
      "step": 40500
    },
    {
      "epoch": 0.5773885803787555,
      "grad_norm": 0.5196506381034851,
      "learning_rate": 0.00016159551735599984,
      "loss": 0.6862,
      "step": 40550
    },
    {
      "epoch": 0.5781005268403816,
      "grad_norm": 0.5180202126502991,
      "learning_rate": 0.0001615489814331165,
      "loss": 0.6916,
      "step": 40600
    },
    {
      "epoch": 0.5788124733020077,
      "grad_norm": 0.4850539267063141,
      "learning_rate": 0.00016150149579752124,
      "loss": 0.6516,
      "step": 40650
    },
    {
      "epoch": 0.5795244197636338,
      "grad_norm": 0.33006542921066284,
      "learning_rate": 0.00016145401016192602,
      "loss": 0.6671,
      "step": 40700
    },
    {
      "epoch": 0.5802363662252599,
      "grad_norm": 0.5571639537811279,
      "learning_rate": 0.0001614065245263308,
      "loss": 0.6803,
      "step": 40750
    },
    {
      "epoch": 0.580948312686886,
      "grad_norm": 0.47514840960502625,
      "learning_rate": 0.00016135903889073555,
      "loss": 0.6794,
      "step": 40800
    },
    {
      "epoch": 0.581660259148512,
      "grad_norm": 0.6315642595291138,
      "learning_rate": 0.00016131250296785223,
      "loss": 0.674,
      "step": 40850
    },
    {
      "epoch": 0.5823722056101381,
      "grad_norm": 0.514869749546051,
      "learning_rate": 0.000161265017332257,
      "loss": 0.6761,
      "step": 40900
    },
    {
      "epoch": 0.5830841520717642,
      "grad_norm": 0.6091417670249939,
      "learning_rate": 0.00016121753169666175,
      "loss": 0.6513,
      "step": 40950
    },
    {
      "epoch": 0.5837960985333903,
      "grad_norm": 0.4855555593967438,
      "learning_rate": 0.00016117004606106653,
      "loss": 0.6824,
      "step": 41000
    },
    {
      "epoch": 0.5845080449950164,
      "grad_norm": 0.35543572902679443,
      "learning_rate": 0.00016112256042547128,
      "loss": 0.6637,
      "step": 41050
    },
    {
      "epoch": 0.5852199914566425,
      "grad_norm": 0.4191708266735077,
      "learning_rate": 0.00016107507478987606,
      "loss": 0.6628,
      "step": 41100
    },
    {
      "epoch": 0.5859319379182686,
      "grad_norm": 0.712510347366333,
      "learning_rate": 0.00016102758915428084,
      "loss": 0.6234,
      "step": 41150
    },
    {
      "epoch": 0.5866438843798947,
      "grad_norm": 0.4112408757209778,
      "learning_rate": 0.0001609801035186856,
      "loss": 0.6312,
      "step": 41200
    },
    {
      "epoch": 0.5873558308415208,
      "grad_norm": 0.7829216122627258,
      "learning_rate": 0.00016093261788309037,
      "loss": 0.7002,
      "step": 41250
    },
    {
      "epoch": 0.5880677773031469,
      "grad_norm": 0.6089222431182861,
      "learning_rate": 0.00016088513224749514,
      "loss": 0.6894,
      "step": 41300
    },
    {
      "epoch": 0.5887797237647729,
      "grad_norm": 0.43894344568252563,
      "learning_rate": 0.0001608376466118999,
      "loss": 0.6925,
      "step": 41350
    },
    {
      "epoch": 0.5894916702263989,
      "grad_norm": 0.5233023762702942,
      "learning_rate": 0.00016079016097630467,
      "loss": 0.6277,
      "step": 41400
    },
    {
      "epoch": 0.590203616688025,
      "grad_norm": 0.49154695868492126,
      "learning_rate": 0.00016074267534070945,
      "loss": 0.6637,
      "step": 41450
    },
    {
      "epoch": 0.5909155631496511,
      "grad_norm": 0.6139885783195496,
      "learning_rate": 0.00016069518970511423,
      "loss": 0.6848,
      "step": 41500
    },
    {
      "epoch": 0.5916275096112772,
      "grad_norm": 0.5061021447181702,
      "learning_rate": 0.00016064770406951898,
      "loss": 0.652,
      "step": 41550
    },
    {
      "epoch": 0.5923394560729033,
      "grad_norm": 0.4703654944896698,
      "learning_rate": 0.00016060021843392375,
      "loss": 0.6152,
      "step": 41600
    },
    {
      "epoch": 0.5930514025345294,
      "grad_norm": 0.5491198301315308,
      "learning_rate": 0.00016055273279832853,
      "loss": 0.7195,
      "step": 41650
    },
    {
      "epoch": 0.5937633489961555,
      "grad_norm": 0.5638359189033508,
      "learning_rate": 0.00016050524716273328,
      "loss": 0.613,
      "step": 41700
    },
    {
      "epoch": 0.5944752954577815,
      "grad_norm": 0.5419908761978149,
      "learning_rate": 0.00016045776152713806,
      "loss": 0.7005,
      "step": 41750
    },
    {
      "epoch": 0.5951872419194076,
      "grad_norm": 0.5710875391960144,
      "learning_rate": 0.0001604102758915428,
      "loss": 0.6389,
      "step": 41800
    },
    {
      "epoch": 0.5958991883810337,
      "grad_norm": 0.7872911691665649,
      "learning_rate": 0.0001603627902559476,
      "loss": 0.691,
      "step": 41850
    },
    {
      "epoch": 0.5966111348426598,
      "grad_norm": 0.5696393847465515,
      "learning_rate": 0.00016031530462035237,
      "loss": 0.6662,
      "step": 41900
    },
    {
      "epoch": 0.5973230813042859,
      "grad_norm": 0.45197322964668274,
      "learning_rate": 0.00016026781898475712,
      "loss": 0.6858,
      "step": 41950
    },
    {
      "epoch": 0.598035027765912,
      "grad_norm": 0.9894557595252991,
      "learning_rate": 0.0001602203333491619,
      "loss": 0.6804,
      "step": 42000
    },
    {
      "epoch": 0.5987469742275381,
      "grad_norm": 0.4826708137989044,
      "learning_rate": 0.00016017284771356664,
      "loss": 0.7002,
      "step": 42050
    },
    {
      "epoch": 0.5994589206891642,
      "grad_norm": 0.6446958780288696,
      "learning_rate": 0.00016012536207797142,
      "loss": 0.7273,
      "step": 42100
    },
    {
      "epoch": 0.6001708671507903,
      "grad_norm": 0.5392485857009888,
      "learning_rate": 0.0001600778764423762,
      "loss": 0.7022,
      "step": 42150
    },
    {
      "epoch": 0.6008828136124164,
      "grad_norm": 0.3986678123474121,
      "learning_rate": 0.00016003039080678095,
      "loss": 0.6644,
      "step": 42200
    },
    {
      "epoch": 0.6015947600740424,
      "grad_norm": 0.3405541181564331,
      "learning_rate": 0.00015998290517118573,
      "loss": 0.6064,
      "step": 42250
    },
    {
      "epoch": 0.6023067065356685,
      "grad_norm": 0.41316255927085876,
      "learning_rate": 0.00015993541953559048,
      "loss": 0.6989,
      "step": 42300
    },
    {
      "epoch": 0.6030186529972946,
      "grad_norm": 0.44273507595062256,
      "learning_rate": 0.00015988793389999526,
      "loss": 0.6321,
      "step": 42350
    },
    {
      "epoch": 0.6037305994589207,
      "grad_norm": 0.38769543170928955,
      "learning_rate": 0.00015984044826440003,
      "loss": 0.7005,
      "step": 42400
    },
    {
      "epoch": 0.6044425459205468,
      "grad_norm": 0.36866894364356995,
      "learning_rate": 0.00015979296262880478,
      "loss": 0.6447,
      "step": 42450
    },
    {
      "epoch": 0.6051544923821729,
      "grad_norm": 0.5075907707214355,
      "learning_rate": 0.00015974547699320956,
      "loss": 0.63,
      "step": 42500
    },
    {
      "epoch": 0.605866438843799,
      "grad_norm": 0.5636134743690491,
      "learning_rate": 0.0001596979913576143,
      "loss": 0.6156,
      "step": 42550
    },
    {
      "epoch": 0.6065783853054251,
      "grad_norm": 0.5135008692741394,
      "learning_rate": 0.0001596505057220191,
      "loss": 0.6854,
      "step": 42600
    },
    {
      "epoch": 0.6072903317670512,
      "grad_norm": 0.4160917401313782,
      "learning_rate": 0.00015960302008642387,
      "loss": 0.6914,
      "step": 42650
    },
    {
      "epoch": 0.6080022782286773,
      "grad_norm": 0.7610266208648682,
      "learning_rate": 0.00015955553445082862,
      "loss": 0.7218,
      "step": 42700
    },
    {
      "epoch": 0.6087142246903033,
      "grad_norm": 0.6426644325256348,
      "learning_rate": 0.0001595080488152334,
      "loss": 0.6427,
      "step": 42750
    },
    {
      "epoch": 0.6094261711519294,
      "grad_norm": 0.4601852297782898,
      "learning_rate": 0.00015946056317963815,
      "loss": 0.6055,
      "step": 42800
    },
    {
      "epoch": 0.6101381176135554,
      "grad_norm": 0.5471464395523071,
      "learning_rate": 0.00015941307754404292,
      "loss": 0.7032,
      "step": 42850
    },
    {
      "epoch": 0.6108500640751815,
      "grad_norm": 1.044219732284546,
      "learning_rate": 0.0001593655919084477,
      "loss": 0.7067,
      "step": 42900
    },
    {
      "epoch": 0.6115620105368076,
      "grad_norm": 0.5534868836402893,
      "learning_rate": 0.00015931810627285245,
      "loss": 0.6744,
      "step": 42950
    },
    {
      "epoch": 0.6122739569984337,
      "grad_norm": 0.6081308126449585,
      "learning_rate": 0.00015927062063725723,
      "loss": 0.6436,
      "step": 43000
    },
    {
      "epoch": 0.6129859034600598,
      "grad_norm": 0.6624608635902405,
      "learning_rate": 0.0001592240847143739,
      "loss": 0.6526,
      "step": 43050
    },
    {
      "epoch": 0.6136978499216859,
      "grad_norm": 0.726389467716217,
      "learning_rate": 0.00015917659907877869,
      "loss": 0.6545,
      "step": 43100
    },
    {
      "epoch": 0.614409796383312,
      "grad_norm": 0.45067429542541504,
      "learning_rate": 0.00015912911344318344,
      "loss": 0.7119,
      "step": 43150
    },
    {
      "epoch": 0.615121742844938,
      "grad_norm": 0.7054116129875183,
      "learning_rate": 0.00015908162780758821,
      "loss": 0.6374,
      "step": 43200
    },
    {
      "epoch": 0.6158336893065641,
      "grad_norm": 0.44526275992393494,
      "learning_rate": 0.00015903414217199296,
      "loss": 0.6869,
      "step": 43250
    },
    {
      "epoch": 0.6165456357681902,
      "grad_norm": 0.6915913820266724,
      "learning_rate": 0.00015898665653639774,
      "loss": 0.6642,
      "step": 43300
    },
    {
      "epoch": 0.6172575822298163,
      "grad_norm": 0.5157249569892883,
      "learning_rate": 0.00015893917090080252,
      "loss": 0.6575,
      "step": 43350
    },
    {
      "epoch": 0.6179695286914424,
      "grad_norm": 0.6572285294532776,
      "learning_rate": 0.00015889168526520727,
      "loss": 0.6716,
      "step": 43400
    },
    {
      "epoch": 0.6186814751530685,
      "grad_norm": 0.4397391974925995,
      "learning_rate": 0.00015884419962961205,
      "loss": 0.6656,
      "step": 43450
    },
    {
      "epoch": 0.6193934216146946,
      "grad_norm": 0.3501693606376648,
      "learning_rate": 0.0001587967139940168,
      "loss": 0.646,
      "step": 43500
    },
    {
      "epoch": 0.6201053680763207,
      "grad_norm": 0.42144012451171875,
      "learning_rate": 0.00015874922835842158,
      "loss": 0.6734,
      "step": 43550
    },
    {
      "epoch": 0.6208173145379468,
      "grad_norm": 0.6852348446846008,
      "learning_rate": 0.00015870174272282635,
      "loss": 0.6883,
      "step": 43600
    },
    {
      "epoch": 0.6215292609995728,
      "grad_norm": 0.6408162713050842,
      "learning_rate": 0.0001586542570872311,
      "loss": 0.6799,
      "step": 43650
    },
    {
      "epoch": 0.6222412074611989,
      "grad_norm": 0.41414710879325867,
      "learning_rate": 0.00015860677145163588,
      "loss": 0.6735,
      "step": 43700
    },
    {
      "epoch": 0.622953153922825,
      "grad_norm": 0.6371641159057617,
      "learning_rate": 0.00015855928581604066,
      "loss": 0.5976,
      "step": 43750
    },
    {
      "epoch": 0.6236651003844511,
      "grad_norm": 0.4767855107784271,
      "learning_rate": 0.00015851180018044544,
      "loss": 0.6686,
      "step": 43800
    },
    {
      "epoch": 0.6243770468460772,
      "grad_norm": 0.6659448742866516,
      "learning_rate": 0.0001584643145448502,
      "loss": 0.6965,
      "step": 43850
    },
    {
      "epoch": 0.6250889933077033,
      "grad_norm": 0.35632869601249695,
      "learning_rate": 0.00015841682890925497,
      "loss": 0.6397,
      "step": 43900
    },
    {
      "epoch": 0.6258009397693294,
      "grad_norm": 0.7143546938896179,
      "learning_rate": 0.00015836934327365974,
      "loss": 0.6588,
      "step": 43950
    },
    {
      "epoch": 0.6265128862309555,
      "grad_norm": 0.44593125581741333,
      "learning_rate": 0.0001583218576380645,
      "loss": 0.6251,
      "step": 44000
    },
    {
      "epoch": 0.6272248326925816,
      "grad_norm": 0.5970734357833862,
      "learning_rate": 0.00015827437200246927,
      "loss": 0.6641,
      "step": 44050
    },
    {
      "epoch": 0.6279367791542076,
      "grad_norm": 0.4717406630516052,
      "learning_rate": 0.00015822688636687405,
      "loss": 0.6633,
      "step": 44100
    },
    {
      "epoch": 0.6286487256158337,
      "grad_norm": 0.45285120606422424,
      "learning_rate": 0.0001581794007312788,
      "loss": 0.6664,
      "step": 44150
    },
    {
      "epoch": 0.6293606720774598,
      "grad_norm": 0.35514768958091736,
      "learning_rate": 0.00015813191509568358,
      "loss": 0.6702,
      "step": 44200
    },
    {
      "epoch": 0.6300726185390859,
      "grad_norm": 0.6015962362289429,
      "learning_rate": 0.00015808442946008833,
      "loss": 0.6672,
      "step": 44250
    },
    {
      "epoch": 0.6307845650007119,
      "grad_norm": 0.6165345907211304,
      "learning_rate": 0.0001580369438244931,
      "loss": 0.6162,
      "step": 44300
    },
    {
      "epoch": 0.631496511462338,
      "grad_norm": 0.5858595967292786,
      "learning_rate": 0.00015798945818889788,
      "loss": 0.6986,
      "step": 44350
    },
    {
      "epoch": 0.6322084579239641,
      "grad_norm": 0.5255721211433411,
      "learning_rate": 0.00015794197255330263,
      "loss": 0.6626,
      "step": 44400
    },
    {
      "epoch": 0.6329204043855902,
      "grad_norm": 0.5663081407546997,
      "learning_rate": 0.0001578944869177074,
      "loss": 0.6771,
      "step": 44450
    },
    {
      "epoch": 0.6336323508472163,
      "grad_norm": 0.6816524863243103,
      "learning_rate": 0.00015784700128211216,
      "loss": 0.7378,
      "step": 44500
    },
    {
      "epoch": 0.6343442973088423,
      "grad_norm": 0.511813759803772,
      "learning_rate": 0.00015779951564651694,
      "loss": 0.6823,
      "step": 44550
    },
    {
      "epoch": 0.6350562437704684,
      "grad_norm": 0.5002267956733704,
      "learning_rate": 0.00015775203001092172,
      "loss": 0.7297,
      "step": 44600
    },
    {
      "epoch": 0.6357681902320945,
      "grad_norm": 0.6661080121994019,
      "learning_rate": 0.00015770454437532647,
      "loss": 0.648,
      "step": 44650
    },
    {
      "epoch": 0.6364801366937206,
      "grad_norm": 0.5519765615463257,
      "learning_rate": 0.00015765705873973124,
      "loss": 0.6392,
      "step": 44700
    },
    {
      "epoch": 0.6371920831553467,
      "grad_norm": 0.562578022480011,
      "learning_rate": 0.000157609573104136,
      "loss": 0.6944,
      "step": 44750
    },
    {
      "epoch": 0.6379040296169728,
      "grad_norm": 0.8380330801010132,
      "learning_rate": 0.00015756208746854077,
      "loss": 0.6448,
      "step": 44800
    },
    {
      "epoch": 0.6386159760785989,
      "grad_norm": 0.45329511165618896,
      "learning_rate": 0.00015751460183294555,
      "loss": 0.6625,
      "step": 44850
    },
    {
      "epoch": 0.639327922540225,
      "grad_norm": 0.41718363761901855,
      "learning_rate": 0.0001574671161973503,
      "loss": 0.7032,
      "step": 44900
    },
    {
      "epoch": 0.6400398690018511,
      "grad_norm": 0.43900442123413086,
      "learning_rate": 0.00015741963056175508,
      "loss": 0.6777,
      "step": 44950
    },
    {
      "epoch": 0.6407518154634771,
      "grad_norm": 0.49233993887901306,
      "learning_rate": 0.00015737214492615983,
      "loss": 0.6685,
      "step": 45000
    },
    {
      "epoch": 0.6414637619251032,
      "grad_norm": 0.6282164454460144,
      "learning_rate": 0.0001573246592905646,
      "loss": 0.7015,
      "step": 45050
    },
    {
      "epoch": 0.6421757083867293,
      "grad_norm": 0.4255218505859375,
      "learning_rate": 0.00015727717365496938,
      "loss": 0.6744,
      "step": 45100
    },
    {
      "epoch": 0.6428876548483554,
      "grad_norm": 0.5278517603874207,
      "learning_rate": 0.00015722968801937413,
      "loss": 0.6597,
      "step": 45150
    },
    {
      "epoch": 0.6435996013099815,
      "grad_norm": 0.4659639298915863,
      "learning_rate": 0.0001571822023837789,
      "loss": 0.6834,
      "step": 45200
    },
    {
      "epoch": 0.6443115477716076,
      "grad_norm": 0.47359415888786316,
      "learning_rate": 0.00015713471674818366,
      "loss": 0.6588,
      "step": 45250
    },
    {
      "epoch": 0.6450234942332337,
      "grad_norm": 0.37534692883491516,
      "learning_rate": 0.00015708723111258844,
      "loss": 0.6543,
      "step": 45300
    },
    {
      "epoch": 0.6457354406948598,
      "grad_norm": 0.3917732834815979,
      "learning_rate": 0.00015703974547699322,
      "loss": 0.6632,
      "step": 45350
    },
    {
      "epoch": 0.6464473871564859,
      "grad_norm": 0.4626691937446594,
      "learning_rate": 0.000156992259841398,
      "loss": 0.6956,
      "step": 45400
    },
    {
      "epoch": 0.647159333618112,
      "grad_norm": 0.5095815062522888,
      "learning_rate": 0.00015694477420580275,
      "loss": 0.7007,
      "step": 45450
    },
    {
      "epoch": 0.647871280079738,
      "grad_norm": 0.2624909579753876,
      "learning_rate": 0.00015689728857020752,
      "loss": 0.6057,
      "step": 45500
    },
    {
      "epoch": 0.6485832265413641,
      "grad_norm": 0.6116001605987549,
      "learning_rate": 0.0001568498029346123,
      "loss": 0.7035,
      "step": 45550
    },
    {
      "epoch": 0.6492951730029902,
      "grad_norm": 0.33796682953834534,
      "learning_rate": 0.00015680231729901708,
      "loss": 0.6322,
      "step": 45600
    },
    {
      "epoch": 0.6500071194646163,
      "grad_norm": 0.36891478300094604,
      "learning_rate": 0.00015675483166342183,
      "loss": 0.6251,
      "step": 45650
    },
    {
      "epoch": 0.6507190659262424,
      "grad_norm": 0.46689265966415405,
      "learning_rate": 0.0001567073460278266,
      "loss": 0.6574,
      "step": 45700
    },
    {
      "epoch": 0.6514310123878684,
      "grad_norm": 0.4109092056751251,
      "learning_rate": 0.00015665986039223136,
      "loss": 0.6143,
      "step": 45750
    },
    {
      "epoch": 0.6521429588494945,
      "grad_norm": 0.4247898757457733,
      "learning_rate": 0.00015661237475663613,
      "loss": 0.6431,
      "step": 45800
    },
    {
      "epoch": 0.6528549053111206,
      "grad_norm": 0.4836762547492981,
      "learning_rate": 0.0001565648891210409,
      "loss": 0.6505,
      "step": 45850
    },
    {
      "epoch": 0.6535668517727466,
      "grad_norm": 0.6672163009643555,
      "learning_rate": 0.00015651740348544566,
      "loss": 0.6857,
      "step": 45900
    },
    {
      "epoch": 0.6542787982343727,
      "grad_norm": 0.5503724813461304,
      "learning_rate": 0.00015646991784985044,
      "loss": 0.6085,
      "step": 45950
    },
    {
      "epoch": 0.6549907446959988,
      "grad_norm": 0.5826755166053772,
      "learning_rate": 0.0001564224322142552,
      "loss": 0.6406,
      "step": 46000
    },
    {
      "epoch": 0.6557026911576249,
      "grad_norm": 0.6396545171737671,
      "learning_rate": 0.00015637494657865997,
      "loss": 0.6458,
      "step": 46050
    },
    {
      "epoch": 0.656414637619251,
      "grad_norm": 0.4260285794734955,
      "learning_rate": 0.00015632746094306475,
      "loss": 0.6663,
      "step": 46100
    },
    {
      "epoch": 0.6571265840808771,
      "grad_norm": 0.5885871052742004,
      "learning_rate": 0.0001562799753074695,
      "loss": 0.6252,
      "step": 46150
    },
    {
      "epoch": 0.6578385305425032,
      "grad_norm": 0.44013628363609314,
      "learning_rate": 0.00015623343938458618,
      "loss": 0.7097,
      "step": 46200
    },
    {
      "epoch": 0.6585504770041293,
      "grad_norm": 0.4682617783546448,
      "learning_rate": 0.00015618595374899095,
      "loss": 0.6687,
      "step": 46250
    },
    {
      "epoch": 0.6592624234657554,
      "grad_norm": 0.6745122075080872,
      "learning_rate": 0.0001561384681133957,
      "loss": 0.7014,
      "step": 46300
    },
    {
      "epoch": 0.6599743699273815,
      "grad_norm": 0.3886183798313141,
      "learning_rate": 0.00015609098247780048,
      "loss": 0.6329,
      "step": 46350
    },
    {
      "epoch": 0.6606863163890075,
      "grad_norm": 0.4287640452384949,
      "learning_rate": 0.00015604349684220526,
      "loss": 0.667,
      "step": 46400
    },
    {
      "epoch": 0.6613982628506336,
      "grad_norm": 0.6392888426780701,
      "learning_rate": 0.00015599601120661,
      "loss": 0.5971,
      "step": 46450
    },
    {
      "epoch": 0.6621102093122597,
      "grad_norm": 0.48283159732818604,
      "learning_rate": 0.0001559485255710148,
      "loss": 0.6219,
      "step": 46500
    },
    {
      "epoch": 0.6628221557738858,
      "grad_norm": 0.46939265727996826,
      "learning_rate": 0.00015590103993541954,
      "loss": 0.6423,
      "step": 46550
    },
    {
      "epoch": 0.6635341022355119,
      "grad_norm": 0.5603742003440857,
      "learning_rate": 0.00015585355429982432,
      "loss": 0.7016,
      "step": 46600
    },
    {
      "epoch": 0.664246048697138,
      "grad_norm": 0.4784272015094757,
      "learning_rate": 0.0001558060686642291,
      "loss": 0.6782,
      "step": 46650
    },
    {
      "epoch": 0.6649579951587641,
      "grad_norm": 0.4359712600708008,
      "learning_rate": 0.00015575858302863384,
      "loss": 0.6378,
      "step": 46700
    },
    {
      "epoch": 0.6656699416203902,
      "grad_norm": 0.5238354206085205,
      "learning_rate": 0.00015571109739303862,
      "loss": 0.6615,
      "step": 46750
    },
    {
      "epoch": 0.6663818880820163,
      "grad_norm": 0.41251906752586365,
      "learning_rate": 0.00015566361175744337,
      "loss": 0.6835,
      "step": 46800
    },
    {
      "epoch": 0.6670938345436423,
      "grad_norm": 0.36588403582572937,
      "learning_rate": 0.00015561612612184815,
      "loss": 0.6615,
      "step": 46850
    },
    {
      "epoch": 0.6678057810052684,
      "grad_norm": 0.6980614066123962,
      "learning_rate": 0.00015556864048625293,
      "loss": 0.6627,
      "step": 46900
    },
    {
      "epoch": 0.6685177274668945,
      "grad_norm": 0.6704576015472412,
      "learning_rate": 0.00015552115485065768,
      "loss": 0.6306,
      "step": 46950
    },
    {
      "epoch": 0.6692296739285206,
      "grad_norm": 0.5513014197349548,
      "learning_rate": 0.00015547366921506246,
      "loss": 0.6789,
      "step": 47000
    },
    {
      "epoch": 0.6699416203901467,
      "grad_norm": 0.4075104594230652,
      "learning_rate": 0.0001554261835794672,
      "loss": 0.6538,
      "step": 47050
    },
    {
      "epoch": 0.6706535668517728,
      "grad_norm": 0.6990155577659607,
      "learning_rate": 0.00015537869794387198,
      "loss": 0.6469,
      "step": 47100
    },
    {
      "epoch": 0.6713655133133989,
      "grad_norm": 0.49857380986213684,
      "learning_rate": 0.00015533121230827676,
      "loss": 0.6656,
      "step": 47150
    },
    {
      "epoch": 0.6720774597750249,
      "grad_norm": 0.48798882961273193,
      "learning_rate": 0.0001552837266726815,
      "loss": 0.5888,
      "step": 47200
    },
    {
      "epoch": 0.672789406236651,
      "grad_norm": 0.31256332993507385,
      "learning_rate": 0.0001552362410370863,
      "loss": 0.6415,
      "step": 47250
    },
    {
      "epoch": 0.673501352698277,
      "grad_norm": 0.555732786655426,
      "learning_rate": 0.00015518875540149104,
      "loss": 0.6854,
      "step": 47300
    },
    {
      "epoch": 0.6742132991599031,
      "grad_norm": 0.3267259895801544,
      "learning_rate": 0.00015514126976589582,
      "loss": 0.6536,
      "step": 47350
    },
    {
      "epoch": 0.6749252456215292,
      "grad_norm": 0.5769504308700562,
      "learning_rate": 0.0001550937841303006,
      "loss": 0.6836,
      "step": 47400
    },
    {
      "epoch": 0.6756371920831553,
      "grad_norm": 0.29537519812583923,
      "learning_rate": 0.00015504629849470535,
      "loss": 0.6885,
      "step": 47450
    },
    {
      "epoch": 0.6763491385447814,
      "grad_norm": 0.3123586177825928,
      "learning_rate": 0.00015499881285911012,
      "loss": 0.646,
      "step": 47500
    },
    {
      "epoch": 0.6770610850064075,
      "grad_norm": 0.4370967447757721,
      "learning_rate": 0.00015495132722351487,
      "loss": 0.6567,
      "step": 47550
    },
    {
      "epoch": 0.6777730314680336,
      "grad_norm": 0.31832489371299744,
      "learning_rate": 0.00015490384158791965,
      "loss": 0.677,
      "step": 47600
    },
    {
      "epoch": 0.6784849779296597,
      "grad_norm": 0.4784974157810211,
      "learning_rate": 0.00015485635595232443,
      "loss": 0.6695,
      "step": 47650
    },
    {
      "epoch": 0.6791969243912858,
      "grad_norm": 0.4186631739139557,
      "learning_rate": 0.0001548088703167292,
      "loss": 0.6355,
      "step": 47700
    },
    {
      "epoch": 0.6799088708529119,
      "grad_norm": 0.4213690757751465,
      "learning_rate": 0.00015476138468113396,
      "loss": 0.6671,
      "step": 47750
    },
    {
      "epoch": 0.6806208173145379,
      "grad_norm": 0.6855946779251099,
      "learning_rate": 0.00015471389904553873,
      "loss": 0.6421,
      "step": 47800
    },
    {
      "epoch": 0.681332763776164,
      "grad_norm": 0.5109344124794006,
      "learning_rate": 0.0001546664134099435,
      "loss": 0.6781,
      "step": 47850
    },
    {
      "epoch": 0.6820447102377901,
      "grad_norm": 0.6814456582069397,
      "learning_rate": 0.0001546189277743483,
      "loss": 0.6596,
      "step": 47900
    },
    {
      "epoch": 0.6827566566994162,
      "grad_norm": 0.7664925456047058,
      "learning_rate": 0.00015457144213875304,
      "loss": 0.6234,
      "step": 47950
    },
    {
      "epoch": 0.6834686031610423,
      "grad_norm": 0.6100960373878479,
      "learning_rate": 0.00015452395650315782,
      "loss": 0.6733,
      "step": 48000
    },
    {
      "epoch": 0.6841805496226684,
      "grad_norm": 0.5209154486656189,
      "learning_rate": 0.00015447647086756257,
      "loss": 0.6633,
      "step": 48050
    },
    {
      "epoch": 0.6848924960842945,
      "grad_norm": 0.48574453592300415,
      "learning_rate": 0.00015442898523196735,
      "loss": 0.6595,
      "step": 48100
    },
    {
      "epoch": 0.6856044425459206,
      "grad_norm": 0.5165219306945801,
      "learning_rate": 0.00015438149959637212,
      "loss": 0.7017,
      "step": 48150
    },
    {
      "epoch": 0.6863163890075467,
      "grad_norm": 0.5245000720024109,
      "learning_rate": 0.00015433401396077687,
      "loss": 0.64,
      "step": 48200
    },
    {
      "epoch": 0.6870283354691727,
      "grad_norm": 0.3185290992259979,
      "learning_rate": 0.00015428652832518165,
      "loss": 0.6664,
      "step": 48250
    },
    {
      "epoch": 0.6877402819307988,
      "grad_norm": 0.4568677246570587,
      "learning_rate": 0.0001542390426895864,
      "loss": 0.616,
      "step": 48300
    },
    {
      "epoch": 0.6884522283924249,
      "grad_norm": 0.6878650784492493,
      "learning_rate": 0.00015419155705399118,
      "loss": 0.6394,
      "step": 48350
    },
    {
      "epoch": 0.689164174854051,
      "grad_norm": 0.5854731798171997,
      "learning_rate": 0.00015414407141839596,
      "loss": 0.6792,
      "step": 48400
    },
    {
      "epoch": 0.6898761213156771,
      "grad_norm": 0.49133536219596863,
      "learning_rate": 0.0001540965857828007,
      "loss": 0.632,
      "step": 48450
    },
    {
      "epoch": 0.6905880677773032,
      "grad_norm": 0.3877113461494446,
      "learning_rate": 0.00015404910014720549,
      "loss": 0.6729,
      "step": 48500
    },
    {
      "epoch": 0.6913000142389293,
      "grad_norm": 0.48983219265937805,
      "learning_rate": 0.00015400161451161024,
      "loss": 0.6995,
      "step": 48550
    },
    {
      "epoch": 0.6920119607005554,
      "grad_norm": 0.45993685722351074,
      "learning_rate": 0.000153954128876015,
      "loss": 0.6516,
      "step": 48600
    },
    {
      "epoch": 0.6927239071621814,
      "grad_norm": 0.6738225221633911,
      "learning_rate": 0.0001539066432404198,
      "loss": 0.6133,
      "step": 48650
    },
    {
      "epoch": 0.6934358536238074,
      "grad_norm": 0.3188026547431946,
      "learning_rate": 0.00015385915760482454,
      "loss": 0.6691,
      "step": 48700
    },
    {
      "epoch": 0.6941478000854335,
      "grad_norm": 0.6480088233947754,
      "learning_rate": 0.00015381167196922932,
      "loss": 0.6305,
      "step": 48750
    },
    {
      "epoch": 0.6948597465470596,
      "grad_norm": 0.3403708338737488,
      "learning_rate": 0.00015376418633363407,
      "loss": 0.6246,
      "step": 48800
    },
    {
      "epoch": 0.6955716930086857,
      "grad_norm": 0.48247265815734863,
      "learning_rate": 0.00015371670069803885,
      "loss": 0.5726,
      "step": 48850
    },
    {
      "epoch": 0.6962836394703118,
      "grad_norm": 0.41438671946525574,
      "learning_rate": 0.00015366921506244362,
      "loss": 0.6512,
      "step": 48900
    },
    {
      "epoch": 0.6969955859319379,
      "grad_norm": 0.4729806184768677,
      "learning_rate": 0.00015362172942684838,
      "loss": 0.6348,
      "step": 48950
    },
    {
      "epoch": 0.697707532393564,
      "grad_norm": 0.7328174114227295,
      "learning_rate": 0.00015357424379125315,
      "loss": 0.7067,
      "step": 49000
    },
    {
      "epoch": 0.6984194788551901,
      "grad_norm": 0.581019937992096,
      "learning_rate": 0.0001535267581556579,
      "loss": 0.6795,
      "step": 49050
    },
    {
      "epoch": 0.6991314253168162,
      "grad_norm": 0.4489437937736511,
      "learning_rate": 0.00015347927252006268,
      "loss": 0.7106,
      "step": 49100
    },
    {
      "epoch": 0.6998433717784422,
      "grad_norm": 0.7372879385948181,
      "learning_rate": 0.00015343178688446746,
      "loss": 0.7186,
      "step": 49150
    },
    {
      "epoch": 0.7005553182400683,
      "grad_norm": 0.4623333215713501,
      "learning_rate": 0.0001533843012488722,
      "loss": 0.6292,
      "step": 49200
    },
    {
      "epoch": 0.7012672647016944,
      "grad_norm": 0.4078308641910553,
      "learning_rate": 0.000153336815613277,
      "loss": 0.6833,
      "step": 49250
    },
    {
      "epoch": 0.7019792111633205,
      "grad_norm": 0.7805388569831848,
      "learning_rate": 0.00015328932997768174,
      "loss": 0.7351,
      "step": 49300
    },
    {
      "epoch": 0.7026911576249466,
      "grad_norm": 0.7713062167167664,
      "learning_rate": 0.00015324184434208651,
      "loss": 0.6332,
      "step": 49350
    },
    {
      "epoch": 0.7034031040865727,
      "grad_norm": 0.5968050360679626,
      "learning_rate": 0.0001531943587064913,
      "loss": 0.6233,
      "step": 49400
    },
    {
      "epoch": 0.7041150505481988,
      "grad_norm": 0.2940697968006134,
      "learning_rate": 0.00015314687307089607,
      "loss": 0.7172,
      "step": 49450
    },
    {
      "epoch": 0.7048269970098249,
      "grad_norm": 0.4724520742893219,
      "learning_rate": 0.00015309938743530082,
      "loss": 0.6849,
      "step": 49500
    },
    {
      "epoch": 0.705538943471451,
      "grad_norm": 0.7390763759613037,
      "learning_rate": 0.0001530519017997056,
      "loss": 0.6872,
      "step": 49550
    },
    {
      "epoch": 0.706250889933077,
      "grad_norm": 0.5218976140022278,
      "learning_rate": 0.00015300441616411038,
      "loss": 0.6432,
      "step": 49600
    },
    {
      "epoch": 0.7069628363947031,
      "grad_norm": 0.4342899024486542,
      "learning_rate": 0.00015295693052851515,
      "loss": 0.684,
      "step": 49650
    },
    {
      "epoch": 0.7076747828563292,
      "grad_norm": 0.4230656325817108,
      "learning_rate": 0.0001529094448929199,
      "loss": 0.6393,
      "step": 49700
    },
    {
      "epoch": 0.7083867293179553,
      "grad_norm": 0.7112512588500977,
      "learning_rate": 0.00015286195925732468,
      "loss": 0.6466,
      "step": 49750
    },
    {
      "epoch": 0.7090986757795814,
      "grad_norm": 0.5896084308624268,
      "learning_rate": 0.00015281447362172943,
      "loss": 0.6315,
      "step": 49800
    },
    {
      "epoch": 0.7098106222412075,
      "grad_norm": 0.32071611285209656,
      "learning_rate": 0.0001527669879861342,
      "loss": 0.6166,
      "step": 49850
    },
    {
      "epoch": 0.7105225687028336,
      "grad_norm": 0.7654476761817932,
      "learning_rate": 0.000152719502350539,
      "loss": 0.6513,
      "step": 49900
    },
    {
      "epoch": 0.7112345151644597,
      "grad_norm": 0.7380357384681702,
      "learning_rate": 0.00015267201671494374,
      "loss": 0.6122,
      "step": 49950
    },
    {
      "epoch": 0.7119464616260858,
      "grad_norm": 0.6486925482749939,
      "learning_rate": 0.00015262453107934852,
      "loss": 0.6452,
      "step": 50000
    },
    {
      "epoch": 0.7126584080877119,
      "grad_norm": 0.7676141262054443,
      "learning_rate": 0.00015257704544375327,
      "loss": 0.6897,
      "step": 50050
    },
    {
      "epoch": 0.7133703545493378,
      "grad_norm": 0.7874782085418701,
      "learning_rate": 0.00015252955980815804,
      "loss": 0.6264,
      "step": 50100
    },
    {
      "epoch": 0.7140823010109639,
      "grad_norm": 0.47979736328125,
      "learning_rate": 0.00015248207417256282,
      "loss": 0.6413,
      "step": 50150
    },
    {
      "epoch": 0.71479424747259,
      "grad_norm": 0.9408154487609863,
      "learning_rate": 0.0001524355382496795,
      "loss": 0.7159,
      "step": 50200
    },
    {
      "epoch": 0.7155061939342161,
      "grad_norm": 0.6114115118980408,
      "learning_rate": 0.00015238805261408425,
      "loss": 0.6059,
      "step": 50250
    },
    {
      "epoch": 0.7162181403958422,
      "grad_norm": 0.3411446213722229,
      "learning_rate": 0.00015234056697848903,
      "loss": 0.6729,
      "step": 50300
    },
    {
      "epoch": 0.7169300868574683,
      "grad_norm": 0.5738707184791565,
      "learning_rate": 0.00015229308134289378,
      "loss": 0.5997,
      "step": 50350
    },
    {
      "epoch": 0.7176420333190944,
      "grad_norm": 0.6012222766876221,
      "learning_rate": 0.00015224559570729856,
      "loss": 0.7144,
      "step": 50400
    },
    {
      "epoch": 0.7183539797807205,
      "grad_norm": 0.5346806049346924,
      "learning_rate": 0.00015219811007170333,
      "loss": 0.658,
      "step": 50450
    },
    {
      "epoch": 0.7190659262423466,
      "grad_norm": 0.4049279987812042,
      "learning_rate": 0.00015215062443610808,
      "loss": 0.6853,
      "step": 50500
    },
    {
      "epoch": 0.7197778727039726,
      "grad_norm": 0.4881901443004608,
      "learning_rate": 0.00015210313880051286,
      "loss": 0.625,
      "step": 50550
    },
    {
      "epoch": 0.7204898191655987,
      "grad_norm": 0.9522566795349121,
      "learning_rate": 0.0001520556531649176,
      "loss": 0.6548,
      "step": 50600
    },
    {
      "epoch": 0.7212017656272248,
      "grad_norm": 0.641697108745575,
      "learning_rate": 0.0001520081675293224,
      "loss": 0.6185,
      "step": 50650
    },
    {
      "epoch": 0.7219137120888509,
      "grad_norm": 0.5728837847709656,
      "learning_rate": 0.00015196068189372717,
      "loss": 0.6114,
      "step": 50700
    },
    {
      "epoch": 0.722625658550477,
      "grad_norm": 0.39745137095451355,
      "learning_rate": 0.00015191319625813192,
      "loss": 0.6962,
      "step": 50750
    },
    {
      "epoch": 0.7233376050121031,
      "grad_norm": 0.45873335003852844,
      "learning_rate": 0.0001518657106225367,
      "loss": 0.6395,
      "step": 50800
    },
    {
      "epoch": 0.7240495514737292,
      "grad_norm": 0.4952635169029236,
      "learning_rate": 0.00015181822498694145,
      "loss": 0.6851,
      "step": 50850
    },
    {
      "epoch": 0.7247614979353553,
      "grad_norm": 0.5667476058006287,
      "learning_rate": 0.00015177073935134622,
      "loss": 0.6956,
      "step": 50900
    },
    {
      "epoch": 0.7254734443969814,
      "grad_norm": 0.6184585690498352,
      "learning_rate": 0.000151723253715751,
      "loss": 0.6721,
      "step": 50950
    },
    {
      "epoch": 0.7261853908586074,
      "grad_norm": 0.4591776728630066,
      "learning_rate": 0.00015167576808015575,
      "loss": 0.6443,
      "step": 51000
    },
    {
      "epoch": 0.7268973373202335,
      "grad_norm": 0.5385019183158875,
      "learning_rate": 0.00015162828244456053,
      "loss": 0.6527,
      "step": 51050
    },
    {
      "epoch": 0.7276092837818596,
      "grad_norm": 0.46326881647109985,
      "learning_rate": 0.00015158079680896528,
      "loss": 0.6771,
      "step": 51100
    },
    {
      "epoch": 0.7283212302434857,
      "grad_norm": 0.409527987241745,
      "learning_rate": 0.00015153331117337006,
      "loss": 0.6176,
      "step": 51150
    },
    {
      "epoch": 0.7290331767051118,
      "grad_norm": 0.4015268087387085,
      "learning_rate": 0.00015148582553777484,
      "loss": 0.6748,
      "step": 51200
    },
    {
      "epoch": 0.7297451231667379,
      "grad_norm": 0.44920608401298523,
      "learning_rate": 0.00015143833990217959,
      "loss": 0.6186,
      "step": 51250
    },
    {
      "epoch": 0.730457069628364,
      "grad_norm": 0.4452069103717804,
      "learning_rate": 0.00015139085426658436,
      "loss": 0.6768,
      "step": 51300
    },
    {
      "epoch": 0.7311690160899901,
      "grad_norm": 0.5846912860870361,
      "learning_rate": 0.00015134336863098911,
      "loss": 0.6447,
      "step": 51350
    },
    {
      "epoch": 0.7318809625516162,
      "grad_norm": 0.6120848059654236,
      "learning_rate": 0.0001512958829953939,
      "loss": 0.6166,
      "step": 51400
    },
    {
      "epoch": 0.7325929090132423,
      "grad_norm": 0.5459585189819336,
      "learning_rate": 0.00015124839735979867,
      "loss": 0.6899,
      "step": 51450
    },
    {
      "epoch": 0.7333048554748683,
      "grad_norm": 0.5442039966583252,
      "learning_rate": 0.00015120091172420342,
      "loss": 0.7233,
      "step": 51500
    },
    {
      "epoch": 0.7340168019364943,
      "grad_norm": 0.49574437737464905,
      "learning_rate": 0.0001511534260886082,
      "loss": 0.7045,
      "step": 51550
    },
    {
      "epoch": 0.7347287483981204,
      "grad_norm": 0.4665497839450836,
      "learning_rate": 0.00015110594045301298,
      "loss": 0.6137,
      "step": 51600
    },
    {
      "epoch": 0.7354406948597465,
      "grad_norm": 0.3602527379989624,
      "learning_rate": 0.00015105845481741773,
      "loss": 0.6534,
      "step": 51650
    },
    {
      "epoch": 0.7361526413213726,
      "grad_norm": 0.3397287130355835,
      "learning_rate": 0.0001510109691818225,
      "loss": 0.6872,
      "step": 51700
    },
    {
      "epoch": 0.7368645877829987,
      "grad_norm": 0.5945467948913574,
      "learning_rate": 0.00015096348354622728,
      "loss": 0.7025,
      "step": 51750
    },
    {
      "epoch": 0.7375765342446248,
      "grad_norm": 0.8463958501815796,
      "learning_rate": 0.00015091599791063206,
      "loss": 0.7152,
      "step": 51800
    },
    {
      "epoch": 0.7382884807062509,
      "grad_norm": 0.6956467628479004,
      "learning_rate": 0.0001508685122750368,
      "loss": 0.7162,
      "step": 51850
    },
    {
      "epoch": 0.739000427167877,
      "grad_norm": 0.42221400141716003,
      "learning_rate": 0.0001508210266394416,
      "loss": 0.6681,
      "step": 51900
    },
    {
      "epoch": 0.739712373629503,
      "grad_norm": 0.5346534848213196,
      "learning_rate": 0.00015077354100384636,
      "loss": 0.6645,
      "step": 51950
    },
    {
      "epoch": 0.7404243200911291,
      "grad_norm": 0.44960644841194153,
      "learning_rate": 0.00015072605536825111,
      "loss": 0.6146,
      "step": 52000
    },
    {
      "epoch": 0.7411362665527552,
      "grad_norm": 0.5464375019073486,
      "learning_rate": 0.0001506785697326559,
      "loss": 0.675,
      "step": 52050
    },
    {
      "epoch": 0.7418482130143813,
      "grad_norm": 0.9353196620941162,
      "learning_rate": 0.00015063108409706064,
      "loss": 0.6824,
      "step": 52100
    },
    {
      "epoch": 0.7425601594760074,
      "grad_norm": 0.5059053897857666,
      "learning_rate": 0.00015058359846146542,
      "loss": 0.6532,
      "step": 52150
    },
    {
      "epoch": 0.7432721059376335,
      "grad_norm": 0.5275987982749939,
      "learning_rate": 0.0001505361128258702,
      "loss": 0.6556,
      "step": 52200
    },
    {
      "epoch": 0.7439840523992596,
      "grad_norm": 0.5501773357391357,
      "learning_rate": 0.00015048862719027495,
      "loss": 0.6705,
      "step": 52250
    },
    {
      "epoch": 0.7446959988608857,
      "grad_norm": 0.4061739444732666,
      "learning_rate": 0.00015044114155467973,
      "loss": 0.6275,
      "step": 52300
    },
    {
      "epoch": 0.7454079453225118,
      "grad_norm": 0.5070427060127258,
      "learning_rate": 0.00015039365591908448,
      "loss": 0.6739,
      "step": 52350
    },
    {
      "epoch": 0.7461198917841378,
      "grad_norm": 0.527820885181427,
      "learning_rate": 0.00015034617028348925,
      "loss": 0.6764,
      "step": 52400
    },
    {
      "epoch": 0.7468318382457639,
      "grad_norm": 0.3980223834514618,
      "learning_rate": 0.00015029868464789403,
      "loss": 0.6023,
      "step": 52450
    },
    {
      "epoch": 0.74754378470739,
      "grad_norm": 0.7076829671859741,
      "learning_rate": 0.0001502521487250107,
      "loss": 0.6484,
      "step": 52500
    },
    {
      "epoch": 0.7482557311690161,
      "grad_norm": 0.7146350741386414,
      "learning_rate": 0.00015020466308941546,
      "loss": 0.6387,
      "step": 52550
    },
    {
      "epoch": 0.7489676776306422,
      "grad_norm": 0.5726128220558167,
      "learning_rate": 0.00015015717745382024,
      "loss": 0.6385,
      "step": 52600
    },
    {
      "epoch": 0.7496796240922683,
      "grad_norm": 0.7571142315864563,
      "learning_rate": 0.000150109691818225,
      "loss": 0.6538,
      "step": 52650
    },
    {
      "epoch": 0.7503915705538944,
      "grad_norm": 0.47790849208831787,
      "learning_rate": 0.00015006220618262977,
      "loss": 0.6779,
      "step": 52700
    },
    {
      "epoch": 0.7511035170155205,
      "grad_norm": 0.3446134626865387,
      "learning_rate": 0.00015001472054703455,
      "loss": 0.6928,
      "step": 52750
    },
    {
      "epoch": 0.7518154634771466,
      "grad_norm": 0.6085982918739319,
      "learning_rate": 0.0001499672349114393,
      "loss": 0.6523,
      "step": 52800
    },
    {
      "epoch": 0.7525274099387727,
      "grad_norm": 0.6263906955718994,
      "learning_rate": 0.00014991974927584407,
      "loss": 0.6375,
      "step": 52850
    },
    {
      "epoch": 0.7532393564003987,
      "grad_norm": 0.5902153849601746,
      "learning_rate": 0.00014987226364024882,
      "loss": 0.6895,
      "step": 52900
    },
    {
      "epoch": 0.7539513028620248,
      "grad_norm": 0.25811436772346497,
      "learning_rate": 0.0001498247780046536,
      "loss": 0.6728,
      "step": 52950
    },
    {
      "epoch": 0.7546632493236508,
      "grad_norm": 0.4378393292427063,
      "learning_rate": 0.00014977729236905838,
      "loss": 0.6514,
      "step": 53000
    },
    {
      "epoch": 0.7553751957852769,
      "grad_norm": 0.5032783150672913,
      "learning_rate": 0.00014972980673346313,
      "loss": 0.6975,
      "step": 53050
    },
    {
      "epoch": 0.756087142246903,
      "grad_norm": 0.5786357522010803,
      "learning_rate": 0.0001496832708105798,
      "loss": 0.7412,
      "step": 53100
    },
    {
      "epoch": 0.7567990887085291,
      "grad_norm": 0.33768150210380554,
      "learning_rate": 0.00014963578517498459,
      "loss": 0.6385,
      "step": 53150
    },
    {
      "epoch": 0.7575110351701552,
      "grad_norm": 0.6135404109954834,
      "learning_rate": 0.00014958829953938934,
      "loss": 0.6978,
      "step": 53200
    },
    {
      "epoch": 0.7582229816317813,
      "grad_norm": 0.5276135206222534,
      "learning_rate": 0.00014954081390379411,
      "loss": 0.6341,
      "step": 53250
    },
    {
      "epoch": 0.7589349280934073,
      "grad_norm": 0.5506346225738525,
      "learning_rate": 0.0001494933282681989,
      "loss": 0.639,
      "step": 53300
    },
    {
      "epoch": 0.7596468745550334,
      "grad_norm": 0.5938308835029602,
      "learning_rate": 0.00014944584263260364,
      "loss": 0.6352,
      "step": 53350
    },
    {
      "epoch": 0.7603588210166595,
      "grad_norm": 0.36114591360092163,
      "learning_rate": 0.00014939835699700842,
      "loss": 0.6017,
      "step": 53400
    },
    {
      "epoch": 0.7610707674782856,
      "grad_norm": 0.43865731358528137,
      "learning_rate": 0.00014935087136141317,
      "loss": 0.6846,
      "step": 53450
    },
    {
      "epoch": 0.7617827139399117,
      "grad_norm": 0.44970861077308655,
      "learning_rate": 0.00014930338572581795,
      "loss": 0.6335,
      "step": 53500
    },
    {
      "epoch": 0.7624946604015378,
      "grad_norm": 0.28893738985061646,
      "learning_rate": 0.00014925590009022273,
      "loss": 0.6528,
      "step": 53550
    },
    {
      "epoch": 0.7632066068631639,
      "grad_norm": 0.48146525025367737,
      "learning_rate": 0.00014920841445462748,
      "loss": 0.6381,
      "step": 53600
    },
    {
      "epoch": 0.76391855332479,
      "grad_norm": 0.4622558653354645,
      "learning_rate": 0.00014916092881903225,
      "loss": 0.6393,
      "step": 53650
    },
    {
      "epoch": 0.7646304997864161,
      "grad_norm": 0.4952245354652405,
      "learning_rate": 0.000149113443183437,
      "loss": 0.6936,
      "step": 53700
    },
    {
      "epoch": 0.7653424462480422,
      "grad_norm": 0.46662548184394836,
      "learning_rate": 0.00014906595754784178,
      "loss": 0.6445,
      "step": 53750
    },
    {
      "epoch": 0.7660543927096682,
      "grad_norm": 0.38530704379081726,
      "learning_rate": 0.00014901847191224656,
      "loss": 0.6901,
      "step": 53800
    },
    {
      "epoch": 0.7667663391712943,
      "grad_norm": 0.3345043957233429,
      "learning_rate": 0.0001489709862766513,
      "loss": 0.6122,
      "step": 53850
    },
    {
      "epoch": 0.7674782856329204,
      "grad_norm": 0.44318434596061707,
      "learning_rate": 0.0001489235006410561,
      "loss": 0.6558,
      "step": 53900
    },
    {
      "epoch": 0.7681902320945465,
      "grad_norm": 0.39026886224746704,
      "learning_rate": 0.00014887601500546084,
      "loss": 0.6718,
      "step": 53950
    },
    {
      "epoch": 0.7689021785561726,
      "grad_norm": 0.7338082790374756,
      "learning_rate": 0.00014882852936986562,
      "loss": 0.6153,
      "step": 54000
    },
    {
      "epoch": 0.7696141250177987,
      "grad_norm": 0.6740036606788635,
      "learning_rate": 0.0001487810437342704,
      "loss": 0.6977,
      "step": 54050
    },
    {
      "epoch": 0.7703260714794248,
      "grad_norm": 0.27102038264274597,
      "learning_rate": 0.00014873355809867514,
      "loss": 0.6543,
      "step": 54100
    },
    {
      "epoch": 0.7710380179410509,
      "grad_norm": 0.7167770266532898,
      "learning_rate": 0.00014868607246307992,
      "loss": 0.6861,
      "step": 54150
    },
    {
      "epoch": 0.771749964402677,
      "grad_norm": 0.3879164755344391,
      "learning_rate": 0.00014863858682748467,
      "loss": 0.6257,
      "step": 54200
    },
    {
      "epoch": 0.772461910864303,
      "grad_norm": 0.5244269371032715,
      "learning_rate": 0.00014859110119188945,
      "loss": 0.6718,
      "step": 54250
    },
    {
      "epoch": 0.7731738573259291,
      "grad_norm": 0.47350865602493286,
      "learning_rate": 0.00014854361555629423,
      "loss": 0.6329,
      "step": 54300
    },
    {
      "epoch": 0.7738858037875552,
      "grad_norm": 0.5041874051094055,
      "learning_rate": 0.00014849612992069898,
      "loss": 0.6603,
      "step": 54350
    },
    {
      "epoch": 0.7745977502491813,
      "grad_norm": 0.5707675814628601,
      "learning_rate": 0.00014844864428510376,
      "loss": 0.7005,
      "step": 54400
    },
    {
      "epoch": 0.7753096967108073,
      "grad_norm": 0.4030483663082123,
      "learning_rate": 0.00014840115864950853,
      "loss": 0.7366,
      "step": 54450
    },
    {
      "epoch": 0.7760216431724334,
      "grad_norm": 0.44971519708633423,
      "learning_rate": 0.00014835367301391328,
      "loss": 0.643,
      "step": 54500
    },
    {
      "epoch": 0.7767335896340595,
      "grad_norm": 0.5172896385192871,
      "learning_rate": 0.00014830618737831806,
      "loss": 0.6687,
      "step": 54550
    },
    {
      "epoch": 0.7774455360956856,
      "grad_norm": 0.4884103238582611,
      "learning_rate": 0.00014825870174272284,
      "loss": 0.6511,
      "step": 54600
    },
    {
      "epoch": 0.7781574825573117,
      "grad_norm": 0.6099235415458679,
      "learning_rate": 0.00014821121610712762,
      "loss": 0.6448,
      "step": 54650
    },
    {
      "epoch": 0.7788694290189377,
      "grad_norm": 0.6100174188613892,
      "learning_rate": 0.00014816373047153237,
      "loss": 0.6401,
      "step": 54700
    },
    {
      "epoch": 0.7795813754805638,
      "grad_norm": 0.660361111164093,
      "learning_rate": 0.00014811624483593714,
      "loss": 0.6479,
      "step": 54750
    },
    {
      "epoch": 0.7802933219421899,
      "grad_norm": 0.9233698844909668,
      "learning_rate": 0.00014806875920034192,
      "loss": 0.6581,
      "step": 54800
    },
    {
      "epoch": 0.781005268403816,
      "grad_norm": 0.41343629360198975,
      "learning_rate": 0.00014802127356474667,
      "loss": 0.6978,
      "step": 54850
    },
    {
      "epoch": 0.7817172148654421,
      "grad_norm": 0.4334467649459839,
      "learning_rate": 0.00014797378792915145,
      "loss": 0.6949,
      "step": 54900
    },
    {
      "epoch": 0.7824291613270682,
      "grad_norm": 0.9354721903800964,
      "learning_rate": 0.0001479263022935562,
      "loss": 0.6924,
      "step": 54950
    },
    {
      "epoch": 0.7831411077886943,
      "grad_norm": 0.5863620638847351,
      "learning_rate": 0.00014787881665796098,
      "loss": 0.635,
      "step": 55000
    },
    {
      "epoch": 0.7838530542503204,
      "grad_norm": 0.6424823999404907,
      "learning_rate": 0.00014783133102236576,
      "loss": 0.6669,
      "step": 55050
    },
    {
      "epoch": 0.7845650007119465,
      "grad_norm": 0.5241668820381165,
      "learning_rate": 0.0001477838453867705,
      "loss": 0.6689,
      "step": 55100
    },
    {
      "epoch": 0.7852769471735725,
      "grad_norm": 0.7331588864326477,
      "learning_rate": 0.00014773635975117528,
      "loss": 0.6607,
      "step": 55150
    },
    {
      "epoch": 0.7859888936351986,
      "grad_norm": 0.6578388214111328,
      "learning_rate": 0.00014768887411558003,
      "loss": 0.6651,
      "step": 55200
    },
    {
      "epoch": 0.7867008400968247,
      "grad_norm": 0.5165347456932068,
      "learning_rate": 0.0001476413884799848,
      "loss": 0.7123,
      "step": 55250
    },
    {
      "epoch": 0.7874127865584508,
      "grad_norm": 0.47031471133232117,
      "learning_rate": 0.0001475939028443896,
      "loss": 0.6271,
      "step": 55300
    },
    {
      "epoch": 0.7881247330200769,
      "grad_norm": 0.5204435586929321,
      "learning_rate": 0.00014754641720879434,
      "loss": 0.696,
      "step": 55350
    },
    {
      "epoch": 0.788836679481703,
      "grad_norm": 0.47438347339630127,
      "learning_rate": 0.00014749893157319912,
      "loss": 0.6524,
      "step": 55400
    },
    {
      "epoch": 0.7895486259433291,
      "grad_norm": 0.31253984570503235,
      "learning_rate": 0.00014745144593760387,
      "loss": 0.6232,
      "step": 55450
    },
    {
      "epoch": 0.7902605724049552,
      "grad_norm": 0.555057168006897,
      "learning_rate": 0.00014740396030200865,
      "loss": 0.7031,
      "step": 55500
    },
    {
      "epoch": 0.7909725188665813,
      "grad_norm": 0.573314368724823,
      "learning_rate": 0.00014735647466641342,
      "loss": 0.6326,
      "step": 55550
    },
    {
      "epoch": 0.7916844653282074,
      "grad_norm": 0.20816078782081604,
      "learning_rate": 0.00014730898903081817,
      "loss": 0.6175,
      "step": 55600
    },
    {
      "epoch": 0.7923964117898334,
      "grad_norm": 0.7669435143470764,
      "learning_rate": 0.00014726150339522295,
      "loss": 0.6945,
      "step": 55650
    },
    {
      "epoch": 0.7931083582514595,
      "grad_norm": 0.3491293787956238,
      "learning_rate": 0.0001472140177596277,
      "loss": 0.6521,
      "step": 55700
    },
    {
      "epoch": 0.7938203047130856,
      "grad_norm": 0.6844140887260437,
      "learning_rate": 0.00014716653212403248,
      "loss": 0.6902,
      "step": 55750
    },
    {
      "epoch": 0.7945322511747117,
      "grad_norm": 0.5104662775993347,
      "learning_rate": 0.00014711904648843726,
      "loss": 0.6103,
      "step": 55800
    },
    {
      "epoch": 0.7952441976363378,
      "grad_norm": 0.47241950035095215,
      "learning_rate": 0.000147071560852842,
      "loss": 0.665,
      "step": 55850
    },
    {
      "epoch": 0.7959561440979638,
      "grad_norm": 0.3689918518066406,
      "learning_rate": 0.00014702407521724679,
      "loss": 0.6574,
      "step": 55900
    },
    {
      "epoch": 0.7966680905595899,
      "grad_norm": 0.309067964553833,
      "learning_rate": 0.00014697658958165154,
      "loss": 0.6668,
      "step": 55950
    },
    {
      "epoch": 0.797380037021216,
      "grad_norm": 0.6456667184829712,
      "learning_rate": 0.00014692910394605631,
      "loss": 0.6183,
      "step": 56000
    },
    {
      "epoch": 0.798091983482842,
      "grad_norm": 0.5600939989089966,
      "learning_rate": 0.0001468816183104611,
      "loss": 0.6744,
      "step": 56050
    },
    {
      "epoch": 0.7988039299444681,
      "grad_norm": 0.5105839967727661,
      "learning_rate": 0.00014683413267486584,
      "loss": 0.6438,
      "step": 56100
    },
    {
      "epoch": 0.7995158764060942,
      "grad_norm": 0.39129960536956787,
      "learning_rate": 0.00014678664703927062,
      "loss": 0.6446,
      "step": 56150
    },
    {
      "epoch": 0.8002278228677203,
      "grad_norm": 0.7712526917457581,
      "learning_rate": 0.0001467391614036754,
      "loss": 0.6286,
      "step": 56200
    },
    {
      "epoch": 0.8009397693293464,
      "grad_norm": 0.5799089074134827,
      "learning_rate": 0.00014669167576808017,
      "loss": 0.6073,
      "step": 56250
    },
    {
      "epoch": 0.8016517157909725,
      "grad_norm": 0.576008141040802,
      "learning_rate": 0.00014664419013248493,
      "loss": 0.7001,
      "step": 56300
    },
    {
      "epoch": 0.8023636622525986,
      "grad_norm": 0.5155251622200012,
      "learning_rate": 0.0001465967044968897,
      "loss": 0.6678,
      "step": 56350
    },
    {
      "epoch": 0.8030756087142247,
      "grad_norm": 0.41183680295944214,
      "learning_rate": 0.00014654921886129448,
      "loss": 0.6756,
      "step": 56400
    },
    {
      "epoch": 0.8037875551758508,
      "grad_norm": 0.48599815368652344,
      "learning_rate": 0.00014650173322569923,
      "loss": 0.5961,
      "step": 56450
    },
    {
      "epoch": 0.8044995016374769,
      "grad_norm": 0.6090569496154785,
      "learning_rate": 0.000146454247590104,
      "loss": 0.6442,
      "step": 56500
    },
    {
      "epoch": 0.8052114480991029,
      "grad_norm": 0.484708696603775,
      "learning_rate": 0.00014640676195450879,
      "loss": 0.7013,
      "step": 56550
    },
    {
      "epoch": 0.805923394560729,
      "grad_norm": 0.5934929847717285,
      "learning_rate": 0.00014635927631891354,
      "loss": 0.6725,
      "step": 56600
    },
    {
      "epoch": 0.8066353410223551,
      "grad_norm": 0.48158812522888184,
      "learning_rate": 0.00014631179068331831,
      "loss": 0.6378,
      "step": 56650
    },
    {
      "epoch": 0.8073472874839812,
      "grad_norm": 0.5587996244430542,
      "learning_rate": 0.00014626430504772306,
      "loss": 0.6078,
      "step": 56700
    },
    {
      "epoch": 0.8080592339456073,
      "grad_norm": 0.38437020778656006,
      "learning_rate": 0.00014621681941212784,
      "loss": 0.6533,
      "step": 56750
    },
    {
      "epoch": 0.8087711804072334,
      "grad_norm": 0.4676339328289032,
      "learning_rate": 0.00014616933377653262,
      "loss": 0.6813,
      "step": 56800
    },
    {
      "epoch": 0.8094831268688595,
      "grad_norm": 0.30341318249702454,
      "learning_rate": 0.00014612184814093737,
      "loss": 0.7019,
      "step": 56850
    },
    {
      "epoch": 0.8101950733304856,
      "grad_norm": 0.6247700452804565,
      "learning_rate": 0.00014607436250534215,
      "loss": 0.6239,
      "step": 56900
    },
    {
      "epoch": 0.8109070197921117,
      "grad_norm": 0.5998862981796265,
      "learning_rate": 0.0001460268768697469,
      "loss": 0.6611,
      "step": 56950
    },
    {
      "epoch": 0.8116189662537378,
      "grad_norm": 0.4279775321483612,
      "learning_rate": 0.00014597939123415168,
      "loss": 0.6618,
      "step": 57000
    },
    {
      "epoch": 0.8123309127153638,
      "grad_norm": 0.47005918622016907,
      "learning_rate": 0.00014593190559855645,
      "loss": 0.6613,
      "step": 57050
    },
    {
      "epoch": 0.8130428591769899,
      "grad_norm": 0.4581316411495209,
      "learning_rate": 0.0001458844199629612,
      "loss": 0.6698,
      "step": 57100
    },
    {
      "epoch": 0.813754805638616,
      "grad_norm": 0.6445809602737427,
      "learning_rate": 0.00014583693432736598,
      "loss": 0.6966,
      "step": 57150
    },
    {
      "epoch": 0.8144667521002421,
      "grad_norm": 0.6508769989013672,
      "learning_rate": 0.00014578944869177073,
      "loss": 0.7119,
      "step": 57200
    },
    {
      "epoch": 0.8151786985618682,
      "grad_norm": 0.5795552730560303,
      "learning_rate": 0.0001457419630561755,
      "loss": 0.6238,
      "step": 57250
    },
    {
      "epoch": 0.8158906450234943,
      "grad_norm": 0.5830053687095642,
      "learning_rate": 0.0001456944774205803,
      "loss": 0.6634,
      "step": 57300
    },
    {
      "epoch": 0.8166025914851203,
      "grad_norm": 0.7213240265846252,
      "learning_rate": 0.00014564699178498504,
      "loss": 0.6921,
      "step": 57350
    },
    {
      "epoch": 0.8173145379467464,
      "grad_norm": 0.3197375535964966,
      "learning_rate": 0.00014559950614938982,
      "loss": 0.6325,
      "step": 57400
    },
    {
      "epoch": 0.8180264844083724,
      "grad_norm": 0.33142176270484924,
      "learning_rate": 0.00014555202051379457,
      "loss": 0.5975,
      "step": 57450
    },
    {
      "epoch": 0.8187384308699985,
      "grad_norm": 0.4855876564979553,
      "learning_rate": 0.00014550548459091127,
      "loss": 0.6345,
      "step": 57500
    },
    {
      "epoch": 0.8194503773316246,
      "grad_norm": 0.7260386943817139,
      "learning_rate": 0.00014545799895531602,
      "loss": 0.6797,
      "step": 57550
    },
    {
      "epoch": 0.8201623237932507,
      "grad_norm": 0.5086742043495178,
      "learning_rate": 0.0001454105133197208,
      "loss": 0.7002,
      "step": 57600
    },
    {
      "epoch": 0.8208742702548768,
      "grad_norm": 0.6544076800346375,
      "learning_rate": 0.00014536302768412555,
      "loss": 0.6856,
      "step": 57650
    },
    {
      "epoch": 0.8215862167165029,
      "grad_norm": 0.43369433283805847,
      "learning_rate": 0.00014531554204853033,
      "loss": 0.6425,
      "step": 57700
    },
    {
      "epoch": 0.822298163178129,
      "grad_norm": 0.47183677554130554,
      "learning_rate": 0.0001452680564129351,
      "loss": 0.6546,
      "step": 57750
    },
    {
      "epoch": 0.8230101096397551,
      "grad_norm": 0.5124987959861755,
      "learning_rate": 0.00014522057077733986,
      "loss": 0.6811,
      "step": 57800
    },
    {
      "epoch": 0.8237220561013812,
      "grad_norm": 0.5114421844482422,
      "learning_rate": 0.00014517308514174463,
      "loss": 0.6728,
      "step": 57850
    },
    {
      "epoch": 0.8244340025630073,
      "grad_norm": 0.8106206655502319,
      "learning_rate": 0.00014512559950614939,
      "loss": 0.6661,
      "step": 57900
    },
    {
      "epoch": 0.8251459490246333,
      "grad_norm": 0.4629417359828949,
      "learning_rate": 0.00014507811387055416,
      "loss": 0.7001,
      "step": 57950
    },
    {
      "epoch": 0.8258578954862594,
      "grad_norm": 0.6639186143875122,
      "learning_rate": 0.00014503062823495894,
      "loss": 0.6467,
      "step": 58000
    },
    {
      "epoch": 0.8265698419478855,
      "grad_norm": 0.8629668354988098,
      "learning_rate": 0.0001449831425993637,
      "loss": 0.6669,
      "step": 58050
    },
    {
      "epoch": 0.8272817884095116,
      "grad_norm": 0.8317816257476807,
      "learning_rate": 0.00014493565696376847,
      "loss": 0.6728,
      "step": 58100
    },
    {
      "epoch": 0.8279937348711377,
      "grad_norm": 0.31390172243118286,
      "learning_rate": 0.00014488817132817322,
      "loss": 0.6581,
      "step": 58150
    },
    {
      "epoch": 0.8287056813327638,
      "grad_norm": 0.4370722770690918,
      "learning_rate": 0.000144840685692578,
      "loss": 0.642,
      "step": 58200
    },
    {
      "epoch": 0.8294176277943899,
      "grad_norm": 0.6302483677864075,
      "learning_rate": 0.00014479320005698277,
      "loss": 0.6203,
      "step": 58250
    },
    {
      "epoch": 0.830129574256016,
      "grad_norm": 0.6043593883514404,
      "learning_rate": 0.00014474571442138752,
      "loss": 0.6674,
      "step": 58300
    },
    {
      "epoch": 0.8308415207176421,
      "grad_norm": 0.3907907009124756,
      "learning_rate": 0.0001446982287857923,
      "loss": 0.645,
      "step": 58350
    },
    {
      "epoch": 0.8315534671792681,
      "grad_norm": 0.5559673309326172,
      "learning_rate": 0.00014465074315019705,
      "loss": 0.6575,
      "step": 58400
    },
    {
      "epoch": 0.8322654136408942,
      "grad_norm": 0.4541856348514557,
      "learning_rate": 0.00014460325751460183,
      "loss": 0.6502,
      "step": 58450
    },
    {
      "epoch": 0.8329773601025203,
      "grad_norm": 0.47906237840652466,
      "learning_rate": 0.0001445557718790066,
      "loss": 0.6752,
      "step": 58500
    },
    {
      "epoch": 0.8336893065641464,
      "grad_norm": 0.6326133012771606,
      "learning_rate": 0.00014450828624341139,
      "loss": 0.6511,
      "step": 58550
    },
    {
      "epoch": 0.8344012530257725,
      "grad_norm": 0.6200360655784607,
      "learning_rate": 0.00014446080060781614,
      "loss": 0.6351,
      "step": 58600
    },
    {
      "epoch": 0.8351131994873986,
      "grad_norm": 0.7428551316261292,
      "learning_rate": 0.00014441331497222091,
      "loss": 0.6455,
      "step": 58650
    },
    {
      "epoch": 0.8358251459490247,
      "grad_norm": 0.5424624681472778,
      "learning_rate": 0.0001443658293366257,
      "loss": 0.5885,
      "step": 58700
    },
    {
      "epoch": 0.8365370924106508,
      "grad_norm": 0.45961639285087585,
      "learning_rate": 0.00014431834370103047,
      "loss": 0.6964,
      "step": 58750
    },
    {
      "epoch": 0.8372490388722768,
      "grad_norm": 0.3866863548755646,
      "learning_rate": 0.00014427085806543522,
      "loss": 0.6781,
      "step": 58800
    },
    {
      "epoch": 0.8379609853339028,
      "grad_norm": 0.5524325370788574,
      "learning_rate": 0.00014422337242984,
      "loss": 0.6729,
      "step": 58850
    },
    {
      "epoch": 0.8386729317955289,
      "grad_norm": 0.5108965635299683,
      "learning_rate": 0.00014417588679424475,
      "loss": 0.6608,
      "step": 58900
    },
    {
      "epoch": 0.839384878257155,
      "grad_norm": 0.7942814826965332,
      "learning_rate": 0.0001441293508713614,
      "loss": 0.6856,
      "step": 58950
    },
    {
      "epoch": 0.8400968247187811,
      "grad_norm": 0.48587873578071594,
      "learning_rate": 0.00014408186523576618,
      "loss": 0.6878,
      "step": 59000
    },
    {
      "epoch": 0.8408087711804072,
      "grad_norm": 0.7043174505233765,
      "learning_rate": 0.00014403437960017095,
      "loss": 0.6962,
      "step": 59050
    },
    {
      "epoch": 0.8415207176420333,
      "grad_norm": 0.5882165431976318,
      "learning_rate": 0.00014398689396457573,
      "loss": 0.5913,
      "step": 59100
    },
    {
      "epoch": 0.8422326641036594,
      "grad_norm": 0.657768189907074,
      "learning_rate": 0.00014393940832898048,
      "loss": 0.6464,
      "step": 59150
    },
    {
      "epoch": 0.8429446105652855,
      "grad_norm": 0.625328540802002,
      "learning_rate": 0.00014389192269338526,
      "loss": 0.671,
      "step": 59200
    },
    {
      "epoch": 0.8436565570269116,
      "grad_norm": 0.40378937125205994,
      "learning_rate": 0.00014384443705779004,
      "loss": 0.6159,
      "step": 59250
    },
    {
      "epoch": 0.8443685034885376,
      "grad_norm": 0.7868186831474304,
      "learning_rate": 0.00014379695142219482,
      "loss": 0.7058,
      "step": 59300
    },
    {
      "epoch": 0.8450804499501637,
      "grad_norm": 0.6351896524429321,
      "learning_rate": 0.00014374946578659957,
      "loss": 0.6678,
      "step": 59350
    },
    {
      "epoch": 0.8457923964117898,
      "grad_norm": 0.6487915515899658,
      "learning_rate": 0.00014370198015100434,
      "loss": 0.6696,
      "step": 59400
    },
    {
      "epoch": 0.8465043428734159,
      "grad_norm": 0.7280182838439941,
      "learning_rate": 0.0001436544945154091,
      "loss": 0.6604,
      "step": 59450
    },
    {
      "epoch": 0.847216289335042,
      "grad_norm": 0.4318082928657532,
      "learning_rate": 0.00014360700887981387,
      "loss": 0.6487,
      "step": 59500
    },
    {
      "epoch": 0.8479282357966681,
      "grad_norm": 0.5967143774032593,
      "learning_rate": 0.00014355952324421865,
      "loss": 0.6591,
      "step": 59550
    },
    {
      "epoch": 0.8486401822582942,
      "grad_norm": 0.34749677777290344,
      "learning_rate": 0.0001435120376086234,
      "loss": 0.6262,
      "step": 59600
    },
    {
      "epoch": 0.8493521287199203,
      "grad_norm": 0.47855713963508606,
      "learning_rate": 0.00014346455197302818,
      "loss": 0.6692,
      "step": 59650
    },
    {
      "epoch": 0.8500640751815464,
      "grad_norm": 0.6480231881141663,
      "learning_rate": 0.00014341706633743293,
      "loss": 0.6302,
      "step": 59700
    },
    {
      "epoch": 0.8507760216431725,
      "grad_norm": 0.39930224418640137,
      "learning_rate": 0.0001433695807018377,
      "loss": 0.6057,
      "step": 59750
    },
    {
      "epoch": 0.8514879681047985,
      "grad_norm": 1.0746427774429321,
      "learning_rate": 0.00014332209506624248,
      "loss": 0.6773,
      "step": 59800
    },
    {
      "epoch": 0.8521999145664246,
      "grad_norm": 0.5190696716308594,
      "learning_rate": 0.00014327460943064723,
      "loss": 0.6435,
      "step": 59850
    },
    {
      "epoch": 0.8529118610280507,
      "grad_norm": 0.32774657011032104,
      "learning_rate": 0.000143227123795052,
      "loss": 0.6076,
      "step": 59900
    },
    {
      "epoch": 0.8536238074896768,
      "grad_norm": 0.306980699300766,
      "learning_rate": 0.00014317963815945676,
      "loss": 0.672,
      "step": 59950
    },
    {
      "epoch": 0.8543357539513029,
      "grad_norm": 0.4313177764415741,
      "learning_rate": 0.00014313215252386154,
      "loss": 0.6422,
      "step": 60000
    },
    {
      "epoch": 0.855047700412929,
      "grad_norm": 0.43492430448532104,
      "learning_rate": 0.00014308466688826632,
      "loss": 0.6838,
      "step": 60050
    },
    {
      "epoch": 0.8557596468745551,
      "grad_norm": 0.6776618957519531,
      "learning_rate": 0.00014303718125267107,
      "loss": 0.6458,
      "step": 60100
    },
    {
      "epoch": 0.8564715933361812,
      "grad_norm": 0.5504241585731506,
      "learning_rate": 0.00014298969561707585,
      "loss": 0.717,
      "step": 60150
    },
    {
      "epoch": 0.8571835397978073,
      "grad_norm": 0.5254831910133362,
      "learning_rate": 0.0001429422099814806,
      "loss": 0.6555,
      "step": 60200
    },
    {
      "epoch": 0.8578954862594332,
      "grad_norm": 0.30760619044303894,
      "learning_rate": 0.00014289472434588537,
      "loss": 0.6521,
      "step": 60250
    },
    {
      "epoch": 0.8586074327210593,
      "grad_norm": 0.45667922496795654,
      "learning_rate": 0.00014284723871029015,
      "loss": 0.6716,
      "step": 60300
    },
    {
      "epoch": 0.8593193791826854,
      "grad_norm": 0.6172874569892883,
      "learning_rate": 0.0001427997530746949,
      "loss": 0.679,
      "step": 60350
    },
    {
      "epoch": 0.8600313256443115,
      "grad_norm": 0.5341033339500427,
      "learning_rate": 0.00014275226743909968,
      "loss": 0.6699,
      "step": 60400
    },
    {
      "epoch": 0.8607432721059376,
      "grad_norm": 0.9018417000770569,
      "learning_rate": 0.00014270478180350443,
      "loss": 0.6422,
      "step": 60450
    },
    {
      "epoch": 0.8614552185675637,
      "grad_norm": 0.7478324770927429,
      "learning_rate": 0.0001426572961679092,
      "loss": 0.6511,
      "step": 60500
    },
    {
      "epoch": 0.8621671650291898,
      "grad_norm": 0.2976413071155548,
      "learning_rate": 0.00014260981053231399,
      "loss": 0.6127,
      "step": 60550
    },
    {
      "epoch": 0.8628791114908159,
      "grad_norm": 0.294125497341156,
      "learning_rate": 0.00014256232489671874,
      "loss": 0.6448,
      "step": 60600
    },
    {
      "epoch": 0.863591057952442,
      "grad_norm": 0.579761266708374,
      "learning_rate": 0.0001425148392611235,
      "loss": 0.5999,
      "step": 60650
    },
    {
      "epoch": 0.864303004414068,
      "grad_norm": 0.292634516954422,
      "learning_rate": 0.00014246735362552826,
      "loss": 0.6581,
      "step": 60700
    },
    {
      "epoch": 0.8650149508756941,
      "grad_norm": 0.39534005522727966,
      "learning_rate": 0.00014241986798993304,
      "loss": 0.6872,
      "step": 60750
    },
    {
      "epoch": 0.8657268973373202,
      "grad_norm": 0.4153805375099182,
      "learning_rate": 0.00014237238235433782,
      "loss": 0.6612,
      "step": 60800
    },
    {
      "epoch": 0.8664388437989463,
      "grad_norm": 0.7193692326545715,
      "learning_rate": 0.0001423248967187426,
      "loss": 0.7052,
      "step": 60850
    },
    {
      "epoch": 0.8671507902605724,
      "grad_norm": 0.6312792301177979,
      "learning_rate": 0.00014227741108314737,
      "loss": 0.717,
      "step": 60900
    },
    {
      "epoch": 0.8678627367221985,
      "grad_norm": 0.5086228847503662,
      "learning_rate": 0.00014222992544755212,
      "loss": 0.6613,
      "step": 60950
    },
    {
      "epoch": 0.8685746831838246,
      "grad_norm": 0.4467088282108307,
      "learning_rate": 0.0001421824398119569,
      "loss": 0.6738,
      "step": 61000
    },
    {
      "epoch": 0.8692866296454507,
      "grad_norm": 0.4312174320220947,
      "learning_rate": 0.00014213495417636168,
      "loss": 0.7115,
      "step": 61050
    },
    {
      "epoch": 0.8699985761070768,
      "grad_norm": 0.4647456705570221,
      "learning_rate": 0.00014208746854076643,
      "loss": 0.6091,
      "step": 61100
    },
    {
      "epoch": 0.8707105225687028,
      "grad_norm": 0.35976406931877136,
      "learning_rate": 0.0001420399829051712,
      "loss": 0.6374,
      "step": 61150
    },
    {
      "epoch": 0.8714224690303289,
      "grad_norm": 0.5497675538063049,
      "learning_rate": 0.00014199249726957596,
      "loss": 0.6533,
      "step": 61200
    },
    {
      "epoch": 0.872134415491955,
      "grad_norm": 0.7985344529151917,
      "learning_rate": 0.00014194501163398074,
      "loss": 0.6835,
      "step": 61250
    },
    {
      "epoch": 0.8728463619535811,
      "grad_norm": 0.6652771234512329,
      "learning_rate": 0.00014189752599838551,
      "loss": 0.6134,
      "step": 61300
    },
    {
      "epoch": 0.8735583084152072,
      "grad_norm": 0.702634871006012,
      "learning_rate": 0.00014185004036279026,
      "loss": 0.6722,
      "step": 61350
    },
    {
      "epoch": 0.8742702548768333,
      "grad_norm": 0.36353275179862976,
      "learning_rate": 0.00014180255472719504,
      "loss": 0.566,
      "step": 61400
    },
    {
      "epoch": 0.8749822013384594,
      "grad_norm": 0.5372328758239746,
      "learning_rate": 0.0001417550690915998,
      "loss": 0.6268,
      "step": 61450
    },
    {
      "epoch": 0.8756941478000855,
      "grad_norm": 0.3750351667404175,
      "learning_rate": 0.00014170758345600457,
      "loss": 0.6678,
      "step": 61500
    },
    {
      "epoch": 0.8764060942617116,
      "grad_norm": 0.31647151708602905,
      "learning_rate": 0.00014166009782040935,
      "loss": 0.6741,
      "step": 61550
    },
    {
      "epoch": 0.8771180407233377,
      "grad_norm": 0.6390255093574524,
      "learning_rate": 0.0001416126121848141,
      "loss": 0.7335,
      "step": 61600
    },
    {
      "epoch": 0.8778299871849637,
      "grad_norm": 0.6143791079521179,
      "learning_rate": 0.00014156512654921888,
      "loss": 0.6318,
      "step": 61650
    },
    {
      "epoch": 0.8785419336465897,
      "grad_norm": 0.5362884998321533,
      "learning_rate": 0.00014151764091362363,
      "loss": 0.6682,
      "step": 61700
    },
    {
      "epoch": 0.8792538801082158,
      "grad_norm": 0.5464571118354797,
      "learning_rate": 0.0001414701552780284,
      "loss": 0.697,
      "step": 61750
    },
    {
      "epoch": 0.8799658265698419,
      "grad_norm": 0.5532748699188232,
      "learning_rate": 0.00014142266964243318,
      "loss": 0.6759,
      "step": 61800
    },
    {
      "epoch": 0.880677773031468,
      "grad_norm": 0.3863440454006195,
      "learning_rate": 0.00014137518400683793,
      "loss": 0.6803,
      "step": 61850
    },
    {
      "epoch": 0.8813897194930941,
      "grad_norm": 0.6642690896987915,
      "learning_rate": 0.0001413276983712427,
      "loss": 0.6683,
      "step": 61900
    },
    {
      "epoch": 0.8821016659547202,
      "grad_norm": 0.37190571427345276,
      "learning_rate": 0.00014128021273564746,
      "loss": 0.7026,
      "step": 61950
    },
    {
      "epoch": 0.8828136124163463,
      "grad_norm": 0.4809802770614624,
      "learning_rate": 0.00014123272710005224,
      "loss": 0.6565,
      "step": 62000
    },
    {
      "epoch": 0.8835255588779724,
      "grad_norm": 0.5514233708381653,
      "learning_rate": 0.00014118524146445702,
      "loss": 0.6417,
      "step": 62050
    },
    {
      "epoch": 0.8842375053395984,
      "grad_norm": 0.38439249992370605,
      "learning_rate": 0.00014113775582886177,
      "loss": 0.6404,
      "step": 62100
    },
    {
      "epoch": 0.8849494518012245,
      "grad_norm": 0.40880128741264343,
      "learning_rate": 0.00014109027019326654,
      "loss": 0.6596,
      "step": 62150
    },
    {
      "epoch": 0.8856613982628506,
      "grad_norm": 0.9380702972412109,
      "learning_rate": 0.0001410427845576713,
      "loss": 0.6452,
      "step": 62200
    },
    {
      "epoch": 0.8863733447244767,
      "grad_norm": 0.6874595284461975,
      "learning_rate": 0.00014099529892207607,
      "loss": 0.646,
      "step": 62250
    },
    {
      "epoch": 0.8870852911861028,
      "grad_norm": 0.5261988639831543,
      "learning_rate": 0.00014094781328648085,
      "loss": 0.6419,
      "step": 62300
    },
    {
      "epoch": 0.8877972376477289,
      "grad_norm": 0.7200352549552917,
      "learning_rate": 0.0001409003276508856,
      "loss": 0.672,
      "step": 62350
    },
    {
      "epoch": 0.888509184109355,
      "grad_norm": 0.528594970703125,
      "learning_rate": 0.00014085284201529038,
      "loss": 0.712,
      "step": 62400
    },
    {
      "epoch": 0.8892211305709811,
      "grad_norm": 0.4597925841808319,
      "learning_rate": 0.00014080535637969515,
      "loss": 0.656,
      "step": 62450
    },
    {
      "epoch": 0.8899330770326072,
      "grad_norm": 0.6485849022865295,
      "learning_rate": 0.0001407578707440999,
      "loss": 0.6993,
      "step": 62500
    },
    {
      "epoch": 0.8906450234942332,
      "grad_norm": 0.5628343224525452,
      "learning_rate": 0.00014071038510850468,
      "loss": 0.6476,
      "step": 62550
    },
    {
      "epoch": 0.8913569699558593,
      "grad_norm": 0.6905158758163452,
      "learning_rate": 0.00014066289947290946,
      "loss": 0.6348,
      "step": 62600
    },
    {
      "epoch": 0.8920689164174854,
      "grad_norm": 0.5288245677947998,
      "learning_rate": 0.00014061541383731424,
      "loss": 0.6605,
      "step": 62650
    },
    {
      "epoch": 0.8927808628791115,
      "grad_norm": 0.45463743805885315,
      "learning_rate": 0.000140567928201719,
      "loss": 0.6847,
      "step": 62700
    },
    {
      "epoch": 0.8934928093407376,
      "grad_norm": 0.45687335729599,
      "learning_rate": 0.00014052044256612377,
      "loss": 0.6232,
      "step": 62750
    },
    {
      "epoch": 0.8942047558023637,
      "grad_norm": 0.6992866396903992,
      "learning_rate": 0.00014047295693052854,
      "loss": 0.707,
      "step": 62800
    },
    {
      "epoch": 0.8949167022639898,
      "grad_norm": 0.4434107542037964,
      "learning_rate": 0.0001404254712949333,
      "loss": 0.6576,
      "step": 62850
    },
    {
      "epoch": 0.8956286487256159,
      "grad_norm": 0.5144449472427368,
      "learning_rate": 0.00014037798565933807,
      "loss": 0.7064,
      "step": 62900
    },
    {
      "epoch": 0.896340595187242,
      "grad_norm": 0.5899283289909363,
      "learning_rate": 0.00014033050002374282,
      "loss": 0.6637,
      "step": 62950
    },
    {
      "epoch": 0.897052541648868,
      "grad_norm": 0.37318047881126404,
      "learning_rate": 0.0001402830143881476,
      "loss": 0.6398,
      "step": 63000
    },
    {
      "epoch": 0.8977644881104941,
      "grad_norm": 0.9354310631752014,
      "learning_rate": 0.00014023552875255238,
      "loss": 0.6448,
      "step": 63050
    },
    {
      "epoch": 0.8984764345721202,
      "grad_norm": 0.4462570250034332,
      "learning_rate": 0.00014018804311695713,
      "loss": 0.6565,
      "step": 63100
    },
    {
      "epoch": 0.8991883810337462,
      "grad_norm": 0.8582934141159058,
      "learning_rate": 0.0001401405574813619,
      "loss": 0.6184,
      "step": 63150
    },
    {
      "epoch": 0.8999003274953723,
      "grad_norm": 0.672037661075592,
      "learning_rate": 0.00014009307184576666,
      "loss": 0.7101,
      "step": 63200
    },
    {
      "epoch": 0.9006122739569984,
      "grad_norm": 0.5407945513725281,
      "learning_rate": 0.00014004558621017143,
      "loss": 0.6697,
      "step": 63250
    },
    {
      "epoch": 0.9013242204186245,
      "grad_norm": 0.538438081741333,
      "learning_rate": 0.0001399981005745762,
      "loss": 0.7189,
      "step": 63300
    },
    {
      "epoch": 0.9020361668802506,
      "grad_norm": 0.6639280915260315,
      "learning_rate": 0.00013995061493898096,
      "loss": 0.6686,
      "step": 63350
    },
    {
      "epoch": 0.9027481133418767,
      "grad_norm": 0.4311470091342926,
      "learning_rate": 0.00013990312930338574,
      "loss": 0.6887,
      "step": 63400
    },
    {
      "epoch": 0.9034600598035027,
      "grad_norm": 0.4978852868080139,
      "learning_rate": 0.0001398556436677905,
      "loss": 0.665,
      "step": 63450
    },
    {
      "epoch": 0.9041720062651288,
      "grad_norm": 0.5133583545684814,
      "learning_rate": 0.00013980815803219527,
      "loss": 0.6706,
      "step": 63500
    },
    {
      "epoch": 0.9048839527267549,
      "grad_norm": 0.313888818025589,
      "learning_rate": 0.00013976067239660005,
      "loss": 0.636,
      "step": 63550
    },
    {
      "epoch": 0.905595899188381,
      "grad_norm": 0.5572362542152405,
      "learning_rate": 0.0001397131867610048,
      "loss": 0.6506,
      "step": 63600
    },
    {
      "epoch": 0.9063078456500071,
      "grad_norm": 0.630510151386261,
      "learning_rate": 0.00013966665083812148,
      "loss": 0.6266,
      "step": 63650
    },
    {
      "epoch": 0.9070197921116332,
      "grad_norm": 0.5532183051109314,
      "learning_rate": 0.00013961916520252625,
      "loss": 0.6055,
      "step": 63700
    },
    {
      "epoch": 0.9077317385732593,
      "grad_norm": 0.4306483864784241,
      "learning_rate": 0.000139571679566931,
      "loss": 0.6447,
      "step": 63750
    },
    {
      "epoch": 0.9084436850348854,
      "grad_norm": 0.8478338122367859,
      "learning_rate": 0.00013952419393133578,
      "loss": 0.6277,
      "step": 63800
    },
    {
      "epoch": 0.9091556314965115,
      "grad_norm": 0.4602032005786896,
      "learning_rate": 0.00013947670829574056,
      "loss": 0.6567,
      "step": 63850
    },
    {
      "epoch": 0.9098675779581376,
      "grad_norm": 0.6052549481391907,
      "learning_rate": 0.00013943017237285724,
      "loss": 0.6812,
      "step": 63900
    },
    {
      "epoch": 0.9105795244197636,
      "grad_norm": 0.48408856987953186,
      "learning_rate": 0.000139382686737262,
      "loss": 0.7074,
      "step": 63950
    },
    {
      "epoch": 0.9112914708813897,
      "grad_norm": 0.5404201149940491,
      "learning_rate": 0.00013933520110166677,
      "loss": 0.6641,
      "step": 64000
    },
    {
      "epoch": 0.9120034173430158,
      "grad_norm": 0.4485943913459778,
      "learning_rate": 0.00013928771546607152,
      "loss": 0.6225,
      "step": 64050
    },
    {
      "epoch": 0.9127153638046419,
      "grad_norm": 0.6354125142097473,
      "learning_rate": 0.0001392402298304763,
      "loss": 0.6997,
      "step": 64100
    },
    {
      "epoch": 0.913427310266268,
      "grad_norm": 0.5101593136787415,
      "learning_rate": 0.00013919274419488107,
      "loss": 0.6268,
      "step": 64150
    },
    {
      "epoch": 0.9141392567278941,
      "grad_norm": 0.4233754277229309,
      "learning_rate": 0.00013914525855928582,
      "loss": 0.6314,
      "step": 64200
    },
    {
      "epoch": 0.9148512031895202,
      "grad_norm": 0.46294450759887695,
      "learning_rate": 0.0001390977729236906,
      "loss": 0.6846,
      "step": 64250
    },
    {
      "epoch": 0.9155631496511463,
      "grad_norm": 0.560112714767456,
      "learning_rate": 0.00013905028728809535,
      "loss": 0.6749,
      "step": 64300
    },
    {
      "epoch": 0.9162750961127724,
      "grad_norm": 0.5118286609649658,
      "learning_rate": 0.00013900280165250013,
      "loss": 0.6632,
      "step": 64350
    },
    {
      "epoch": 0.9169870425743984,
      "grad_norm": 0.3423866629600525,
      "learning_rate": 0.0001389553160169049,
      "loss": 0.6315,
      "step": 64400
    },
    {
      "epoch": 0.9176989890360245,
      "grad_norm": 0.36746230721473694,
      "learning_rate": 0.00013890783038130966,
      "loss": 0.7064,
      "step": 64450
    },
    {
      "epoch": 0.9184109354976506,
      "grad_norm": 0.3451414704322815,
      "learning_rate": 0.00013886034474571443,
      "loss": 0.6367,
      "step": 64500
    },
    {
      "epoch": 0.9191228819592767,
      "grad_norm": 0.5849420428276062,
      "learning_rate": 0.00013881285911011918,
      "loss": 0.6259,
      "step": 64550
    },
    {
      "epoch": 0.9198348284209027,
      "grad_norm": 0.4353715181350708,
      "learning_rate": 0.00013876537347452396,
      "loss": 0.6389,
      "step": 64600
    },
    {
      "epoch": 0.9205467748825288,
      "grad_norm": 0.6495471000671387,
      "learning_rate": 0.00013871788783892874,
      "loss": 0.6965,
      "step": 64650
    },
    {
      "epoch": 0.9212587213441549,
      "grad_norm": 0.6344324350357056,
      "learning_rate": 0.0001386704022033335,
      "loss": 0.6433,
      "step": 64700
    },
    {
      "epoch": 0.921970667805781,
      "grad_norm": 0.44984373450279236,
      "learning_rate": 0.00013862291656773827,
      "loss": 0.6904,
      "step": 64750
    },
    {
      "epoch": 0.922682614267407,
      "grad_norm": 1.1461811065673828,
      "learning_rate": 0.00013857543093214302,
      "loss": 0.7046,
      "step": 64800
    },
    {
      "epoch": 0.9233945607290331,
      "grad_norm": 0.6440412998199463,
      "learning_rate": 0.0001385279452965478,
      "loss": 0.6547,
      "step": 64850
    },
    {
      "epoch": 0.9241065071906592,
      "grad_norm": 0.28772637248039246,
      "learning_rate": 0.00013848045966095257,
      "loss": 0.6217,
      "step": 64900
    },
    {
      "epoch": 0.9248184536522853,
      "grad_norm": 0.5201690196990967,
      "learning_rate": 0.00013843297402535732,
      "loss": 0.6813,
      "step": 64950
    },
    {
      "epoch": 0.9255304001139114,
      "grad_norm": 0.48644566535949707,
      "learning_rate": 0.0001383854883897621,
      "loss": 0.6263,
      "step": 65000
    },
    {
      "epoch": 0.9262423465755375,
      "grad_norm": 0.6035979986190796,
      "learning_rate": 0.00013833800275416685,
      "loss": 0.6817,
      "step": 65050
    },
    {
      "epoch": 0.9269542930371636,
      "grad_norm": 0.5787747502326965,
      "learning_rate": 0.00013829051711857163,
      "loss": 0.6408,
      "step": 65100
    },
    {
      "epoch": 0.9276662394987897,
      "grad_norm": 0.4368496537208557,
      "learning_rate": 0.0001382430314829764,
      "loss": 0.6685,
      "step": 65150
    },
    {
      "epoch": 0.9283781859604158,
      "grad_norm": 0.3798821270465851,
      "learning_rate": 0.00013819554584738116,
      "loss": 0.6652,
      "step": 65200
    },
    {
      "epoch": 0.9290901324220419,
      "grad_norm": 0.4359630346298218,
      "learning_rate": 0.00013814806021178593,
      "loss": 0.6452,
      "step": 65250
    },
    {
      "epoch": 0.929802078883668,
      "grad_norm": 0.7300412058830261,
      "learning_rate": 0.0001381005745761907,
      "loss": 0.6599,
      "step": 65300
    },
    {
      "epoch": 0.930514025345294,
      "grad_norm": 0.6193451285362244,
      "learning_rate": 0.00013805308894059546,
      "loss": 0.6568,
      "step": 65350
    },
    {
      "epoch": 0.9312259718069201,
      "grad_norm": 0.592250645160675,
      "learning_rate": 0.00013800560330500024,
      "loss": 0.658,
      "step": 65400
    },
    {
      "epoch": 0.9319379182685462,
      "grad_norm": 0.44028082489967346,
      "learning_rate": 0.00013795811766940502,
      "loss": 0.6921,
      "step": 65450
    },
    {
      "epoch": 0.9326498647301723,
      "grad_norm": 0.5099149942398071,
      "learning_rate": 0.0001379106320338098,
      "loss": 0.6152,
      "step": 65500
    },
    {
      "epoch": 0.9333618111917984,
      "grad_norm": 0.463512659072876,
      "learning_rate": 0.00013786314639821455,
      "loss": 0.6409,
      "step": 65550
    },
    {
      "epoch": 0.9340737576534245,
      "grad_norm": 0.39183685183525085,
      "learning_rate": 0.00013781566076261932,
      "loss": 0.7022,
      "step": 65600
    },
    {
      "epoch": 0.9347857041150506,
      "grad_norm": 0.6429950594902039,
      "learning_rate": 0.0001377681751270241,
      "loss": 0.6274,
      "step": 65650
    },
    {
      "epoch": 0.9354976505766767,
      "grad_norm": 0.5520524382591248,
      "learning_rate": 0.00013772068949142885,
      "loss": 0.6344,
      "step": 65700
    },
    {
      "epoch": 0.9362095970383028,
      "grad_norm": 0.40672585368156433,
      "learning_rate": 0.00013767320385583363,
      "loss": 0.6852,
      "step": 65750
    },
    {
      "epoch": 0.9369215434999288,
      "grad_norm": 0.5589221715927124,
      "learning_rate": 0.00013762571822023838,
      "loss": 0.6352,
      "step": 65800
    },
    {
      "epoch": 0.9376334899615549,
      "grad_norm": 0.446467787027359,
      "learning_rate": 0.00013757823258464316,
      "loss": 0.6599,
      "step": 65850
    },
    {
      "epoch": 0.938345436423181,
      "grad_norm": 0.48954060673713684,
      "learning_rate": 0.00013753074694904794,
      "loss": 0.6303,
      "step": 65900
    },
    {
      "epoch": 0.9390573828848071,
      "grad_norm": 0.5446077585220337,
      "learning_rate": 0.0001374842110261646,
      "loss": 0.6649,
      "step": 65950
    },
    {
      "epoch": 0.9397693293464332,
      "grad_norm": 0.8809083104133606,
      "learning_rate": 0.00013743672539056937,
      "loss": 0.6884,
      "step": 66000
    },
    {
      "epoch": 0.9404812758080592,
      "grad_norm": 0.3236980438232422,
      "learning_rate": 0.00013738923975497414,
      "loss": 0.7357,
      "step": 66050
    },
    {
      "epoch": 0.9411932222696853,
      "grad_norm": 0.32733407616615295,
      "learning_rate": 0.0001373417541193789,
      "loss": 0.6515,
      "step": 66100
    },
    {
      "epoch": 0.9419051687313114,
      "grad_norm": 0.5014273524284363,
      "learning_rate": 0.00013729426848378367,
      "loss": 0.6434,
      "step": 66150
    },
    {
      "epoch": 0.9426171151929374,
      "grad_norm": 0.4039563536643982,
      "learning_rate": 0.00013724678284818845,
      "loss": 0.6876,
      "step": 66200
    },
    {
      "epoch": 0.9433290616545635,
      "grad_norm": 0.6448734402656555,
      "learning_rate": 0.0001371992972125932,
      "loss": 0.7031,
      "step": 66250
    },
    {
      "epoch": 0.9440410081161896,
      "grad_norm": 0.7278155088424683,
      "learning_rate": 0.00013715181157699798,
      "loss": 0.6657,
      "step": 66300
    },
    {
      "epoch": 0.9447529545778157,
      "grad_norm": 0.3132573664188385,
      "learning_rate": 0.00013710432594140273,
      "loss": 0.6791,
      "step": 66350
    },
    {
      "epoch": 0.9454649010394418,
      "grad_norm": 0.4961088299751282,
      "learning_rate": 0.0001370568403058075,
      "loss": 0.6372,
      "step": 66400
    },
    {
      "epoch": 0.9461768475010679,
      "grad_norm": 0.5208338499069214,
      "learning_rate": 0.00013700935467021228,
      "loss": 0.6559,
      "step": 66450
    },
    {
      "epoch": 0.946888793962694,
      "grad_norm": 0.46119827032089233,
      "learning_rate": 0.00013696186903461703,
      "loss": 0.6365,
      "step": 66500
    },
    {
      "epoch": 0.9476007404243201,
      "grad_norm": 0.6112073063850403,
      "learning_rate": 0.0001369143833990218,
      "loss": 0.6348,
      "step": 66550
    },
    {
      "epoch": 0.9483126868859462,
      "grad_norm": 0.5867947340011597,
      "learning_rate": 0.00013686689776342656,
      "loss": 0.6651,
      "step": 66600
    },
    {
      "epoch": 0.9490246333475723,
      "grad_norm": 0.6140403151512146,
      "learning_rate": 0.00013681941212783134,
      "loss": 0.7028,
      "step": 66650
    },
    {
      "epoch": 0.9497365798091983,
      "grad_norm": 0.28097793459892273,
      "learning_rate": 0.00013677192649223612,
      "loss": 0.6278,
      "step": 66700
    },
    {
      "epoch": 0.9504485262708244,
      "grad_norm": 0.6147369146347046,
      "learning_rate": 0.00013672444085664087,
      "loss": 0.6591,
      "step": 66750
    },
    {
      "epoch": 0.9511604727324505,
      "grad_norm": 0.6794235110282898,
      "learning_rate": 0.00013667695522104564,
      "loss": 0.653,
      "step": 66800
    },
    {
      "epoch": 0.9518724191940766,
      "grad_norm": 0.5804795622825623,
      "learning_rate": 0.0001366294695854504,
      "loss": 0.6646,
      "step": 66850
    },
    {
      "epoch": 0.9525843656557027,
      "grad_norm": 0.33042246103286743,
      "learning_rate": 0.00013658198394985517,
      "loss": 0.6318,
      "step": 66900
    },
    {
      "epoch": 0.9532963121173288,
      "grad_norm": 0.4760649502277374,
      "learning_rate": 0.00013653449831425995,
      "loss": 0.6398,
      "step": 66950
    },
    {
      "epoch": 0.9540082585789549,
      "grad_norm": 0.6100760102272034,
      "learning_rate": 0.0001364870126786647,
      "loss": 0.5989,
      "step": 67000
    },
    {
      "epoch": 0.954720205040581,
      "grad_norm": 0.4603536128997803,
      "learning_rate": 0.00013643952704306948,
      "loss": 0.653,
      "step": 67050
    },
    {
      "epoch": 0.9554321515022071,
      "grad_norm": 0.7994479537010193,
      "learning_rate": 0.00013639204140747423,
      "loss": 0.7244,
      "step": 67100
    },
    {
      "epoch": 0.9561440979638332,
      "grad_norm": 0.5374940633773804,
      "learning_rate": 0.000136344555771879,
      "loss": 0.6262,
      "step": 67150
    },
    {
      "epoch": 0.9568560444254592,
      "grad_norm": 0.47080063819885254,
      "learning_rate": 0.00013629707013628378,
      "loss": 0.6221,
      "step": 67200
    },
    {
      "epoch": 0.9575679908870853,
      "grad_norm": 0.5062544345855713,
      "learning_rate": 0.00013624958450068853,
      "loss": 0.5979,
      "step": 67250
    },
    {
      "epoch": 0.9582799373487114,
      "grad_norm": 0.4507087767124176,
      "learning_rate": 0.0001362020988650933,
      "loss": 0.7026,
      "step": 67300
    },
    {
      "epoch": 0.9589918838103375,
      "grad_norm": 0.7663868069648743,
      "learning_rate": 0.00013615461322949806,
      "loss": 0.5967,
      "step": 67350
    },
    {
      "epoch": 0.9597038302719636,
      "grad_norm": 0.46987858414649963,
      "learning_rate": 0.00013610712759390284,
      "loss": 0.607,
      "step": 67400
    },
    {
      "epoch": 0.9604157767335897,
      "grad_norm": 0.5045331120491028,
      "learning_rate": 0.00013605964195830762,
      "loss": 0.6738,
      "step": 67450
    },
    {
      "epoch": 0.9611277231952157,
      "grad_norm": 0.6809507608413696,
      "learning_rate": 0.00013601215632271237,
      "loss": 0.6608,
      "step": 67500
    },
    {
      "epoch": 0.9618396696568418,
      "grad_norm": 0.36102572083473206,
      "learning_rate": 0.00013596467068711715,
      "loss": 0.5974,
      "step": 67550
    },
    {
      "epoch": 0.9625516161184678,
      "grad_norm": 0.419910728931427,
      "learning_rate": 0.00013591718505152192,
      "loss": 0.6794,
      "step": 67600
    },
    {
      "epoch": 0.9632635625800939,
      "grad_norm": 0.6251446008682251,
      "learning_rate": 0.0001358696994159267,
      "loss": 0.6669,
      "step": 67650
    },
    {
      "epoch": 0.96397550904172,
      "grad_norm": 0.37034422159194946,
      "learning_rate": 0.00013582221378033145,
      "loss": 0.627,
      "step": 67700
    },
    {
      "epoch": 0.9646874555033461,
      "grad_norm": 0.6097822785377502,
      "learning_rate": 0.00013577472814473623,
      "loss": 0.6968,
      "step": 67750
    },
    {
      "epoch": 0.9653994019649722,
      "grad_norm": 0.6094024181365967,
      "learning_rate": 0.000135727242509141,
      "loss": 0.6439,
      "step": 67800
    },
    {
      "epoch": 0.9661113484265983,
      "grad_norm": 0.566390872001648,
      "learning_rate": 0.00013567975687354576,
      "loss": 0.6989,
      "step": 67850
    },
    {
      "epoch": 0.9668232948882244,
      "grad_norm": 0.5292977690696716,
      "learning_rate": 0.00013563227123795053,
      "loss": 0.6535,
      "step": 67900
    },
    {
      "epoch": 0.9675352413498505,
      "grad_norm": 0.3552877902984619,
      "learning_rate": 0.0001355847856023553,
      "loss": 0.6516,
      "step": 67950
    },
    {
      "epoch": 0.9682471878114766,
      "grad_norm": 0.7763157486915588,
      "learning_rate": 0.00013553729996676006,
      "loss": 0.6726,
      "step": 68000
    },
    {
      "epoch": 0.9689591342731027,
      "grad_norm": 0.7381300330162048,
      "learning_rate": 0.00013548981433116484,
      "loss": 0.6741,
      "step": 68050
    },
    {
      "epoch": 0.9696710807347287,
      "grad_norm": 0.601723313331604,
      "learning_rate": 0.0001354423286955696,
      "loss": 0.6281,
      "step": 68100
    },
    {
      "epoch": 0.9703830271963548,
      "grad_norm": 0.5039021968841553,
      "learning_rate": 0.00013539484305997437,
      "loss": 0.666,
      "step": 68150
    },
    {
      "epoch": 0.9710949736579809,
      "grad_norm": 0.5368293523788452,
      "learning_rate": 0.00013534735742437915,
      "loss": 0.7288,
      "step": 68200
    },
    {
      "epoch": 0.971806920119607,
      "grad_norm": 0.6760801672935486,
      "learning_rate": 0.0001352998717887839,
      "loss": 0.6948,
      "step": 68250
    },
    {
      "epoch": 0.9725188665812331,
      "grad_norm": 0.516226053237915,
      "learning_rate": 0.00013525238615318867,
      "loss": 0.6237,
      "step": 68300
    },
    {
      "epoch": 0.9732308130428592,
      "grad_norm": 0.4791925847530365,
      "learning_rate": 0.00013520490051759342,
      "loss": 0.6053,
      "step": 68350
    },
    {
      "epoch": 0.9739427595044853,
      "grad_norm": 0.6236211061477661,
      "learning_rate": 0.0001351574148819982,
      "loss": 0.6622,
      "step": 68400
    },
    {
      "epoch": 0.9746547059661114,
      "grad_norm": 0.4910255968570709,
      "learning_rate": 0.00013510992924640298,
      "loss": 0.5959,
      "step": 68450
    },
    {
      "epoch": 0.9753666524277375,
      "grad_norm": 0.5608391165733337,
      "learning_rate": 0.00013506244361080773,
      "loss": 0.6288,
      "step": 68500
    },
    {
      "epoch": 0.9760785988893635,
      "grad_norm": 0.4838709533214569,
      "learning_rate": 0.0001350149579752125,
      "loss": 0.6616,
      "step": 68550
    },
    {
      "epoch": 0.9767905453509896,
      "grad_norm": 0.4455021321773529,
      "learning_rate": 0.00013496747233961726,
      "loss": 0.6807,
      "step": 68600
    },
    {
      "epoch": 0.9775024918126157,
      "grad_norm": 0.5211889743804932,
      "learning_rate": 0.00013491998670402204,
      "loss": 0.6176,
      "step": 68650
    },
    {
      "epoch": 0.9782144382742418,
      "grad_norm": 0.461954802274704,
      "learning_rate": 0.00013487250106842681,
      "loss": 0.6696,
      "step": 68700
    },
    {
      "epoch": 0.9789263847358679,
      "grad_norm": 0.6286314129829407,
      "learning_rate": 0.00013482501543283156,
      "loss": 0.6687,
      "step": 68750
    },
    {
      "epoch": 0.979638331197494,
      "grad_norm": 0.43173474073410034,
      "learning_rate": 0.00013477752979723634,
      "loss": 0.6591,
      "step": 68800
    },
    {
      "epoch": 0.9803502776591201,
      "grad_norm": 0.6330611109733582,
      "learning_rate": 0.0001347300441616411,
      "loss": 0.6718,
      "step": 68850
    },
    {
      "epoch": 0.9810622241207462,
      "grad_norm": 0.6480296850204468,
      "learning_rate": 0.00013468255852604587,
      "loss": 0.653,
      "step": 68900
    },
    {
      "epoch": 0.9817741705823722,
      "grad_norm": 0.6492584943771362,
      "learning_rate": 0.00013463507289045065,
      "loss": 0.6426,
      "step": 68950
    },
    {
      "epoch": 0.9824861170439982,
      "grad_norm": 0.5864617824554443,
      "learning_rate": 0.0001345875872548554,
      "loss": 0.6421,
      "step": 69000
    },
    {
      "epoch": 0.9831980635056243,
      "grad_norm": 0.3649630546569824,
      "learning_rate": 0.00013454010161926018,
      "loss": 0.7276,
      "step": 69050
    },
    {
      "epoch": 0.9839100099672504,
      "grad_norm": 0.3664572238922119,
      "learning_rate": 0.00013449261598366493,
      "loss": 0.6881,
      "step": 69100
    },
    {
      "epoch": 0.9846219564288765,
      "grad_norm": 0.5640323162078857,
      "learning_rate": 0.0001344451303480697,
      "loss": 0.628,
      "step": 69150
    },
    {
      "epoch": 0.9853339028905026,
      "grad_norm": 0.4334121644496918,
      "learning_rate": 0.00013439764471247448,
      "loss": 0.6408,
      "step": 69200
    },
    {
      "epoch": 0.9860458493521287,
      "grad_norm": 0.33450642228126526,
      "learning_rate": 0.00013435015907687923,
      "loss": 0.6873,
      "step": 69250
    },
    {
      "epoch": 0.9867577958137548,
      "grad_norm": 0.4995139539241791,
      "learning_rate": 0.000134302673441284,
      "loss": 0.6829,
      "step": 69300
    },
    {
      "epoch": 0.9874697422753809,
      "grad_norm": 0.6318371891975403,
      "learning_rate": 0.0001342551878056888,
      "loss": 0.6667,
      "step": 69350
    },
    {
      "epoch": 0.988181688737007,
      "grad_norm": 0.30119311809539795,
      "learning_rate": 0.00013420770217009357,
      "loss": 0.6496,
      "step": 69400
    },
    {
      "epoch": 0.988893635198633,
      "grad_norm": 0.41731321811676025,
      "learning_rate": 0.00013416021653449832,
      "loss": 0.6703,
      "step": 69450
    },
    {
      "epoch": 0.9896055816602591,
      "grad_norm": 0.7682379484176636,
      "learning_rate": 0.0001341127308989031,
      "loss": 0.6307,
      "step": 69500
    },
    {
      "epoch": 0.9903175281218852,
      "grad_norm": 0.5121591091156006,
      "learning_rate": 0.00013406524526330787,
      "loss": 0.6546,
      "step": 69550
    },
    {
      "epoch": 0.9910294745835113,
      "grad_norm": 0.8756881952285767,
      "learning_rate": 0.00013401775962771262,
      "loss": 0.6598,
      "step": 69600
    },
    {
      "epoch": 0.9917414210451374,
      "grad_norm": 0.7854837775230408,
      "learning_rate": 0.0001339702739921174,
      "loss": 0.6544,
      "step": 69650
    },
    {
      "epoch": 0.9924533675067635,
      "grad_norm": 0.46734797954559326,
      "learning_rate": 0.00013392278835652218,
      "loss": 0.691,
      "step": 69700
    },
    {
      "epoch": 0.9931653139683896,
      "grad_norm": 0.5473067164421082,
      "learning_rate": 0.00013387530272092693,
      "loss": 0.6798,
      "step": 69750
    },
    {
      "epoch": 0.9938772604300157,
      "grad_norm": 0.4318293631076813,
      "learning_rate": 0.0001338278170853317,
      "loss": 0.6002,
      "step": 69800
    },
    {
      "epoch": 0.9945892068916418,
      "grad_norm": 0.5176501274108887,
      "learning_rate": 0.00013378033144973646,
      "loss": 0.6874,
      "step": 69850
    },
    {
      "epoch": 0.9953011533532679,
      "grad_norm": 0.6921990513801575,
      "learning_rate": 0.00013373284581414123,
      "loss": 0.7102,
      "step": 69900
    },
    {
      "epoch": 0.9960130998148939,
      "grad_norm": 0.5490244030952454,
      "learning_rate": 0.000133685360178546,
      "loss": 0.6624,
      "step": 69950
    },
    {
      "epoch": 0.99672504627652,
      "grad_norm": 0.6166821122169495,
      "learning_rate": 0.00013363787454295076,
      "loss": 0.6872,
      "step": 70000
    },
    {
      "epoch": 0.9974369927381461,
      "grad_norm": 0.608874499797821,
      "learning_rate": 0.00013359038890735554,
      "loss": 0.7363,
      "step": 70050
    },
    {
      "epoch": 0.9981489391997722,
      "grad_norm": 0.4713915288448334,
      "learning_rate": 0.0001335429032717603,
      "loss": 0.6301,
      "step": 70100
    },
    {
      "epoch": 0.9988608856613983,
      "grad_norm": 0.6270211935043335,
      "learning_rate": 0.00013349541763616507,
      "loss": 0.6331,
      "step": 70150
    },
    {
      "epoch": 0.9995728321230244,
      "grad_norm": 0.48498085141181946,
      "learning_rate": 0.00013344793200056984,
      "loss": 0.6779,
      "step": 70200
    },
    {
      "epoch": 1.0002847785846505,
      "grad_norm": 0.8059627413749695,
      "learning_rate": 0.0001334004463649746,
      "loss": 0.6581,
      "step": 70250
    },
    {
      "epoch": 1.0009967250462766,
      "grad_norm": 0.47232481837272644,
      "learning_rate": 0.00013335296072937937,
      "loss": 0.6556,
      "step": 70300
    },
    {
      "epoch": 1.0017086715079027,
      "grad_norm": 0.5239051580429077,
      "learning_rate": 0.00013330547509378412,
      "loss": 0.6472,
      "step": 70350
    },
    {
      "epoch": 1.0024206179695287,
      "grad_norm": 0.6883394718170166,
      "learning_rate": 0.0001332579894581889,
      "loss": 0.6131,
      "step": 70400
    },
    {
      "epoch": 1.0031325644311548,
      "grad_norm": 0.7065223455429077,
      "learning_rate": 0.00013321050382259368,
      "loss": 0.6294,
      "step": 70450
    },
    {
      "epoch": 1.003844510892781,
      "grad_norm": 0.5031778216362,
      "learning_rate": 0.00013316301818699843,
      "loss": 0.6872,
      "step": 70500
    },
    {
      "epoch": 1.004556457354407,
      "grad_norm": 0.30378544330596924,
      "learning_rate": 0.0001331155325514032,
      "loss": 0.6468,
      "step": 70550
    },
    {
      "epoch": 1.005268403816033,
      "grad_norm": 0.6548518538475037,
      "learning_rate": 0.00013306804691580796,
      "loss": 0.6688,
      "step": 70600
    },
    {
      "epoch": 1.0059803502776592,
      "grad_norm": 0.6347266435623169,
      "learning_rate": 0.00013302056128021273,
      "loss": 0.6385,
      "step": 70650
    },
    {
      "epoch": 1.0066922967392853,
      "grad_norm": 0.5759202241897583,
      "learning_rate": 0.0001329730756446175,
      "loss": 0.618,
      "step": 70700
    },
    {
      "epoch": 1.0074042432009114,
      "grad_norm": 0.48899081349372864,
      "learning_rate": 0.00013292559000902226,
      "loss": 0.6537,
      "step": 70750
    },
    {
      "epoch": 1.0081161896625375,
      "grad_norm": 0.5855479836463928,
      "learning_rate": 0.00013287810437342704,
      "loss": 0.6116,
      "step": 70800
    },
    {
      "epoch": 1.0088281361241636,
      "grad_norm": 0.7235864996910095,
      "learning_rate": 0.0001328306187378318,
      "loss": 0.6847,
      "step": 70850
    },
    {
      "epoch": 1.0095400825857896,
      "grad_norm": 0.4499681293964386,
      "learning_rate": 0.00013278313310223657,
      "loss": 0.6856,
      "step": 70900
    },
    {
      "epoch": 1.0102520290474157,
      "grad_norm": 0.3155563771724701,
      "learning_rate": 0.00013273564746664135,
      "loss": 0.6586,
      "step": 70950
    },
    {
      "epoch": 1.0109639755090418,
      "grad_norm": 0.6270769834518433,
      "learning_rate": 0.0001326881618310461,
      "loss": 0.65,
      "step": 71000
    },
    {
      "epoch": 1.011675921970668,
      "grad_norm": 0.31900474429130554,
      "learning_rate": 0.00013264067619545087,
      "loss": 0.5866,
      "step": 71050
    },
    {
      "epoch": 1.012387868432294,
      "grad_norm": 0.6032268404960632,
      "learning_rate": 0.00013259319055985565,
      "loss": 0.6659,
      "step": 71100
    },
    {
      "epoch": 1.0130998148939199,
      "grad_norm": 0.5982205867767334,
      "learning_rate": 0.00013254570492426043,
      "loss": 0.6202,
      "step": 71150
    },
    {
      "epoch": 1.013811761355546,
      "grad_norm": 0.48245102167129517,
      "learning_rate": 0.00013249821928866518,
      "loss": 0.6008,
      "step": 71200
    },
    {
      "epoch": 1.014523707817172,
      "grad_norm": 0.5297989845275879,
      "learning_rate": 0.00013245073365306996,
      "loss": 0.6981,
      "step": 71250
    },
    {
      "epoch": 1.0152356542787981,
      "grad_norm": 0.5074607729911804,
      "learning_rate": 0.00013240324801747473,
      "loss": 0.6191,
      "step": 71300
    },
    {
      "epoch": 1.0159476007404242,
      "grad_norm": 0.258564293384552,
      "learning_rate": 0.00013235576238187949,
      "loss": 0.6349,
      "step": 71350
    },
    {
      "epoch": 1.0166595472020503,
      "grad_norm": 0.5413159132003784,
      "learning_rate": 0.00013230827674628426,
      "loss": 0.6776,
      "step": 71400
    },
    {
      "epoch": 1.0173714936636764,
      "grad_norm": 0.6595034003257751,
      "learning_rate": 0.00013226079111068904,
      "loss": 0.6778,
      "step": 71450
    },
    {
      "epoch": 1.0180834401253025,
      "grad_norm": 0.4774519205093384,
      "learning_rate": 0.0001322133054750938,
      "loss": 0.5783,
      "step": 71500
    },
    {
      "epoch": 1.0187953865869286,
      "grad_norm": 0.6620905995368958,
      "learning_rate": 0.00013216581983949857,
      "loss": 0.6289,
      "step": 71550
    },
    {
      "epoch": 1.0195073330485547,
      "grad_norm": 0.8639129996299744,
      "learning_rate": 0.00013211833420390332,
      "loss": 0.6336,
      "step": 71600
    },
    {
      "epoch": 1.0202192795101808,
      "grad_norm": 0.43631303310394287,
      "learning_rate": 0.0001320708485683081,
      "loss": 0.633,
      "step": 71650
    },
    {
      "epoch": 1.0209312259718069,
      "grad_norm": 0.6406171321868896,
      "learning_rate": 0.00013202336293271287,
      "loss": 0.6544,
      "step": 71700
    },
    {
      "epoch": 1.021643172433433,
      "grad_norm": 0.4495910108089447,
      "learning_rate": 0.00013197682700982955,
      "loss": 0.6351,
      "step": 71750
    },
    {
      "epoch": 1.022355118895059,
      "grad_norm": 0.5654139518737793,
      "learning_rate": 0.0001319293413742343,
      "loss": 0.6413,
      "step": 71800
    },
    {
      "epoch": 1.0230670653566851,
      "grad_norm": 0.6712684035301208,
      "learning_rate": 0.00013188185573863908,
      "loss": 0.644,
      "step": 71850
    },
    {
      "epoch": 1.0237790118183112,
      "grad_norm": 0.4405882656574249,
      "learning_rate": 0.00013183437010304383,
      "loss": 0.7075,
      "step": 71900
    },
    {
      "epoch": 1.0244909582799373,
      "grad_norm": 0.3936416506767273,
      "learning_rate": 0.0001317868844674486,
      "loss": 0.6289,
      "step": 71950
    },
    {
      "epoch": 1.0252029047415634,
      "grad_norm": 0.7312154173851013,
      "learning_rate": 0.0001317393988318534,
      "loss": 0.6291,
      "step": 72000
    },
    {
      "epoch": 1.0259148512031895,
      "grad_norm": 0.2511979639530182,
      "learning_rate": 0.00013169191319625814,
      "loss": 0.6007,
      "step": 72050
    },
    {
      "epoch": 1.0266267976648156,
      "grad_norm": 0.6272907257080078,
      "learning_rate": 0.00013164442756066292,
      "loss": 0.7049,
      "step": 72100
    },
    {
      "epoch": 1.0273387441264417,
      "grad_norm": 0.5543971657752991,
      "learning_rate": 0.00013159694192506767,
      "loss": 0.6285,
      "step": 72150
    },
    {
      "epoch": 1.0280506905880678,
      "grad_norm": 0.5215887427330017,
      "learning_rate": 0.00013154945628947244,
      "loss": 0.6448,
      "step": 72200
    },
    {
      "epoch": 1.0287626370496938,
      "grad_norm": 0.6258928179740906,
      "learning_rate": 0.00013150197065387722,
      "loss": 0.668,
      "step": 72250
    },
    {
      "epoch": 1.02947458351132,
      "grad_norm": 0.5622766017913818,
      "learning_rate": 0.00013145448501828197,
      "loss": 0.7096,
      "step": 72300
    },
    {
      "epoch": 1.030186529972946,
      "grad_norm": 0.722510814666748,
      "learning_rate": 0.00013140699938268675,
      "loss": 0.6653,
      "step": 72350
    },
    {
      "epoch": 1.030898476434572,
      "grad_norm": 0.6209942698478699,
      "learning_rate": 0.0001313595137470915,
      "loss": 0.6366,
      "step": 72400
    },
    {
      "epoch": 1.0316104228961982,
      "grad_norm": 0.3733213245868683,
      "learning_rate": 0.00013131202811149628,
      "loss": 0.6522,
      "step": 72450
    },
    {
      "epoch": 1.0323223693578243,
      "grad_norm": 0.693803071975708,
      "learning_rate": 0.00013126454247590106,
      "loss": 0.6546,
      "step": 72500
    },
    {
      "epoch": 1.0330343158194504,
      "grad_norm": 0.5138214230537415,
      "learning_rate": 0.0001312170568403058,
      "loss": 0.6668,
      "step": 72550
    },
    {
      "epoch": 1.0337462622810765,
      "grad_norm": 0.4235328435897827,
      "learning_rate": 0.00013116957120471058,
      "loss": 0.6549,
      "step": 72600
    },
    {
      "epoch": 1.0344582087427026,
      "grad_norm": 0.4849717319011688,
      "learning_rate": 0.00013112208556911533,
      "loss": 0.6158,
      "step": 72650
    },
    {
      "epoch": 1.0351701552043286,
      "grad_norm": 0.49561208486557007,
      "learning_rate": 0.0001310745999335201,
      "loss": 0.5806,
      "step": 72700
    },
    {
      "epoch": 1.0358821016659547,
      "grad_norm": 0.5772397518157959,
      "learning_rate": 0.0001310271142979249,
      "loss": 0.6459,
      "step": 72750
    },
    {
      "epoch": 1.0365940481275808,
      "grad_norm": 0.38371729850769043,
      "learning_rate": 0.00013097962866232964,
      "loss": 0.6423,
      "step": 72800
    },
    {
      "epoch": 1.037305994589207,
      "grad_norm": 0.5887954831123352,
      "learning_rate": 0.00013093214302673442,
      "loss": 0.6945,
      "step": 72850
    },
    {
      "epoch": 1.038017941050833,
      "grad_norm": 0.987183153629303,
      "learning_rate": 0.00013088465739113917,
      "loss": 0.6263,
      "step": 72900
    },
    {
      "epoch": 1.038729887512459,
      "grad_norm": 0.42917415499687195,
      "learning_rate": 0.00013083717175554395,
      "loss": 0.6236,
      "step": 72950
    },
    {
      "epoch": 1.0394418339740852,
      "grad_norm": 0.3771360516548157,
      "learning_rate": 0.00013078968611994872,
      "loss": 0.6246,
      "step": 73000
    },
    {
      "epoch": 1.0401537804357113,
      "grad_norm": 0.6120349764823914,
      "learning_rate": 0.00013074220048435347,
      "loss": 0.6719,
      "step": 73050
    },
    {
      "epoch": 1.0408657268973374,
      "grad_norm": 0.4541325271129608,
      "learning_rate": 0.00013069471484875825,
      "loss": 0.6352,
      "step": 73100
    },
    {
      "epoch": 1.0415776733589635,
      "grad_norm": 0.5085487365722656,
      "learning_rate": 0.000130647229213163,
      "loss": 0.5957,
      "step": 73150
    },
    {
      "epoch": 1.0422896198205895,
      "grad_norm": 0.5656979084014893,
      "learning_rate": 0.00013059974357756778,
      "loss": 0.6299,
      "step": 73200
    },
    {
      "epoch": 1.0430015662822156,
      "grad_norm": 0.4027128517627716,
      "learning_rate": 0.00013055225794197256,
      "loss": 0.6393,
      "step": 73250
    },
    {
      "epoch": 1.0437135127438417,
      "grad_norm": 0.5045961141586304,
      "learning_rate": 0.00013050477230637733,
      "loss": 0.6505,
      "step": 73300
    },
    {
      "epoch": 1.0444254592054678,
      "grad_norm": 0.6222733855247498,
      "learning_rate": 0.00013045728667078208,
      "loss": 0.6488,
      "step": 73350
    },
    {
      "epoch": 1.045137405667094,
      "grad_norm": 0.7261214256286621,
      "learning_rate": 0.00013040980103518686,
      "loss": 0.6523,
      "step": 73400
    },
    {
      "epoch": 1.04584935212872,
      "grad_norm": 0.615666925907135,
      "learning_rate": 0.00013036231539959164,
      "loss": 0.6135,
      "step": 73450
    },
    {
      "epoch": 1.046561298590346,
      "grad_norm": 0.47921210527420044,
      "learning_rate": 0.00013031482976399642,
      "loss": 0.639,
      "step": 73500
    },
    {
      "epoch": 1.0472732450519722,
      "grad_norm": 0.6548013091087341,
      "learning_rate": 0.00013026734412840117,
      "loss": 0.6575,
      "step": 73550
    },
    {
      "epoch": 1.0479851915135983,
      "grad_norm": 0.46308520436286926,
      "learning_rate": 0.00013021985849280595,
      "loss": 0.6485,
      "step": 73600
    },
    {
      "epoch": 1.0486971379752243,
      "grad_norm": 0.5768016576766968,
      "learning_rate": 0.0001301723728572107,
      "loss": 0.6185,
      "step": 73650
    },
    {
      "epoch": 1.0494090844368504,
      "grad_norm": 0.5039998292922974,
      "learning_rate": 0.00013012583693432738,
      "loss": 0.6945,
      "step": 73700
    },
    {
      "epoch": 1.0501210308984765,
      "grad_norm": 0.6314393877983093,
      "learning_rate": 0.00013007835129873213,
      "loss": 0.7035,
      "step": 73750
    },
    {
      "epoch": 1.0508329773601026,
      "grad_norm": 0.718153715133667,
      "learning_rate": 0.0001300308656631369,
      "loss": 0.6743,
      "step": 73800
    },
    {
      "epoch": 1.0515449238217287,
      "grad_norm": 0.5145428776741028,
      "learning_rate": 0.00012998338002754168,
      "loss": 0.6659,
      "step": 73850
    },
    {
      "epoch": 1.0522568702833548,
      "grad_norm": 0.5465488433837891,
      "learning_rate": 0.00012993589439194643,
      "loss": 0.6103,
      "step": 73900
    },
    {
      "epoch": 1.0529688167449809,
      "grad_norm": 0.6335552930831909,
      "learning_rate": 0.0001298884087563512,
      "loss": 0.6238,
      "step": 73950
    },
    {
      "epoch": 1.0536807632066068,
      "grad_norm": 0.7119390368461609,
      "learning_rate": 0.000129840923120756,
      "loss": 0.6916,
      "step": 74000
    },
    {
      "epoch": 1.0543927096682328,
      "grad_norm": 0.3851250410079956,
      "learning_rate": 0.00012979343748516076,
      "loss": 0.6516,
      "step": 74050
    },
    {
      "epoch": 1.055104656129859,
      "grad_norm": 0.4404018521308899,
      "learning_rate": 0.00012974595184956551,
      "loss": 0.6514,
      "step": 74100
    },
    {
      "epoch": 1.055816602591485,
      "grad_norm": 0.7143114805221558,
      "learning_rate": 0.0001296984662139703,
      "loss": 0.6239,
      "step": 74150
    },
    {
      "epoch": 1.056528549053111,
      "grad_norm": 0.5653202533721924,
      "learning_rate": 0.00012965098057837507,
      "loss": 0.5953,
      "step": 74200
    },
    {
      "epoch": 1.0572404955147372,
      "grad_norm": 0.7066905498504639,
      "learning_rate": 0.00012960349494277982,
      "loss": 0.6908,
      "step": 74250
    },
    {
      "epoch": 1.0579524419763633,
      "grad_norm": 0.5152473449707031,
      "learning_rate": 0.0001295560093071846,
      "loss": 0.6632,
      "step": 74300
    },
    {
      "epoch": 1.0586643884379894,
      "grad_norm": 0.6052958965301514,
      "learning_rate": 0.00012950852367158935,
      "loss": 0.6437,
      "step": 74350
    },
    {
      "epoch": 1.0593763348996155,
      "grad_norm": 0.4238373041152954,
      "learning_rate": 0.00012946103803599413,
      "loss": 0.6015,
      "step": 74400
    },
    {
      "epoch": 1.0600882813612416,
      "grad_norm": 0.49731799960136414,
      "learning_rate": 0.0001294135524003989,
      "loss": 0.6462,
      "step": 74450
    },
    {
      "epoch": 1.0608002278228676,
      "grad_norm": 0.5386534929275513,
      "learning_rate": 0.00012936606676480365,
      "loss": 0.6705,
      "step": 74500
    },
    {
      "epoch": 1.0615121742844937,
      "grad_norm": 0.6378179788589478,
      "learning_rate": 0.00012931858112920843,
      "loss": 0.6689,
      "step": 74550
    },
    {
      "epoch": 1.0622241207461198,
      "grad_norm": 0.5952918529510498,
      "learning_rate": 0.00012927109549361318,
      "loss": 0.6563,
      "step": 74600
    },
    {
      "epoch": 1.062936067207746,
      "grad_norm": 0.5180428624153137,
      "learning_rate": 0.00012922360985801796,
      "loss": 0.6457,
      "step": 74650
    },
    {
      "epoch": 1.063648013669372,
      "grad_norm": 0.6019125580787659,
      "learning_rate": 0.00012917612422242274,
      "loss": 0.6887,
      "step": 74700
    },
    {
      "epoch": 1.064359960130998,
      "grad_norm": 0.5855721235275269,
      "learning_rate": 0.0001291286385868275,
      "loss": 0.6135,
      "step": 74750
    },
    {
      "epoch": 1.0650719065926242,
      "grad_norm": 0.5563069581985474,
      "learning_rate": 0.00012908115295123227,
      "loss": 0.6369,
      "step": 74800
    },
    {
      "epoch": 1.0657838530542503,
      "grad_norm": 0.5864172577857971,
      "learning_rate": 0.00012903366731563702,
      "loss": 0.6144,
      "step": 74850
    },
    {
      "epoch": 1.0664957995158764,
      "grad_norm": 0.3562120795249939,
      "learning_rate": 0.0001289861816800418,
      "loss": 0.6129,
      "step": 74900
    },
    {
      "epoch": 1.0672077459775025,
      "grad_norm": 0.6399267911911011,
      "learning_rate": 0.00012893869604444657,
      "loss": 0.6147,
      "step": 74950
    },
    {
      "epoch": 1.0679196924391285,
      "grad_norm": 0.4221695363521576,
      "learning_rate": 0.00012889121040885132,
      "loss": 0.6031,
      "step": 75000
    },
    {
      "epoch": 1.0686316389007546,
      "grad_norm": 0.4667661190032959,
      "learning_rate": 0.0001288437247732561,
      "loss": 0.6218,
      "step": 75050
    },
    {
      "epoch": 1.0693435853623807,
      "grad_norm": 0.6006444096565247,
      "learning_rate": 0.00012879623913766085,
      "loss": 0.6354,
      "step": 75100
    },
    {
      "epoch": 1.0700555318240068,
      "grad_norm": 0.3085099756717682,
      "learning_rate": 0.00012874875350206563,
      "loss": 0.6655,
      "step": 75150
    },
    {
      "epoch": 1.070767478285633,
      "grad_norm": 0.740900456905365,
      "learning_rate": 0.0001287012678664704,
      "loss": 0.599,
      "step": 75200
    },
    {
      "epoch": 1.071479424747259,
      "grad_norm": 0.3556308448314667,
      "learning_rate": 0.00012865378223087516,
      "loss": 0.6506,
      "step": 75250
    },
    {
      "epoch": 1.072191371208885,
      "grad_norm": 0.40551358461380005,
      "learning_rate": 0.00012860629659527993,
      "loss": 0.6353,
      "step": 75300
    },
    {
      "epoch": 1.0729033176705112,
      "grad_norm": 0.3658919632434845,
      "learning_rate": 0.00012855881095968468,
      "loss": 0.6524,
      "step": 75350
    },
    {
      "epoch": 1.0736152641321373,
      "grad_norm": 0.419251024723053,
      "learning_rate": 0.00012851132532408946,
      "loss": 0.639,
      "step": 75400
    },
    {
      "epoch": 1.0743272105937633,
      "grad_norm": 0.5992416143417358,
      "learning_rate": 0.00012846383968849424,
      "loss": 0.6432,
      "step": 75450
    },
    {
      "epoch": 1.0750391570553894,
      "grad_norm": 0.5974162817001343,
      "learning_rate": 0.000128416354052899,
      "loss": 0.6385,
      "step": 75500
    },
    {
      "epoch": 1.0757511035170155,
      "grad_norm": 0.35188233852386475,
      "learning_rate": 0.00012836886841730377,
      "loss": 0.5691,
      "step": 75550
    },
    {
      "epoch": 1.0764630499786416,
      "grad_norm": 0.49610304832458496,
      "learning_rate": 0.00012832138278170855,
      "loss": 0.6541,
      "step": 75600
    },
    {
      "epoch": 1.0771749964402677,
      "grad_norm": 0.6335718631744385,
      "learning_rate": 0.0001282738971461133,
      "loss": 0.6097,
      "step": 75650
    },
    {
      "epoch": 1.0778869429018938,
      "grad_norm": 0.6632840037345886,
      "learning_rate": 0.00012822641151051807,
      "loss": 0.6652,
      "step": 75700
    },
    {
      "epoch": 1.0785988893635199,
      "grad_norm": 0.44679558277130127,
      "learning_rate": 0.00012817892587492285,
      "loss": 0.5456,
      "step": 75750
    },
    {
      "epoch": 1.079310835825146,
      "grad_norm": 0.6476907134056091,
      "learning_rate": 0.00012813144023932763,
      "loss": 0.6128,
      "step": 75800
    },
    {
      "epoch": 1.080022782286772,
      "grad_norm": 0.8253048658370972,
      "learning_rate": 0.00012808395460373238,
      "loss": 0.6529,
      "step": 75850
    },
    {
      "epoch": 1.0807347287483982,
      "grad_norm": 0.4842992424964905,
      "learning_rate": 0.00012803646896813716,
      "loss": 0.6251,
      "step": 75900
    },
    {
      "epoch": 1.0814466752100242,
      "grad_norm": 0.6950103044509888,
      "learning_rate": 0.00012798898333254193,
      "loss": 0.6416,
      "step": 75950
    },
    {
      "epoch": 1.0821586216716503,
      "grad_norm": 0.6470046043395996,
      "learning_rate": 0.00012794149769694668,
      "loss": 0.6764,
      "step": 76000
    },
    {
      "epoch": 1.0828705681332764,
      "grad_norm": 0.6695711016654968,
      "learning_rate": 0.00012789401206135146,
      "loss": 0.5768,
      "step": 76050
    },
    {
      "epoch": 1.0835825145949025,
      "grad_norm": 0.5853793025016785,
      "learning_rate": 0.0001278465264257562,
      "loss": 0.6435,
      "step": 76100
    },
    {
      "epoch": 1.0842944610565286,
      "grad_norm": 0.6994169354438782,
      "learning_rate": 0.000127799040790161,
      "loss": 0.7012,
      "step": 76150
    },
    {
      "epoch": 1.0850064075181547,
      "grad_norm": 0.46821239590644836,
      "learning_rate": 0.00012775155515456577,
      "loss": 0.6784,
      "step": 76200
    },
    {
      "epoch": 1.0857183539797808,
      "grad_norm": 0.6295931339263916,
      "learning_rate": 0.00012770406951897052,
      "loss": 0.645,
      "step": 76250
    },
    {
      "epoch": 1.0864303004414069,
      "grad_norm": 0.41180166602134705,
      "learning_rate": 0.0001276565838833753,
      "loss": 0.6328,
      "step": 76300
    },
    {
      "epoch": 1.087142246903033,
      "grad_norm": 0.4445687234401703,
      "learning_rate": 0.00012760909824778005,
      "loss": 0.6456,
      "step": 76350
    },
    {
      "epoch": 1.087854193364659,
      "grad_norm": 0.761570394039154,
      "learning_rate": 0.00012756161261218482,
      "loss": 0.6671,
      "step": 76400
    },
    {
      "epoch": 1.0885661398262851,
      "grad_norm": 0.4337545335292816,
      "learning_rate": 0.0001275141269765896,
      "loss": 0.6947,
      "step": 76450
    },
    {
      "epoch": 1.0892780862879112,
      "grad_norm": 0.6299004554748535,
      "learning_rate": 0.00012746664134099435,
      "loss": 0.6918,
      "step": 76500
    },
    {
      "epoch": 1.0899900327495373,
      "grad_norm": 0.4356544017791748,
      "learning_rate": 0.00012741915570539913,
      "loss": 0.5819,
      "step": 76550
    },
    {
      "epoch": 1.0907019792111634,
      "grad_norm": 0.3799784183502197,
      "learning_rate": 0.00012737167006980388,
      "loss": 0.6461,
      "step": 76600
    },
    {
      "epoch": 1.0914139256727895,
      "grad_norm": 0.3944922685623169,
      "learning_rate": 0.00012732418443420866,
      "loss": 0.6282,
      "step": 76650
    },
    {
      "epoch": 1.0921258721344156,
      "grad_norm": 0.5191189646720886,
      "learning_rate": 0.00012727669879861344,
      "loss": 0.6358,
      "step": 76700
    },
    {
      "epoch": 1.0928378185960417,
      "grad_norm": 0.710904598236084,
      "learning_rate": 0.00012722921316301819,
      "loss": 0.6566,
      "step": 76750
    },
    {
      "epoch": 1.0935497650576678,
      "grad_norm": 0.3520302176475525,
      "learning_rate": 0.00012718172752742296,
      "loss": 0.6268,
      "step": 76800
    },
    {
      "epoch": 1.0942617115192936,
      "grad_norm": 0.718337893486023,
      "learning_rate": 0.00012713424189182771,
      "loss": 0.6921,
      "step": 76850
    },
    {
      "epoch": 1.09497365798092,
      "grad_norm": 0.7372567653656006,
      "learning_rate": 0.0001270867562562325,
      "loss": 0.6853,
      "step": 76900
    },
    {
      "epoch": 1.0956856044425458,
      "grad_norm": 0.723376989364624,
      "learning_rate": 0.00012703927062063727,
      "loss": 0.7047,
      "step": 76950
    },
    {
      "epoch": 1.096397550904172,
      "grad_norm": 0.6199811100959778,
      "learning_rate": 0.00012699178498504202,
      "loss": 0.6665,
      "step": 77000
    },
    {
      "epoch": 1.097109497365798,
      "grad_norm": 0.4857228696346283,
      "learning_rate": 0.0001269442993494468,
      "loss": 0.6195,
      "step": 77050
    },
    {
      "epoch": 1.097821443827424,
      "grad_norm": 0.41164425015449524,
      "learning_rate": 0.00012689681371385155,
      "loss": 0.6443,
      "step": 77100
    },
    {
      "epoch": 1.0985333902890502,
      "grad_norm": 0.44987988471984863,
      "learning_rate": 0.00012684932807825633,
      "loss": 0.6694,
      "step": 77150
    },
    {
      "epoch": 1.0992453367506763,
      "grad_norm": 0.574442982673645,
      "learning_rate": 0.0001268018424426611,
      "loss": 0.6561,
      "step": 77200
    },
    {
      "epoch": 1.0999572832123024,
      "grad_norm": 0.47841307520866394,
      "learning_rate": 0.00012675435680706585,
      "loss": 0.677,
      "step": 77250
    },
    {
      "epoch": 1.1006692296739284,
      "grad_norm": 0.607056736946106,
      "learning_rate": 0.00012670687117147063,
      "loss": 0.5966,
      "step": 77300
    },
    {
      "epoch": 1.1013811761355545,
      "grad_norm": 0.5439670085906982,
      "learning_rate": 0.0001266593855358754,
      "loss": 0.6498,
      "step": 77350
    },
    {
      "epoch": 1.1020931225971806,
      "grad_norm": 0.5435090065002441,
      "learning_rate": 0.0001266118999002802,
      "loss": 0.5926,
      "step": 77400
    },
    {
      "epoch": 1.1028050690588067,
      "grad_norm": 0.3557644188404083,
      "learning_rate": 0.00012656441426468494,
      "loss": 0.6188,
      "step": 77450
    },
    {
      "epoch": 1.1035170155204328,
      "grad_norm": 0.42524269223213196,
      "learning_rate": 0.00012651692862908971,
      "loss": 0.7052,
      "step": 77500
    },
    {
      "epoch": 1.1042289619820589,
      "grad_norm": 0.45997318625450134,
      "learning_rate": 0.0001264694429934945,
      "loss": 0.666,
      "step": 77550
    },
    {
      "epoch": 1.104940908443685,
      "grad_norm": 0.5411323308944702,
      "learning_rate": 0.00012642195735789924,
      "loss": 0.5951,
      "step": 77600
    },
    {
      "epoch": 1.105652854905311,
      "grad_norm": 0.499671071767807,
      "learning_rate": 0.00012637447172230402,
      "loss": 0.6308,
      "step": 77650
    },
    {
      "epoch": 1.1063648013669372,
      "grad_norm": 0.5207851529121399,
      "learning_rate": 0.0001263269860867088,
      "loss": 0.6713,
      "step": 77700
    },
    {
      "epoch": 1.1070767478285632,
      "grad_norm": 0.5120776891708374,
      "learning_rate": 0.00012627950045111355,
      "loss": 0.6708,
      "step": 77750
    },
    {
      "epoch": 1.1077886942901893,
      "grad_norm": 0.5503153204917908,
      "learning_rate": 0.00012623201481551833,
      "loss": 0.651,
      "step": 77800
    },
    {
      "epoch": 1.1085006407518154,
      "grad_norm": 0.6897556781768799,
      "learning_rate": 0.00012618452917992308,
      "loss": 0.6275,
      "step": 77850
    },
    {
      "epoch": 1.1092125872134415,
      "grad_norm": 0.42322614789009094,
      "learning_rate": 0.00012613704354432785,
      "loss": 0.6259,
      "step": 77900
    },
    {
      "epoch": 1.1099245336750676,
      "grad_norm": 0.8012436628341675,
      "learning_rate": 0.00012608955790873263,
      "loss": 0.6346,
      "step": 77950
    },
    {
      "epoch": 1.1106364801366937,
      "grad_norm": 0.5478126406669617,
      "learning_rate": 0.00012604207227313738,
      "loss": 0.5824,
      "step": 78000
    },
    {
      "epoch": 1.1113484265983198,
      "grad_norm": 0.5495908856391907,
      "learning_rate": 0.00012599458663754216,
      "loss": 0.616,
      "step": 78050
    },
    {
      "epoch": 1.1120603730599459,
      "grad_norm": 0.6825537085533142,
      "learning_rate": 0.0001259471010019469,
      "loss": 0.6811,
      "step": 78100
    },
    {
      "epoch": 1.112772319521572,
      "grad_norm": 0.5967145562171936,
      "learning_rate": 0.0001258996153663517,
      "loss": 0.5938,
      "step": 78150
    },
    {
      "epoch": 1.113484265983198,
      "grad_norm": 0.6156278252601624,
      "learning_rate": 0.00012585212973075647,
      "loss": 0.6304,
      "step": 78200
    },
    {
      "epoch": 1.1141962124448241,
      "grad_norm": 0.3813917636871338,
      "learning_rate": 0.00012580464409516122,
      "loss": 0.5934,
      "step": 78250
    },
    {
      "epoch": 1.1149081589064502,
      "grad_norm": 0.5153821706771851,
      "learning_rate": 0.000125757158459566,
      "loss": 0.6078,
      "step": 78300
    },
    {
      "epoch": 1.1156201053680763,
      "grad_norm": 0.5505998730659485,
      "learning_rate": 0.00012570967282397074,
      "loss": 0.6158,
      "step": 78350
    },
    {
      "epoch": 1.1163320518297024,
      "grad_norm": 0.3862374722957611,
      "learning_rate": 0.00012566218718837552,
      "loss": 0.6248,
      "step": 78400
    },
    {
      "epoch": 1.1170439982913285,
      "grad_norm": 0.7591134309768677,
      "learning_rate": 0.0001256147015527803,
      "loss": 0.7169,
      "step": 78450
    },
    {
      "epoch": 1.1177559447529546,
      "grad_norm": 0.5007666349411011,
      "learning_rate": 0.00012556721591718505,
      "loss": 0.6574,
      "step": 78500
    },
    {
      "epoch": 1.1184678912145807,
      "grad_norm": 0.49342745542526245,
      "learning_rate": 0.00012551973028158983,
      "loss": 0.6515,
      "step": 78550
    },
    {
      "epoch": 1.1191798376762068,
      "grad_norm": 0.5751875638961792,
      "learning_rate": 0.00012547224464599458,
      "loss": 0.668,
      "step": 78600
    },
    {
      "epoch": 1.1198917841378329,
      "grad_norm": 0.6296718716621399,
      "learning_rate": 0.00012542475901039936,
      "loss": 0.5925,
      "step": 78650
    },
    {
      "epoch": 1.120603730599459,
      "grad_norm": 0.5835669040679932,
      "learning_rate": 0.00012537727337480413,
      "loss": 0.6118,
      "step": 78700
    },
    {
      "epoch": 1.121315677061085,
      "grad_norm": 0.4339548349380493,
      "learning_rate": 0.00012532978773920888,
      "loss": 0.6522,
      "step": 78750
    },
    {
      "epoch": 1.1220276235227111,
      "grad_norm": 0.4658221900463104,
      "learning_rate": 0.00012528230210361366,
      "loss": 0.6559,
      "step": 78800
    },
    {
      "epoch": 1.1227395699843372,
      "grad_norm": 0.509620189666748,
      "learning_rate": 0.0001252348164680184,
      "loss": 0.623,
      "step": 78850
    },
    {
      "epoch": 1.1234515164459633,
      "grad_norm": 0.4452691972255707,
      "learning_rate": 0.0001251873308324232,
      "loss": 0.6339,
      "step": 78900
    },
    {
      "epoch": 1.1241634629075894,
      "grad_norm": 0.6928485035896301,
      "learning_rate": 0.00012513984519682797,
      "loss": 0.6735,
      "step": 78950
    },
    {
      "epoch": 1.1248754093692155,
      "grad_norm": 0.7790766954421997,
      "learning_rate": 0.00012509235956123272,
      "loss": 0.6686,
      "step": 79000
    },
    {
      "epoch": 1.1255873558308416,
      "grad_norm": 0.4883701801300049,
      "learning_rate": 0.0001250448739256375,
      "loss": 0.6296,
      "step": 79050
    },
    {
      "epoch": 1.1262993022924677,
      "grad_norm": 0.9177268743515015,
      "learning_rate": 0.00012499738829004227,
      "loss": 0.6392,
      "step": 79100
    },
    {
      "epoch": 1.1270112487540938,
      "grad_norm": 0.4815865755081177,
      "learning_rate": 0.00012494990265444705,
      "loss": 0.5717,
      "step": 79150
    },
    {
      "epoch": 1.1277231952157198,
      "grad_norm": 0.39157480001449585,
      "learning_rate": 0.0001249024170188518,
      "loss": 0.6409,
      "step": 79200
    },
    {
      "epoch": 1.128435141677346,
      "grad_norm": 0.7131044864654541,
      "learning_rate": 0.00012485493138325658,
      "loss": 0.5869,
      "step": 79250
    },
    {
      "epoch": 1.129147088138972,
      "grad_norm": 0.7746138572692871,
      "learning_rate": 0.00012480744574766136,
      "loss": 0.622,
      "step": 79300
    },
    {
      "epoch": 1.1298590346005981,
      "grad_norm": 0.5281521677970886,
      "learning_rate": 0.000124760909824778,
      "loss": 0.6559,
      "step": 79350
    },
    {
      "epoch": 1.1305709810622242,
      "grad_norm": 0.6627699732780457,
      "learning_rate": 0.00012471342418918276,
      "loss": 0.6313,
      "step": 79400
    },
    {
      "epoch": 1.1312829275238503,
      "grad_norm": 0.514557421207428,
      "learning_rate": 0.00012466593855358754,
      "loss": 0.6571,
      "step": 79450
    },
    {
      "epoch": 1.1319948739854764,
      "grad_norm": 0.4793640673160553,
      "learning_rate": 0.00012461845291799231,
      "loss": 0.5787,
      "step": 79500
    },
    {
      "epoch": 1.1327068204471025,
      "grad_norm": 0.5255433320999146,
      "learning_rate": 0.00012457096728239706,
      "loss": 0.6269,
      "step": 79550
    },
    {
      "epoch": 1.1334187669087286,
      "grad_norm": 0.642378032207489,
      "learning_rate": 0.00012452348164680184,
      "loss": 0.6288,
      "step": 79600
    },
    {
      "epoch": 1.1341307133703546,
      "grad_norm": 0.35937926173210144,
      "learning_rate": 0.00012447599601120662,
      "loss": 0.6461,
      "step": 79650
    },
    {
      "epoch": 1.1348426598319805,
      "grad_norm": 0.6864522099494934,
      "learning_rate": 0.0001244285103756114,
      "loss": 0.5968,
      "step": 79700
    },
    {
      "epoch": 1.1355546062936068,
      "grad_norm": 0.45294928550720215,
      "learning_rate": 0.00012438102474001615,
      "loss": 0.6081,
      "step": 79750
    },
    {
      "epoch": 1.1362665527552327,
      "grad_norm": 0.34424737095832825,
      "learning_rate": 0.00012433353910442093,
      "loss": 0.587,
      "step": 79800
    },
    {
      "epoch": 1.136978499216859,
      "grad_norm": 0.4912426769733429,
      "learning_rate": 0.0001242860534688257,
      "loss": 0.6496,
      "step": 79850
    },
    {
      "epoch": 1.1376904456784849,
      "grad_norm": 0.4884980022907257,
      "learning_rate": 0.00012423856783323045,
      "loss": 0.6682,
      "step": 79900
    },
    {
      "epoch": 1.1384023921401112,
      "grad_norm": 0.7606609463691711,
      "learning_rate": 0.00012419108219763523,
      "loss": 0.6417,
      "step": 79950
    },
    {
      "epoch": 1.139114338601737,
      "grad_norm": 0.9663441181182861,
      "learning_rate": 0.00012414359656204,
      "loss": 0.7212,
      "step": 80000
    },
    {
      "epoch": 1.1398262850633631,
      "grad_norm": 0.6302400827407837,
      "learning_rate": 0.00012409611092644476,
      "loss": 0.6521,
      "step": 80050
    },
    {
      "epoch": 1.1405382315249892,
      "grad_norm": 0.632768988609314,
      "learning_rate": 0.00012404862529084954,
      "loss": 0.6307,
      "step": 80100
    },
    {
      "epoch": 1.1412501779866153,
      "grad_norm": 0.4414566457271576,
      "learning_rate": 0.0001240011396552543,
      "loss": 0.6397,
      "step": 80150
    },
    {
      "epoch": 1.1419621244482414,
      "grad_norm": 0.44862237572669983,
      "learning_rate": 0.00012395365401965907,
      "loss": 0.6627,
      "step": 80200
    },
    {
      "epoch": 1.1426740709098675,
      "grad_norm": 0.6182256937026978,
      "learning_rate": 0.00012390616838406384,
      "loss": 0.5699,
      "step": 80250
    },
    {
      "epoch": 1.1433860173714936,
      "grad_norm": 0.5981020331382751,
      "learning_rate": 0.0001238586827484686,
      "loss": 0.6784,
      "step": 80300
    },
    {
      "epoch": 1.1440979638331197,
      "grad_norm": 0.5081725120544434,
      "learning_rate": 0.00012381119711287337,
      "loss": 0.62,
      "step": 80350
    },
    {
      "epoch": 1.1448099102947458,
      "grad_norm": 0.4705864191055298,
      "learning_rate": 0.00012376371147727812,
      "loss": 0.6239,
      "step": 80400
    },
    {
      "epoch": 1.1455218567563719,
      "grad_norm": 0.5127399563789368,
      "learning_rate": 0.0001237162258416829,
      "loss": 0.6279,
      "step": 80450
    },
    {
      "epoch": 1.146233803217998,
      "grad_norm": 0.40584778785705566,
      "learning_rate": 0.00012366874020608768,
      "loss": 0.6227,
      "step": 80500
    },
    {
      "epoch": 1.146945749679624,
      "grad_norm": 0.6023839712142944,
      "learning_rate": 0.00012362125457049243,
      "loss": 0.5665,
      "step": 80550
    },
    {
      "epoch": 1.1476576961412501,
      "grad_norm": 0.32110491394996643,
      "learning_rate": 0.0001235737689348972,
      "loss": 0.6357,
      "step": 80600
    },
    {
      "epoch": 1.1483696426028762,
      "grad_norm": 0.5040194988250732,
      "learning_rate": 0.00012352628329930196,
      "loss": 0.5586,
      "step": 80650
    },
    {
      "epoch": 1.1490815890645023,
      "grad_norm": 0.4408739507198334,
      "learning_rate": 0.00012347879766370673,
      "loss": 0.6683,
      "step": 80700
    },
    {
      "epoch": 1.1497935355261284,
      "grad_norm": 0.5244653224945068,
      "learning_rate": 0.0001234313120281115,
      "loss": 0.5987,
      "step": 80750
    },
    {
      "epoch": 1.1505054819877545,
      "grad_norm": 0.5429168939590454,
      "learning_rate": 0.00012338382639251626,
      "loss": 0.6685,
      "step": 80800
    },
    {
      "epoch": 1.1512174284493806,
      "grad_norm": 0.7814369797706604,
      "learning_rate": 0.00012333634075692104,
      "loss": 0.6851,
      "step": 80850
    },
    {
      "epoch": 1.1519293749110067,
      "grad_norm": 0.7251108288764954,
      "learning_rate": 0.0001232888551213258,
      "loss": 0.6674,
      "step": 80900
    },
    {
      "epoch": 1.1526413213726328,
      "grad_norm": 0.5124555230140686,
      "learning_rate": 0.00012324136948573057,
      "loss": 0.5995,
      "step": 80950
    },
    {
      "epoch": 1.1533532678342588,
      "grad_norm": 0.5844905972480774,
      "learning_rate": 0.00012319388385013534,
      "loss": 0.6212,
      "step": 81000
    },
    {
      "epoch": 1.154065214295885,
      "grad_norm": 0.38718733191490173,
      "learning_rate": 0.0001231463982145401,
      "loss": 0.6643,
      "step": 81050
    },
    {
      "epoch": 1.154777160757511,
      "grad_norm": 0.34475547075271606,
      "learning_rate": 0.00012309891257894487,
      "loss": 0.6574,
      "step": 81100
    },
    {
      "epoch": 1.1554891072191371,
      "grad_norm": 0.6847631931304932,
      "learning_rate": 0.00012305142694334962,
      "loss": 0.6412,
      "step": 81150
    },
    {
      "epoch": 1.1562010536807632,
      "grad_norm": 0.7536608576774597,
      "learning_rate": 0.0001230039413077544,
      "loss": 0.6707,
      "step": 81200
    },
    {
      "epoch": 1.1569130001423893,
      "grad_norm": 0.6436951160430908,
      "learning_rate": 0.00012295645567215918,
      "loss": 0.6493,
      "step": 81250
    },
    {
      "epoch": 1.1576249466040154,
      "grad_norm": 0.43873143196105957,
      "learning_rate": 0.00012290897003656393,
      "loss": 0.6248,
      "step": 81300
    },
    {
      "epoch": 1.1583368930656415,
      "grad_norm": 0.5641525387763977,
      "learning_rate": 0.0001228624341136806,
      "loss": 0.6477,
      "step": 81350
    },
    {
      "epoch": 1.1590488395272676,
      "grad_norm": 0.495817631483078,
      "learning_rate": 0.00012281494847808539,
      "loss": 0.6179,
      "step": 81400
    },
    {
      "epoch": 1.1597607859888936,
      "grad_norm": 0.7398681044578552,
      "learning_rate": 0.00012276746284249014,
      "loss": 0.662,
      "step": 81450
    },
    {
      "epoch": 1.1604727324505197,
      "grad_norm": 0.7016348838806152,
      "learning_rate": 0.00012271997720689491,
      "loss": 0.6412,
      "step": 81500
    },
    {
      "epoch": 1.1611846789121458,
      "grad_norm": 0.4703695774078369,
      "learning_rate": 0.0001226724915712997,
      "loss": 0.6551,
      "step": 81550
    },
    {
      "epoch": 1.161896625373772,
      "grad_norm": 0.5309774279594421,
      "learning_rate": 0.00012262500593570444,
      "loss": 0.6616,
      "step": 81600
    },
    {
      "epoch": 1.162608571835398,
      "grad_norm": 0.5601902008056641,
      "learning_rate": 0.00012257752030010922,
      "loss": 0.6768,
      "step": 81650
    },
    {
      "epoch": 1.163320518297024,
      "grad_norm": 0.6118005514144897,
      "learning_rate": 0.00012253003466451397,
      "loss": 0.6889,
      "step": 81700
    },
    {
      "epoch": 1.1640324647586502,
      "grad_norm": 0.7931361198425293,
      "learning_rate": 0.00012248254902891875,
      "loss": 0.6448,
      "step": 81750
    },
    {
      "epoch": 1.1647444112202763,
      "grad_norm": 0.4998894929885864,
      "learning_rate": 0.00012243506339332353,
      "loss": 0.6129,
      "step": 81800
    },
    {
      "epoch": 1.1654563576819024,
      "grad_norm": 0.7139471173286438,
      "learning_rate": 0.00012238757775772828,
      "loss": 0.6574,
      "step": 81850
    },
    {
      "epoch": 1.1661683041435285,
      "grad_norm": 0.6440128684043884,
      "learning_rate": 0.00012234009212213305,
      "loss": 0.6794,
      "step": 81900
    },
    {
      "epoch": 1.1668802506051545,
      "grad_norm": 0.42392343282699585,
      "learning_rate": 0.00012229260648653783,
      "loss": 0.651,
      "step": 81950
    },
    {
      "epoch": 1.1675921970667806,
      "grad_norm": 0.5483638048171997,
      "learning_rate": 0.0001222451208509426,
      "loss": 0.6368,
      "step": 82000
    },
    {
      "epoch": 1.1683041435284067,
      "grad_norm": 0.6621649861335754,
      "learning_rate": 0.00012219763521534739,
      "loss": 0.6498,
      "step": 82050
    },
    {
      "epoch": 1.1690160899900328,
      "grad_norm": 0.436236172914505,
      "learning_rate": 0.00012215014957975214,
      "loss": 0.6373,
      "step": 82100
    },
    {
      "epoch": 1.169728036451659,
      "grad_norm": 0.4829235076904297,
      "learning_rate": 0.00012210266394415691,
      "loss": 0.6593,
      "step": 82150
    },
    {
      "epoch": 1.170439982913285,
      "grad_norm": 0.4938903748989105,
      "learning_rate": 0.00012205517830856165,
      "loss": 0.6244,
      "step": 82200
    },
    {
      "epoch": 1.171151929374911,
      "grad_norm": 0.42567333579063416,
      "learning_rate": 0.00012200769267296643,
      "loss": 0.6999,
      "step": 82250
    },
    {
      "epoch": 1.1718638758365372,
      "grad_norm": 0.5664659738540649,
      "learning_rate": 0.0001219602070373712,
      "loss": 0.6443,
      "step": 82300
    },
    {
      "epoch": 1.1725758222981633,
      "grad_norm": 0.7288784980773926,
      "learning_rate": 0.00012191272140177597,
      "loss": 0.6339,
      "step": 82350
    },
    {
      "epoch": 1.1732877687597894,
      "grad_norm": 0.4669366180896759,
      "learning_rate": 0.00012186523576618073,
      "loss": 0.7126,
      "step": 82400
    },
    {
      "epoch": 1.1739997152214154,
      "grad_norm": 0.36396676301956177,
      "learning_rate": 0.0001218177501305855,
      "loss": 0.6508,
      "step": 82450
    },
    {
      "epoch": 1.1747116616830415,
      "grad_norm": 0.5798486471176147,
      "learning_rate": 0.00012177026449499028,
      "loss": 0.625,
      "step": 82500
    },
    {
      "epoch": 1.1754236081446674,
      "grad_norm": 0.6567514538764954,
      "learning_rate": 0.00012172277885939505,
      "loss": 0.6241,
      "step": 82550
    },
    {
      "epoch": 1.1761355546062937,
      "grad_norm": 0.673300564289093,
      "learning_rate": 0.0001216752932237998,
      "loss": 0.6245,
      "step": 82600
    },
    {
      "epoch": 1.1768475010679196,
      "grad_norm": 0.5054097175598145,
      "learning_rate": 0.00012162780758820458,
      "loss": 0.6511,
      "step": 82650
    },
    {
      "epoch": 1.1775594475295459,
      "grad_norm": 0.7798122763633728,
      "learning_rate": 0.00012158032195260933,
      "loss": 0.6018,
      "step": 82700
    },
    {
      "epoch": 1.1782713939911718,
      "grad_norm": 0.5829262733459473,
      "learning_rate": 0.00012153283631701411,
      "loss": 0.6226,
      "step": 82750
    },
    {
      "epoch": 1.178983340452798,
      "grad_norm": 0.6176687479019165,
      "learning_rate": 0.00012148535068141889,
      "loss": 0.6173,
      "step": 82800
    },
    {
      "epoch": 1.179695286914424,
      "grad_norm": 0.6105321645736694,
      "learning_rate": 0.00012143786504582364,
      "loss": 0.6609,
      "step": 82850
    },
    {
      "epoch": 1.18040723337605,
      "grad_norm": 0.36121881008148193,
      "learning_rate": 0.00012139037941022842,
      "loss": 0.6026,
      "step": 82900
    },
    {
      "epoch": 1.1811191798376761,
      "grad_norm": 0.48006096482276917,
      "learning_rate": 0.00012134289377463317,
      "loss": 0.6227,
      "step": 82950
    },
    {
      "epoch": 1.1818311262993022,
      "grad_norm": 0.517505943775177,
      "learning_rate": 0.00012129540813903794,
      "loss": 0.673,
      "step": 83000
    },
    {
      "epoch": 1.1825430727609283,
      "grad_norm": 0.5116896033287048,
      "learning_rate": 0.00012124792250344272,
      "loss": 0.6747,
      "step": 83050
    },
    {
      "epoch": 1.1832550192225544,
      "grad_norm": 0.33456534147262573,
      "learning_rate": 0.00012120043686784747,
      "loss": 0.6221,
      "step": 83100
    },
    {
      "epoch": 1.1839669656841805,
      "grad_norm": 0.5668981075286865,
      "learning_rate": 0.00012115295123225225,
      "loss": 0.6226,
      "step": 83150
    },
    {
      "epoch": 1.1846789121458066,
      "grad_norm": 0.5491194128990173,
      "learning_rate": 0.00012110546559665701,
      "loss": 0.6748,
      "step": 83200
    },
    {
      "epoch": 1.1853908586074327,
      "grad_norm": 0.6286929249763489,
      "learning_rate": 0.00012105797996106179,
      "loss": 0.5984,
      "step": 83250
    },
    {
      "epoch": 1.1861028050690587,
      "grad_norm": 0.5300699472427368,
      "learning_rate": 0.00012101049432546656,
      "loss": 0.6643,
      "step": 83300
    },
    {
      "epoch": 1.1868147515306848,
      "grad_norm": 0.6504377722740173,
      "learning_rate": 0.00012096395840258323,
      "loss": 0.7097,
      "step": 83350
    },
    {
      "epoch": 1.187526697992311,
      "grad_norm": 0.6779047846794128,
      "learning_rate": 0.00012091647276698798,
      "loss": 0.6502,
      "step": 83400
    },
    {
      "epoch": 1.188238644453937,
      "grad_norm": 0.5820705890655518,
      "learning_rate": 0.00012086898713139276,
      "loss": 0.6187,
      "step": 83450
    },
    {
      "epoch": 1.188950590915563,
      "grad_norm": 0.47321590781211853,
      "learning_rate": 0.00012082150149579751,
      "loss": 0.6762,
      "step": 83500
    },
    {
      "epoch": 1.1896625373771892,
      "grad_norm": 0.6179601550102234,
      "learning_rate": 0.00012077401586020229,
      "loss": 0.6389,
      "step": 83550
    },
    {
      "epoch": 1.1903744838388153,
      "grad_norm": 0.561394214630127,
      "learning_rate": 0.00012072653022460707,
      "loss": 0.6551,
      "step": 83600
    },
    {
      "epoch": 1.1910864303004414,
      "grad_norm": 0.5587347745895386,
      "learning_rate": 0.00012067904458901182,
      "loss": 0.598,
      "step": 83650
    },
    {
      "epoch": 1.1917983767620675,
      "grad_norm": 0.6557983160018921,
      "learning_rate": 0.0001206315589534166,
      "loss": 0.6528,
      "step": 83700
    },
    {
      "epoch": 1.1925103232236935,
      "grad_norm": 0.39591389894485474,
      "learning_rate": 0.00012058407331782136,
      "loss": 0.6376,
      "step": 83750
    },
    {
      "epoch": 1.1932222696853196,
      "grad_norm": 0.6360346674919128,
      "learning_rate": 0.00012053658768222614,
      "loss": 0.6262,
      "step": 83800
    },
    {
      "epoch": 1.1939342161469457,
      "grad_norm": 0.6417831778526306,
      "learning_rate": 0.0001204891020466309,
      "loss": 0.6315,
      "step": 83850
    },
    {
      "epoch": 1.1946461626085718,
      "grad_norm": 0.7036064267158508,
      "learning_rate": 0.00012044161641103567,
      "loss": 0.673,
      "step": 83900
    },
    {
      "epoch": 1.195358109070198,
      "grad_norm": 0.4936591386795044,
      "learning_rate": 0.00012039413077544044,
      "loss": 0.6706,
      "step": 83950
    },
    {
      "epoch": 1.196070055531824,
      "grad_norm": 0.5457066893577576,
      "learning_rate": 0.0001203466451398452,
      "loss": 0.6565,
      "step": 84000
    },
    {
      "epoch": 1.19678200199345,
      "grad_norm": 0.5469571948051453,
      "learning_rate": 0.00012029915950424997,
      "loss": 0.5796,
      "step": 84050
    },
    {
      "epoch": 1.1974939484550762,
      "grad_norm": 0.41801396012306213,
      "learning_rate": 0.00012025167386865475,
      "loss": 0.6271,
      "step": 84100
    },
    {
      "epoch": 1.1982058949167023,
      "grad_norm": 0.7199550867080688,
      "learning_rate": 0.0001202041882330595,
      "loss": 0.6811,
      "step": 84150
    },
    {
      "epoch": 1.1989178413783284,
      "grad_norm": 0.4298650324344635,
      "learning_rate": 0.00012015670259746428,
      "loss": 0.6362,
      "step": 84200
    },
    {
      "epoch": 1.1996297878399544,
      "grad_norm": 0.5010488629341125,
      "learning_rate": 0.00012010921696186903,
      "loss": 0.6546,
      "step": 84250
    },
    {
      "epoch": 1.2003417343015805,
      "grad_norm": 0.9569803476333618,
      "learning_rate": 0.0001200617313262738,
      "loss": 0.6379,
      "step": 84300
    },
    {
      "epoch": 1.2010536807632066,
      "grad_norm": 0.4436652958393097,
      "learning_rate": 0.00012001424569067858,
      "loss": 0.6308,
      "step": 84350
    },
    {
      "epoch": 1.2017656272248327,
      "grad_norm": 0.6448560953140259,
      "learning_rate": 0.00011996676005508333,
      "loss": 0.6139,
      "step": 84400
    },
    {
      "epoch": 1.2024775736864588,
      "grad_norm": 0.5185601711273193,
      "learning_rate": 0.00011991927441948811,
      "loss": 0.6491,
      "step": 84450
    },
    {
      "epoch": 1.203189520148085,
      "grad_norm": 0.5649291276931763,
      "learning_rate": 0.00011987178878389288,
      "loss": 0.6241,
      "step": 84500
    },
    {
      "epoch": 1.203901466609711,
      "grad_norm": 0.4994717538356781,
      "learning_rate": 0.00011982430314829764,
      "loss": 0.6466,
      "step": 84550
    },
    {
      "epoch": 1.204613413071337,
      "grad_norm": 0.6759666800498962,
      "learning_rate": 0.00011977681751270242,
      "loss": 0.6282,
      "step": 84600
    },
    {
      "epoch": 1.2053253595329632,
      "grad_norm": 0.5458375215530396,
      "learning_rate": 0.00011972933187710718,
      "loss": 0.6468,
      "step": 84650
    },
    {
      "epoch": 1.2060373059945892,
      "grad_norm": 0.42347416281700134,
      "learning_rate": 0.00011968184624151196,
      "loss": 0.669,
      "step": 84700
    },
    {
      "epoch": 1.2067492524562153,
      "grad_norm": 0.7108750343322754,
      "learning_rate": 0.00011963436060591671,
      "loss": 0.6352,
      "step": 84750
    },
    {
      "epoch": 1.2074611989178414,
      "grad_norm": 0.3447907567024231,
      "learning_rate": 0.00011958687497032149,
      "loss": 0.5964,
      "step": 84800
    },
    {
      "epoch": 1.2081731453794675,
      "grad_norm": 0.8042305707931519,
      "learning_rate": 0.00011953938933472626,
      "loss": 0.5994,
      "step": 84850
    },
    {
      "epoch": 1.2088850918410936,
      "grad_norm": 0.4343103766441345,
      "learning_rate": 0.00011949190369913102,
      "loss": 0.6217,
      "step": 84900
    },
    {
      "epoch": 1.2095970383027197,
      "grad_norm": 0.5752166509628296,
      "learning_rate": 0.00011944441806353579,
      "loss": 0.6516,
      "step": 84950
    },
    {
      "epoch": 1.2103089847643458,
      "grad_norm": 0.586615264415741,
      "learning_rate": 0.00011939693242794054,
      "loss": 0.6627,
      "step": 85000
    },
    {
      "epoch": 1.2110209312259719,
      "grad_norm": 0.6658962965011597,
      "learning_rate": 0.00011934944679234532,
      "loss": 0.6375,
      "step": 85050
    },
    {
      "epoch": 1.211732877687598,
      "grad_norm": 0.5890330672264099,
      "learning_rate": 0.0001193019611567501,
      "loss": 0.6975,
      "step": 85100
    },
    {
      "epoch": 1.212444824149224,
      "grad_norm": 0.40174001455307007,
      "learning_rate": 0.00011925447552115485,
      "loss": 0.6568,
      "step": 85150
    },
    {
      "epoch": 1.2131567706108501,
      "grad_norm": 0.6570588946342468,
      "learning_rate": 0.00011920698988555963,
      "loss": 0.6483,
      "step": 85200
    },
    {
      "epoch": 1.2138687170724762,
      "grad_norm": 0.6001108288764954,
      "learning_rate": 0.00011915950424996438,
      "loss": 0.6032,
      "step": 85250
    },
    {
      "epoch": 1.2145806635341023,
      "grad_norm": 0.48980116844177246,
      "learning_rate": 0.00011911201861436915,
      "loss": 0.6327,
      "step": 85300
    },
    {
      "epoch": 1.2152926099957284,
      "grad_norm": 0.4518028795719147,
      "learning_rate": 0.00011906453297877393,
      "loss": 0.6845,
      "step": 85350
    },
    {
      "epoch": 1.2160045564573545,
      "grad_norm": 0.7326459884643555,
      "learning_rate": 0.00011901704734317868,
      "loss": 0.7034,
      "step": 85400
    },
    {
      "epoch": 1.2167165029189806,
      "grad_norm": 0.4644005000591278,
      "learning_rate": 0.00011896956170758346,
      "loss": 0.6068,
      "step": 85450
    },
    {
      "epoch": 1.2174284493806065,
      "grad_norm": 0.562934935092926,
      "learning_rate": 0.00011892207607198822,
      "loss": 0.6563,
      "step": 85500
    },
    {
      "epoch": 1.2181403958422328,
      "grad_norm": 0.359614759683609,
      "learning_rate": 0.000118874590436393,
      "loss": 0.6308,
      "step": 85550
    },
    {
      "epoch": 1.2188523423038586,
      "grad_norm": 0.6145467758178711,
      "learning_rate": 0.00011882710480079777,
      "loss": 0.583,
      "step": 85600
    },
    {
      "epoch": 1.219564288765485,
      "grad_norm": 0.6830999255180359,
      "learning_rate": 0.00011878056887791445,
      "loss": 0.6734,
      "step": 85650
    },
    {
      "epoch": 1.2202762352271108,
      "grad_norm": 0.43134140968322754,
      "learning_rate": 0.0001187330832423192,
      "loss": 0.6712,
      "step": 85700
    },
    {
      "epoch": 1.2209881816887371,
      "grad_norm": 0.3605848550796509,
      "learning_rate": 0.00011868559760672397,
      "loss": 0.704,
      "step": 85750
    },
    {
      "epoch": 1.221700128150363,
      "grad_norm": 0.8014104962348938,
      "learning_rate": 0.00011863811197112872,
      "loss": 0.6206,
      "step": 85800
    },
    {
      "epoch": 1.222412074611989,
      "grad_norm": 0.5605967044830322,
      "learning_rate": 0.0001185906263355335,
      "loss": 0.7105,
      "step": 85850
    },
    {
      "epoch": 1.2231240210736152,
      "grad_norm": 0.58064866065979,
      "learning_rate": 0.00011854314069993828,
      "loss": 0.6432,
      "step": 85900
    },
    {
      "epoch": 1.2238359675352413,
      "grad_norm": 0.4976196587085724,
      "learning_rate": 0.00011849565506434303,
      "loss": 0.6599,
      "step": 85950
    },
    {
      "epoch": 1.2245479139968674,
      "grad_norm": 0.4494881331920624,
      "learning_rate": 0.00011844816942874781,
      "loss": 0.6374,
      "step": 86000
    },
    {
      "epoch": 1.2252598604584934,
      "grad_norm": 0.4285130798816681,
      "learning_rate": 0.00011840068379315257,
      "loss": 0.609,
      "step": 86050
    },
    {
      "epoch": 1.2259718069201195,
      "grad_norm": 1.2491116523742676,
      "learning_rate": 0.00011835319815755735,
      "loss": 0.5997,
      "step": 86100
    },
    {
      "epoch": 1.2266837533817456,
      "grad_norm": 0.9244603514671326,
      "learning_rate": 0.00011830571252196213,
      "loss": 0.5881,
      "step": 86150
    },
    {
      "epoch": 1.2273956998433717,
      "grad_norm": 0.5426732301712036,
      "learning_rate": 0.00011825822688636688,
      "loss": 0.6524,
      "step": 86200
    },
    {
      "epoch": 1.2281076463049978,
      "grad_norm": 0.5433690547943115,
      "learning_rate": 0.00011821074125077165,
      "loss": 0.6446,
      "step": 86250
    },
    {
      "epoch": 1.228819592766624,
      "grad_norm": 0.548119068145752,
      "learning_rate": 0.0001181632556151764,
      "loss": 0.6378,
      "step": 86300
    },
    {
      "epoch": 1.22953153922825,
      "grad_norm": 0.5953198075294495,
      "learning_rate": 0.00011811576997958118,
      "loss": 0.6109,
      "step": 86350
    },
    {
      "epoch": 1.230243485689876,
      "grad_norm": 0.7384390830993652,
      "learning_rate": 0.00011806828434398596,
      "loss": 0.6542,
      "step": 86400
    },
    {
      "epoch": 1.2309554321515022,
      "grad_norm": 0.7323955297470093,
      "learning_rate": 0.00011802079870839071,
      "loss": 0.661,
      "step": 86450
    },
    {
      "epoch": 1.2316673786131282,
      "grad_norm": 0.383976012468338,
      "learning_rate": 0.00011797331307279549,
      "loss": 0.6118,
      "step": 86500
    },
    {
      "epoch": 1.2323793250747543,
      "grad_norm": 0.5561837553977966,
      "learning_rate": 0.00011792582743720024,
      "loss": 0.6376,
      "step": 86550
    },
    {
      "epoch": 1.2330912715363804,
      "grad_norm": 0.5607913732528687,
      "learning_rate": 0.00011787834180160502,
      "loss": 0.6482,
      "step": 86600
    },
    {
      "epoch": 1.2338032179980065,
      "grad_norm": 0.49620315432548523,
      "learning_rate": 0.0001178308561660098,
      "loss": 0.6663,
      "step": 86650
    },
    {
      "epoch": 1.2345151644596326,
      "grad_norm": 0.5103002786636353,
      "learning_rate": 0.00011778337053041454,
      "loss": 0.6601,
      "step": 86700
    },
    {
      "epoch": 1.2352271109212587,
      "grad_norm": 0.3421765863895416,
      "learning_rate": 0.00011773588489481932,
      "loss": 0.6692,
      "step": 86750
    },
    {
      "epoch": 1.2359390573828848,
      "grad_norm": 0.8355620503425598,
      "learning_rate": 0.00011768839925922409,
      "loss": 0.7058,
      "step": 86800
    },
    {
      "epoch": 1.2366510038445109,
      "grad_norm": 0.7322311401367188,
      "learning_rate": 0.00011764091362362885,
      "loss": 0.6458,
      "step": 86850
    },
    {
      "epoch": 1.237362950306137,
      "grad_norm": 0.8604816794395447,
      "learning_rate": 0.00011759342798803363,
      "loss": 0.6643,
      "step": 86900
    },
    {
      "epoch": 1.238074896767763,
      "grad_norm": 0.5906254053115845,
      "learning_rate": 0.00011754594235243839,
      "loss": 0.6997,
      "step": 86950
    },
    {
      "epoch": 1.2387868432293891,
      "grad_norm": 0.5327112078666687,
      "learning_rate": 0.00011749845671684317,
      "loss": 0.689,
      "step": 87000
    },
    {
      "epoch": 1.2394987896910152,
      "grad_norm": 0.6385761499404907,
      "learning_rate": 0.00011745097108124792,
      "loss": 0.605,
      "step": 87050
    },
    {
      "epoch": 1.2402107361526413,
      "grad_norm": 0.9721237421035767,
      "learning_rate": 0.0001174034854456527,
      "loss": 0.6829,
      "step": 87100
    },
    {
      "epoch": 1.2409226826142674,
      "grad_norm": 0.5447736978530884,
      "learning_rate": 0.00011735599981005748,
      "loss": 0.6415,
      "step": 87150
    },
    {
      "epoch": 1.2416346290758935,
      "grad_norm": 0.7446763515472412,
      "learning_rate": 0.00011730851417446223,
      "loss": 0.6497,
      "step": 87200
    },
    {
      "epoch": 1.2423465755375196,
      "grad_norm": 0.5872963070869446,
      "learning_rate": 0.000117261028538867,
      "loss": 0.6291,
      "step": 87250
    },
    {
      "epoch": 1.2430585219991457,
      "grad_norm": 0.5727181434631348,
      "learning_rate": 0.00011721354290327175,
      "loss": 0.6304,
      "step": 87300
    },
    {
      "epoch": 1.2437704684607718,
      "grad_norm": 0.6551043391227722,
      "learning_rate": 0.00011716605726767653,
      "loss": 0.5684,
      "step": 87350
    },
    {
      "epoch": 1.2444824149223979,
      "grad_norm": 0.5675990581512451,
      "learning_rate": 0.00011711857163208131,
      "loss": 0.6445,
      "step": 87400
    },
    {
      "epoch": 1.245194361384024,
      "grad_norm": 0.5883653163909912,
      "learning_rate": 0.00011707108599648606,
      "loss": 0.5954,
      "step": 87450
    },
    {
      "epoch": 1.24590630784565,
      "grad_norm": 0.5159823298454285,
      "learning_rate": 0.00011702360036089084,
      "loss": 0.6398,
      "step": 87500
    },
    {
      "epoch": 1.2466182543072761,
      "grad_norm": 0.4138937294483185,
      "learning_rate": 0.00011697611472529559,
      "loss": 0.7074,
      "step": 87550
    },
    {
      "epoch": 1.2473302007689022,
      "grad_norm": 0.5408807396888733,
      "learning_rate": 0.00011692862908970037,
      "loss": 0.6083,
      "step": 87600
    },
    {
      "epoch": 1.2480421472305283,
      "grad_norm": 0.7860307693481445,
      "learning_rate": 0.00011688114345410514,
      "loss": 0.6724,
      "step": 87650
    },
    {
      "epoch": 1.2487540936921544,
      "grad_norm": 1.0201135873794556,
      "learning_rate": 0.00011683365781850991,
      "loss": 0.6448,
      "step": 87700
    },
    {
      "epoch": 1.2494660401537805,
      "grad_norm": 0.48549899458885193,
      "learning_rate": 0.00011678617218291467,
      "loss": 0.6087,
      "step": 87750
    },
    {
      "epoch": 1.2501779866154066,
      "grad_norm": 0.5152623653411865,
      "learning_rate": 0.00011673868654731944,
      "loss": 0.6353,
      "step": 87800
    },
    {
      "epoch": 1.2508899330770327,
      "grad_norm": 0.32421237230300903,
      "learning_rate": 0.00011669120091172421,
      "loss": 0.6652,
      "step": 87850
    },
    {
      "epoch": 1.2516018795386588,
      "grad_norm": 0.673136293888092,
      "learning_rate": 0.00011664371527612899,
      "loss": 0.6165,
      "step": 87900
    },
    {
      "epoch": 1.2523138260002848,
      "grad_norm": 0.5224128365516663,
      "learning_rate": 0.00011659622964053374,
      "loss": 0.6537,
      "step": 87950
    },
    {
      "epoch": 1.253025772461911,
      "grad_norm": 0.6389560103416443,
      "learning_rate": 0.00011654874400493852,
      "loss": 0.622,
      "step": 88000
    },
    {
      "epoch": 1.253737718923537,
      "grad_norm": 0.5555737018585205,
      "learning_rate": 0.00011650125836934327,
      "loss": 0.6746,
      "step": 88050
    },
    {
      "epoch": 1.2544496653851631,
      "grad_norm": 0.618391215801239,
      "learning_rate": 0.00011645377273374805,
      "loss": 0.6863,
      "step": 88100
    },
    {
      "epoch": 1.2551616118467892,
      "grad_norm": 0.5434377193450928,
      "learning_rate": 0.00011640628709815282,
      "loss": 0.6202,
      "step": 88150
    },
    {
      "epoch": 1.2558735583084153,
      "grad_norm": 0.6412264108657837,
      "learning_rate": 0.00011635975117526949,
      "loss": 0.6426,
      "step": 88200
    },
    {
      "epoch": 1.2565855047700412,
      "grad_norm": 0.5169031620025635,
      "learning_rate": 0.00011631226553967425,
      "loss": 0.675,
      "step": 88250
    },
    {
      "epoch": 1.2572974512316675,
      "grad_norm": 0.4379952549934387,
      "learning_rate": 0.00011626477990407902,
      "loss": 0.6194,
      "step": 88300
    },
    {
      "epoch": 1.2580093976932933,
      "grad_norm": 0.5500075221061707,
      "learning_rate": 0.00011621729426848378,
      "loss": 0.6128,
      "step": 88350
    },
    {
      "epoch": 1.2587213441549197,
      "grad_norm": 0.6119168996810913,
      "learning_rate": 0.00011616980863288856,
      "loss": 0.6688,
      "step": 88400
    },
    {
      "epoch": 1.2594332906165455,
      "grad_norm": 0.6101887226104736,
      "learning_rate": 0.00011612232299729334,
      "loss": 0.5844,
      "step": 88450
    },
    {
      "epoch": 1.2601452370781718,
      "grad_norm": 0.8326519131660461,
      "learning_rate": 0.00011607483736169809,
      "loss": 0.6636,
      "step": 88500
    },
    {
      "epoch": 1.2608571835397977,
      "grad_norm": 0.39014267921447754,
      "learning_rate": 0.00011602735172610287,
      "loss": 0.7016,
      "step": 88550
    },
    {
      "epoch": 1.261569130001424,
      "grad_norm": 0.645010769367218,
      "learning_rate": 0.00011597986609050762,
      "loss": 0.6248,
      "step": 88600
    },
    {
      "epoch": 1.2622810764630499,
      "grad_norm": 0.8832087516784668,
      "learning_rate": 0.0001159323804549124,
      "loss": 0.6403,
      "step": 88650
    },
    {
      "epoch": 1.2629930229246762,
      "grad_norm": 0.5681992173194885,
      "learning_rate": 0.00011588489481931717,
      "loss": 0.6461,
      "step": 88700
    },
    {
      "epoch": 1.263704969386302,
      "grad_norm": 0.5581429600715637,
      "learning_rate": 0.00011583740918372192,
      "loss": 0.6974,
      "step": 88750
    },
    {
      "epoch": 1.2644169158479284,
      "grad_norm": 0.6001472473144531,
      "learning_rate": 0.0001157899235481267,
      "loss": 0.6198,
      "step": 88800
    },
    {
      "epoch": 1.2651288623095542,
      "grad_norm": 0.5117323398590088,
      "learning_rate": 0.00011574243791253145,
      "loss": 0.6923,
      "step": 88850
    },
    {
      "epoch": 1.2658408087711803,
      "grad_norm": 0.49884656071662903,
      "learning_rate": 0.00011569495227693623,
      "loss": 0.6989,
      "step": 88900
    },
    {
      "epoch": 1.2665527552328064,
      "grad_norm": 0.45177319645881653,
      "learning_rate": 0.000115647466641341,
      "loss": 0.5702,
      "step": 88950
    },
    {
      "epoch": 1.2672647016944325,
      "grad_norm": 0.5537410378456116,
      "learning_rate": 0.00011559998100574576,
      "loss": 0.6588,
      "step": 89000
    },
    {
      "epoch": 1.2679766481560586,
      "grad_norm": 0.45546770095825195,
      "learning_rate": 0.00011555249537015053,
      "loss": 0.7179,
      "step": 89050
    },
    {
      "epoch": 1.2686885946176847,
      "grad_norm": 0.7360188364982605,
      "learning_rate": 0.0001155050097345553,
      "loss": 0.6996,
      "step": 89100
    },
    {
      "epoch": 1.2694005410793108,
      "grad_norm": 0.5186042785644531,
      "learning_rate": 0.00011545752409896007,
      "loss": 0.6619,
      "step": 89150
    },
    {
      "epoch": 1.2701124875409369,
      "grad_norm": 0.5101790428161621,
      "learning_rate": 0.00011541003846336484,
      "loss": 0.6397,
      "step": 89200
    },
    {
      "epoch": 1.270824434002563,
      "grad_norm": 0.5397406220436096,
      "learning_rate": 0.0001153625528277696,
      "loss": 0.6275,
      "step": 89250
    },
    {
      "epoch": 1.271536380464189,
      "grad_norm": 0.5763902068138123,
      "learning_rate": 0.00011531506719217438,
      "loss": 0.6034,
      "step": 89300
    },
    {
      "epoch": 1.2722483269258151,
      "grad_norm": 0.5997428894042969,
      "learning_rate": 0.00011526758155657913,
      "loss": 0.6647,
      "step": 89350
    },
    {
      "epoch": 1.2729602733874412,
      "grad_norm": 0.726417601108551,
      "learning_rate": 0.00011522009592098391,
      "loss": 0.6686,
      "step": 89400
    },
    {
      "epoch": 1.2736722198490673,
      "grad_norm": 0.6331364512443542,
      "learning_rate": 0.00011517261028538869,
      "loss": 0.6678,
      "step": 89450
    },
    {
      "epoch": 1.2743841663106934,
      "grad_norm": 0.5591617226600647,
      "learning_rate": 0.00011512512464979344,
      "loss": 0.6452,
      "step": 89500
    },
    {
      "epoch": 1.2750961127723195,
      "grad_norm": 0.5750671625137329,
      "learning_rate": 0.00011507763901419821,
      "loss": 0.6474,
      "step": 89550
    },
    {
      "epoch": 1.2758080592339456,
      "grad_norm": 0.41383108496665955,
      "learning_rate": 0.00011503015337860296,
      "loss": 0.6921,
      "step": 89600
    },
    {
      "epoch": 1.2765200056955717,
      "grad_norm": 0.3854621350765228,
      "learning_rate": 0.00011498266774300774,
      "loss": 0.6199,
      "step": 89650
    },
    {
      "epoch": 1.2772319521571978,
      "grad_norm": 0.7187201380729675,
      "learning_rate": 0.00011493518210741252,
      "loss": 0.6308,
      "step": 89700
    },
    {
      "epoch": 1.2779438986188238,
      "grad_norm": 0.7640855312347412,
      "learning_rate": 0.00011488769647181727,
      "loss": 0.7485,
      "step": 89750
    },
    {
      "epoch": 1.27865584508045,
      "grad_norm": 0.5544852018356323,
      "learning_rate": 0.00011484021083622205,
      "loss": 0.6923,
      "step": 89800
    },
    {
      "epoch": 1.279367791542076,
      "grad_norm": 0.7242328524589539,
      "learning_rate": 0.0001147927252006268,
      "loss": 0.5949,
      "step": 89850
    },
    {
      "epoch": 1.2800797380037021,
      "grad_norm": 0.42643195390701294,
      "learning_rate": 0.00011474523956503158,
      "loss": 0.6408,
      "step": 89900
    },
    {
      "epoch": 1.2807916844653282,
      "grad_norm": 0.34893155097961426,
      "learning_rate": 0.00011469775392943635,
      "loss": 0.6072,
      "step": 89950
    },
    {
      "epoch": 1.2815036309269543,
      "grad_norm": 0.6608164310455322,
      "learning_rate": 0.00011465026829384112,
      "loss": 0.59,
      "step": 90000
    },
    {
      "epoch": 1.2822155773885804,
      "grad_norm": 0.5650590658187866,
      "learning_rate": 0.00011460278265824588,
      "loss": 0.7226,
      "step": 90050
    },
    {
      "epoch": 1.2829275238502065,
      "grad_norm": 0.32474032044410706,
      "learning_rate": 0.00011455529702265065,
      "loss": 0.6427,
      "step": 90100
    },
    {
      "epoch": 1.2836394703118326,
      "grad_norm": 0.803831160068512,
      "learning_rate": 0.00011450781138705542,
      "loss": 0.6531,
      "step": 90150
    },
    {
      "epoch": 1.2843514167734587,
      "grad_norm": 0.7259895205497742,
      "learning_rate": 0.00011446127546417209,
      "loss": 0.6439,
      "step": 90200
    },
    {
      "epoch": 1.2850633632350847,
      "grad_norm": 0.7844935655593872,
      "learning_rate": 0.00011441378982857687,
      "loss": 0.6553,
      "step": 90250
    },
    {
      "epoch": 1.2857753096967108,
      "grad_norm": 0.8389991521835327,
      "learning_rate": 0.00011436630419298162,
      "loss": 0.6169,
      "step": 90300
    },
    {
      "epoch": 1.286487256158337,
      "grad_norm": 0.4017912447452545,
      "learning_rate": 0.0001143188185573864,
      "loss": 0.6136,
      "step": 90350
    },
    {
      "epoch": 1.287199202619963,
      "grad_norm": 0.38941431045532227,
      "learning_rate": 0.00011427133292179117,
      "loss": 0.6557,
      "step": 90400
    },
    {
      "epoch": 1.287911149081589,
      "grad_norm": 0.7317298054695129,
      "learning_rate": 0.00011422384728619592,
      "loss": 0.6125,
      "step": 90450
    },
    {
      "epoch": 1.2886230955432152,
      "grad_norm": 0.8977290987968445,
      "learning_rate": 0.0001141763616506007,
      "loss": 0.6705,
      "step": 90500
    },
    {
      "epoch": 1.2893350420048413,
      "grad_norm": 0.6330561637878418,
      "learning_rate": 0.00011412887601500547,
      "loss": 0.6168,
      "step": 90550
    },
    {
      "epoch": 1.2900469884664674,
      "grad_norm": 0.5025748014450073,
      "learning_rate": 0.00011408139037941023,
      "loss": 0.6436,
      "step": 90600
    },
    {
      "epoch": 1.2907589349280935,
      "grad_norm": 0.44092315435409546,
      "learning_rate": 0.000114033904743815,
      "loss": 0.6252,
      "step": 90650
    },
    {
      "epoch": 1.2914708813897195,
      "grad_norm": 0.5391337275505066,
      "learning_rate": 0.00011398641910821977,
      "loss": 0.5863,
      "step": 90700
    },
    {
      "epoch": 1.2921828278513456,
      "grad_norm": 0.4642472565174103,
      "learning_rate": 0.00011393893347262455,
      "loss": 0.6209,
      "step": 90750
    },
    {
      "epoch": 1.2928947743129717,
      "grad_norm": 0.39299339056015015,
      "learning_rate": 0.0001138914478370293,
      "loss": 0.6716,
      "step": 90800
    },
    {
      "epoch": 1.2936067207745978,
      "grad_norm": 0.4680125415325165,
      "learning_rate": 0.00011384396220143408,
      "loss": 0.6422,
      "step": 90850
    },
    {
      "epoch": 1.294318667236224,
      "grad_norm": 0.6457155346870422,
      "learning_rate": 0.00011379647656583885,
      "loss": 0.6047,
      "step": 90900
    },
    {
      "epoch": 1.29503061369785,
      "grad_norm": 0.6157360672950745,
      "learning_rate": 0.0001137489909302436,
      "loss": 0.6959,
      "step": 90950
    },
    {
      "epoch": 1.295742560159476,
      "grad_norm": 0.7174908518791199,
      "learning_rate": 0.00011370150529464838,
      "loss": 0.5886,
      "step": 91000
    },
    {
      "epoch": 1.2964545066211022,
      "grad_norm": 0.6094834208488464,
      "learning_rate": 0.00011365401965905313,
      "loss": 0.6229,
      "step": 91050
    },
    {
      "epoch": 1.297166453082728,
      "grad_norm": 0.5511013269424438,
      "learning_rate": 0.00011360653402345791,
      "loss": 0.6691,
      "step": 91100
    },
    {
      "epoch": 1.2978783995443544,
      "grad_norm": 0.49106600880622864,
      "learning_rate": 0.00011355904838786269,
      "loss": 0.6622,
      "step": 91150
    },
    {
      "epoch": 1.2985903460059802,
      "grad_norm": 0.48274993896484375,
      "learning_rate": 0.00011351156275226744,
      "loss": 0.6457,
      "step": 91200
    },
    {
      "epoch": 1.2993022924676065,
      "grad_norm": 0.5509037375450134,
      "learning_rate": 0.00011346407711667222,
      "loss": 0.67,
      "step": 91250
    },
    {
      "epoch": 1.3000142389292324,
      "grad_norm": 0.5222060680389404,
      "learning_rate": 0.00011341659148107697,
      "loss": 0.5911,
      "step": 91300
    },
    {
      "epoch": 1.3007261853908587,
      "grad_norm": 0.6318575143814087,
      "learning_rate": 0.00011336910584548174,
      "loss": 0.6186,
      "step": 91350
    },
    {
      "epoch": 1.3014381318524846,
      "grad_norm": 0.7290272116661072,
      "learning_rate": 0.00011332162020988652,
      "loss": 0.6162,
      "step": 91400
    },
    {
      "epoch": 1.302150078314111,
      "grad_norm": 0.5040566325187683,
      "learning_rate": 0.00011327413457429129,
      "loss": 0.6036,
      "step": 91450
    },
    {
      "epoch": 1.3028620247757368,
      "grad_norm": 0.4001622498035431,
      "learning_rate": 0.00011322664893869605,
      "loss": 0.643,
      "step": 91500
    },
    {
      "epoch": 1.303573971237363,
      "grad_norm": 0.8004602789878845,
      "learning_rate": 0.00011317916330310081,
      "loss": 0.631,
      "step": 91550
    },
    {
      "epoch": 1.304285917698989,
      "grad_norm": 0.7976336479187012,
      "learning_rate": 0.00011313167766750559,
      "loss": 0.649,
      "step": 91600
    },
    {
      "epoch": 1.3049978641606153,
      "grad_norm": 0.5826192498207092,
      "learning_rate": 0.00011308419203191037,
      "loss": 0.6384,
      "step": 91650
    },
    {
      "epoch": 1.3057098106222411,
      "grad_norm": 0.6517210006713867,
      "learning_rate": 0.00011303670639631512,
      "loss": 0.6236,
      "step": 91700
    },
    {
      "epoch": 1.3064217570838674,
      "grad_norm": 0.35307180881500244,
      "learning_rate": 0.0001129892207607199,
      "loss": 0.6971,
      "step": 91750
    },
    {
      "epoch": 1.3071337035454933,
      "grad_norm": 0.5415704846382141,
      "learning_rate": 0.00011294173512512465,
      "loss": 0.665,
      "step": 91800
    },
    {
      "epoch": 1.3078456500071194,
      "grad_norm": 0.6040716767311096,
      "learning_rate": 0.00011289424948952943,
      "loss": 0.6577,
      "step": 91850
    },
    {
      "epoch": 1.3085575964687455,
      "grad_norm": 0.49761882424354553,
      "learning_rate": 0.0001128467638539342,
      "loss": 0.6385,
      "step": 91900
    },
    {
      "epoch": 1.3092695429303716,
      "grad_norm": 0.5506663918495178,
      "learning_rate": 0.00011279927821833895,
      "loss": 0.6735,
      "step": 91950
    },
    {
      "epoch": 1.3099814893919977,
      "grad_norm": 0.42244523763656616,
      "learning_rate": 0.00011275179258274373,
      "loss": 0.5995,
      "step": 92000
    },
    {
      "epoch": 1.3106934358536237,
      "grad_norm": 0.4542563259601593,
      "learning_rate": 0.00011270430694714848,
      "loss": 0.6535,
      "step": 92050
    },
    {
      "epoch": 1.3114053823152498,
      "grad_norm": 0.758419394493103,
      "learning_rate": 0.00011265682131155326,
      "loss": 0.6576,
      "step": 92100
    },
    {
      "epoch": 1.312117328776876,
      "grad_norm": 0.8352330923080444,
      "learning_rate": 0.00011260933567595804,
      "loss": 0.6533,
      "step": 92150
    },
    {
      "epoch": 1.312829275238502,
      "grad_norm": 0.6957542300224304,
      "learning_rate": 0.00011256185004036279,
      "loss": 0.6188,
      "step": 92200
    },
    {
      "epoch": 1.313541221700128,
      "grad_norm": 0.3895805776119232,
      "learning_rate": 0.00011251436440476756,
      "loss": 0.6095,
      "step": 92250
    },
    {
      "epoch": 1.3142531681617542,
      "grad_norm": 1.0378891229629517,
      "learning_rate": 0.00011246687876917233,
      "loss": 0.6352,
      "step": 92300
    },
    {
      "epoch": 1.3149651146233803,
      "grad_norm": 0.5042335987091064,
      "learning_rate": 0.0001124193931335771,
      "loss": 0.6427,
      "step": 92350
    },
    {
      "epoch": 1.3156770610850064,
      "grad_norm": 0.697027325630188,
      "learning_rate": 0.00011237190749798187,
      "loss": 0.5811,
      "step": 92400
    },
    {
      "epoch": 1.3163890075466325,
      "grad_norm": 0.46961888670921326,
      "learning_rate": 0.00011232442186238663,
      "loss": 0.6242,
      "step": 92450
    },
    {
      "epoch": 1.3171009540082586,
      "grad_norm": 0.561093270778656,
      "learning_rate": 0.00011227693622679141,
      "loss": 0.6695,
      "step": 92500
    },
    {
      "epoch": 1.3178129004698846,
      "grad_norm": 0.4010832905769348,
      "learning_rate": 0.00011222945059119616,
      "loss": 0.6524,
      "step": 92550
    },
    {
      "epoch": 1.3185248469315107,
      "grad_norm": 0.40444135665893555,
      "learning_rate": 0.00011218196495560094,
      "loss": 0.5822,
      "step": 92600
    },
    {
      "epoch": 1.3192367933931368,
      "grad_norm": 0.5363429188728333,
      "learning_rate": 0.00011213447932000572,
      "loss": 0.7005,
      "step": 92650
    },
    {
      "epoch": 1.319948739854763,
      "grad_norm": 0.636497437953949,
      "learning_rate": 0.00011208699368441047,
      "loss": 0.6566,
      "step": 92700
    },
    {
      "epoch": 1.320660686316389,
      "grad_norm": 0.6191405653953552,
      "learning_rate": 0.00011203950804881525,
      "loss": 0.5662,
      "step": 92750
    },
    {
      "epoch": 1.321372632778015,
      "grad_norm": 0.9718762636184692,
      "learning_rate": 0.00011199202241322,
      "loss": 0.6651,
      "step": 92800
    },
    {
      "epoch": 1.3220845792396412,
      "grad_norm": 0.3321167826652527,
      "learning_rate": 0.00011194548649033668,
      "loss": 0.6208,
      "step": 92850
    },
    {
      "epoch": 1.3227965257012673,
      "grad_norm": 0.5131735801696777,
      "learning_rate": 0.00011189800085474145,
      "loss": 0.6471,
      "step": 92900
    },
    {
      "epoch": 1.3235084721628934,
      "grad_norm": 0.7076888680458069,
      "learning_rate": 0.00011185051521914622,
      "loss": 0.6331,
      "step": 92950
    },
    {
      "epoch": 1.3242204186245194,
      "grad_norm": 0.4866503179073334,
      "learning_rate": 0.00011180302958355098,
      "loss": 0.6715,
      "step": 93000
    },
    {
      "epoch": 1.3249323650861455,
      "grad_norm": 0.40437647700309753,
      "learning_rate": 0.00011175554394795576,
      "loss": 0.6268,
      "step": 93050
    },
    {
      "epoch": 1.3256443115477716,
      "grad_norm": 0.5919627547264099,
      "learning_rate": 0.00011170805831236051,
      "loss": 0.6914,
      "step": 93100
    },
    {
      "epoch": 1.3263562580093977,
      "grad_norm": 0.9410006999969482,
      "learning_rate": 0.00011166057267676529,
      "loss": 0.5658,
      "step": 93150
    },
    {
      "epoch": 1.3270682044710238,
      "grad_norm": 0.6247655749320984,
      "learning_rate": 0.00011161308704117007,
      "loss": 0.6538,
      "step": 93200
    },
    {
      "epoch": 1.32778015093265,
      "grad_norm": 0.48047006130218506,
      "learning_rate": 0.00011156560140557482,
      "loss": 0.6978,
      "step": 93250
    },
    {
      "epoch": 1.328492097394276,
      "grad_norm": 0.6146837472915649,
      "learning_rate": 0.00011151811576997959,
      "loss": 0.6652,
      "step": 93300
    },
    {
      "epoch": 1.329204043855902,
      "grad_norm": 0.8719491958618164,
      "learning_rate": 0.00011147063013438434,
      "loss": 0.6381,
      "step": 93350
    },
    {
      "epoch": 1.3299159903175282,
      "grad_norm": 0.35967957973480225,
      "learning_rate": 0.00011142314449878912,
      "loss": 0.6118,
      "step": 93400
    },
    {
      "epoch": 1.3306279367791543,
      "grad_norm": 0.4126757085323334,
      "learning_rate": 0.0001113756588631939,
      "loss": 0.6571,
      "step": 93450
    },
    {
      "epoch": 1.3313398832407803,
      "grad_norm": 0.6796298623085022,
      "learning_rate": 0.00011132817322759865,
      "loss": 0.6362,
      "step": 93500
    },
    {
      "epoch": 1.3320518297024064,
      "grad_norm": 0.3943176865577698,
      "learning_rate": 0.00011128068759200343,
      "loss": 0.6608,
      "step": 93550
    },
    {
      "epoch": 1.3327637761640325,
      "grad_norm": 0.5123392939567566,
      "learning_rate": 0.00011123320195640818,
      "loss": 0.6465,
      "step": 93600
    },
    {
      "epoch": 1.3334757226256586,
      "grad_norm": 0.4541686773300171,
      "learning_rate": 0.00011118571632081296,
      "loss": 0.6902,
      "step": 93650
    },
    {
      "epoch": 1.3341876690872847,
      "grad_norm": 0.7156972885131836,
      "learning_rate": 0.00011113823068521773,
      "loss": 0.613,
      "step": 93700
    },
    {
      "epoch": 1.3348996155489108,
      "grad_norm": 0.3070398271083832,
      "learning_rate": 0.0001110907450496225,
      "loss": 0.6087,
      "step": 93750
    },
    {
      "epoch": 1.3356115620105369,
      "grad_norm": 0.4868829846382141,
      "learning_rate": 0.00011104325941402726,
      "loss": 0.6719,
      "step": 93800
    },
    {
      "epoch": 1.336323508472163,
      "grad_norm": 0.5135727524757385,
      "learning_rate": 0.00011099577377843202,
      "loss": 0.6596,
      "step": 93850
    },
    {
      "epoch": 1.337035454933789,
      "grad_norm": 0.7130923271179199,
      "learning_rate": 0.0001109482881428368,
      "loss": 0.6051,
      "step": 93900
    },
    {
      "epoch": 1.3377474013954151,
      "grad_norm": 0.745215117931366,
      "learning_rate": 0.00011090080250724158,
      "loss": 0.6013,
      "step": 93950
    },
    {
      "epoch": 1.3384593478570412,
      "grad_norm": 0.4521245062351227,
      "learning_rate": 0.00011085331687164633,
      "loss": 0.629,
      "step": 94000
    },
    {
      "epoch": 1.339171294318667,
      "grad_norm": 0.5617049932479858,
      "learning_rate": 0.00011080583123605111,
      "loss": 0.6213,
      "step": 94050
    },
    {
      "epoch": 1.3398832407802934,
      "grad_norm": 0.7271102070808411,
      "learning_rate": 0.00011075834560045586,
      "loss": 0.7084,
      "step": 94100
    },
    {
      "epoch": 1.3405951872419193,
      "grad_norm": 0.42062583565711975,
      "learning_rate": 0.00011071085996486064,
      "loss": 0.7036,
      "step": 94150
    },
    {
      "epoch": 1.3413071337035456,
      "grad_norm": 0.5230308771133423,
      "learning_rate": 0.00011066337432926541,
      "loss": 0.627,
      "step": 94200
    },
    {
      "epoch": 1.3420190801651715,
      "grad_norm": 0.734070360660553,
      "learning_rate": 0.00011061588869367016,
      "loss": 0.6484,
      "step": 94250
    },
    {
      "epoch": 1.3427310266267978,
      "grad_norm": 0.5561816096305847,
      "learning_rate": 0.00011056840305807494,
      "loss": 0.6204,
      "step": 94300
    },
    {
      "epoch": 1.3434429730884236,
      "grad_norm": 0.7798705697059631,
      "learning_rate": 0.00011052091742247969,
      "loss": 0.6833,
      "step": 94350
    },
    {
      "epoch": 1.34415491955005,
      "grad_norm": 0.43729880452156067,
      "learning_rate": 0.00011047343178688447,
      "loss": 0.6205,
      "step": 94400
    },
    {
      "epoch": 1.3448668660116758,
      "grad_norm": 0.5674827694892883,
      "learning_rate": 0.00011042594615128925,
      "loss": 0.6497,
      "step": 94450
    },
    {
      "epoch": 1.3455788124733021,
      "grad_norm": 0.535245418548584,
      "learning_rate": 0.000110378460515694,
      "loss": 0.6154,
      "step": 94500
    },
    {
      "epoch": 1.346290758934928,
      "grad_norm": 0.4795893728733063,
      "learning_rate": 0.00011033097488009878,
      "loss": 0.6675,
      "step": 94550
    },
    {
      "epoch": 1.3470027053965543,
      "grad_norm": 0.33655574917793274,
      "learning_rate": 0.00011028348924450354,
      "loss": 0.6751,
      "step": 94600
    },
    {
      "epoch": 1.3477146518581802,
      "grad_norm": 0.5249792337417603,
      "learning_rate": 0.00011023600360890832,
      "loss": 0.6709,
      "step": 94650
    },
    {
      "epoch": 1.3484265983198063,
      "grad_norm": 0.5039539933204651,
      "learning_rate": 0.00011018851797331308,
      "loss": 0.6702,
      "step": 94700
    },
    {
      "epoch": 1.3491385447814324,
      "grad_norm": 0.5370394587516785,
      "learning_rate": 0.00011014103233771785,
      "loss": 0.5932,
      "step": 94750
    },
    {
      "epoch": 1.3498504912430584,
      "grad_norm": 0.40043047070503235,
      "learning_rate": 0.00011009354670212262,
      "loss": 0.6278,
      "step": 94800
    },
    {
      "epoch": 1.3505624377046845,
      "grad_norm": 0.5716332197189331,
      "learning_rate": 0.00011004606106652737,
      "loss": 0.6234,
      "step": 94850
    },
    {
      "epoch": 1.3512743841663106,
      "grad_norm": 0.8620274066925049,
      "learning_rate": 0.00010999857543093215,
      "loss": 0.6942,
      "step": 94900
    },
    {
      "epoch": 1.3519863306279367,
      "grad_norm": 0.4323340952396393,
      "learning_rate": 0.00010995108979533693,
      "loss": 0.6199,
      "step": 94950
    },
    {
      "epoch": 1.3526982770895628,
      "grad_norm": 0.5147616267204285,
      "learning_rate": 0.00010990360415974168,
      "loss": 0.661,
      "step": 95000
    },
    {
      "epoch": 1.353410223551189,
      "grad_norm": 0.5327372550964355,
      "learning_rate": 0.00010985611852414646,
      "loss": 0.5791,
      "step": 95050
    },
    {
      "epoch": 1.354122170012815,
      "grad_norm": 0.7084580063819885,
      "learning_rate": 0.00010980863288855121,
      "loss": 0.6603,
      "step": 95100
    },
    {
      "epoch": 1.354834116474441,
      "grad_norm": 0.7049655318260193,
      "learning_rate": 0.00010976114725295599,
      "loss": 0.616,
      "step": 95150
    },
    {
      "epoch": 1.3555460629360672,
      "grad_norm": 0.46148228645324707,
      "learning_rate": 0.00010971366161736076,
      "loss": 0.6169,
      "step": 95200
    },
    {
      "epoch": 1.3562580093976933,
      "grad_norm": 0.6106599569320679,
      "learning_rate": 0.00010966617598176551,
      "loss": 0.5934,
      "step": 95250
    },
    {
      "epoch": 1.3569699558593193,
      "grad_norm": 0.6698822379112244,
      "learning_rate": 0.00010961869034617029,
      "loss": 0.6157,
      "step": 95300
    },
    {
      "epoch": 1.3576819023209454,
      "grad_norm": 0.37779107689857483,
      "learning_rate": 0.00010957120471057505,
      "loss": 0.5923,
      "step": 95350
    },
    {
      "epoch": 1.3583938487825715,
      "grad_norm": 0.6711459159851074,
      "learning_rate": 0.00010952371907497982,
      "loss": 0.659,
      "step": 95400
    },
    {
      "epoch": 1.3591057952441976,
      "grad_norm": 0.44501495361328125,
      "learning_rate": 0.0001094762334393846,
      "loss": 0.6305,
      "step": 95450
    },
    {
      "epoch": 1.3598177417058237,
      "grad_norm": 0.7364882826805115,
      "learning_rate": 0.00010942874780378936,
      "loss": 0.6549,
      "step": 95500
    },
    {
      "epoch": 1.3605296881674498,
      "grad_norm": 0.6602118611335754,
      "learning_rate": 0.00010938221188090603,
      "loss": 0.6476,
      "step": 95550
    },
    {
      "epoch": 1.3612416346290759,
      "grad_norm": 0.5088481903076172,
      "learning_rate": 0.0001093347262453108,
      "loss": 0.688,
      "step": 95600
    },
    {
      "epoch": 1.361953581090702,
      "grad_norm": 0.5468237996101379,
      "learning_rate": 0.00010928724060971555,
      "loss": 0.7005,
      "step": 95650
    },
    {
      "epoch": 1.362665527552328,
      "grad_norm": 0.35387587547302246,
      "learning_rate": 0.00010923975497412033,
      "loss": 0.6837,
      "step": 95700
    },
    {
      "epoch": 1.3633774740139541,
      "grad_norm": 0.5533866286277771,
      "learning_rate": 0.00010919226933852511,
      "loss": 0.6186,
      "step": 95750
    },
    {
      "epoch": 1.3640894204755802,
      "grad_norm": 0.5704946517944336,
      "learning_rate": 0.00010914478370292986,
      "loss": 0.7637,
      "step": 95800
    },
    {
      "epoch": 1.3648013669372063,
      "grad_norm": 0.33363255858421326,
      "learning_rate": 0.00010909729806733464,
      "loss": 0.6074,
      "step": 95850
    },
    {
      "epoch": 1.3655133133988324,
      "grad_norm": 0.47562846541404724,
      "learning_rate": 0.0001090498124317394,
      "loss": 0.6078,
      "step": 95900
    },
    {
      "epoch": 1.3662252598604585,
      "grad_norm": 0.34706804156303406,
      "learning_rate": 0.00010900232679614417,
      "loss": 0.6741,
      "step": 95950
    },
    {
      "epoch": 1.3669372063220846,
      "grad_norm": 0.6792423725128174,
      "learning_rate": 0.00010895484116054894,
      "loss": 0.6724,
      "step": 96000
    },
    {
      "epoch": 1.3676491527837107,
      "grad_norm": 0.6879494190216064,
      "learning_rate": 0.00010890735552495371,
      "loss": 0.6495,
      "step": 96050
    },
    {
      "epoch": 1.3683610992453368,
      "grad_norm": 0.5488868355751038,
      "learning_rate": 0.00010885986988935849,
      "loss": 0.6797,
      "step": 96100
    },
    {
      "epoch": 1.3690730457069629,
      "grad_norm": 0.694107711315155,
      "learning_rate": 0.00010881238425376324,
      "loss": 0.5874,
      "step": 96150
    },
    {
      "epoch": 1.369784992168589,
      "grad_norm": 0.5304413437843323,
      "learning_rate": 0.00010876489861816801,
      "loss": 0.7019,
      "step": 96200
    },
    {
      "epoch": 1.370496938630215,
      "grad_norm": 0.5702975392341614,
      "learning_rate": 0.00010871741298257279,
      "loss": 0.6073,
      "step": 96250
    },
    {
      "epoch": 1.3712088850918411,
      "grad_norm": 0.9872485399246216,
      "learning_rate": 0.00010866992734697754,
      "loss": 0.6886,
      "step": 96300
    },
    {
      "epoch": 1.3719208315534672,
      "grad_norm": 0.548436164855957,
      "learning_rate": 0.00010862244171138232,
      "loss": 0.6213,
      "step": 96350
    },
    {
      "epoch": 1.3726327780150933,
      "grad_norm": 0.5718751549720764,
      "learning_rate": 0.00010857495607578707,
      "loss": 0.6506,
      "step": 96400
    },
    {
      "epoch": 1.3733447244767194,
      "grad_norm": 0.5382453203201294,
      "learning_rate": 0.00010852842015290375,
      "loss": 0.5845,
      "step": 96450
    },
    {
      "epoch": 1.3740566709383455,
      "grad_norm": 0.5340994000434875,
      "learning_rate": 0.00010848093451730851,
      "loss": 0.6305,
      "step": 96500
    },
    {
      "epoch": 1.3747686173999716,
      "grad_norm": 0.31942999362945557,
      "learning_rate": 0.00010843344888171329,
      "loss": 0.6311,
      "step": 96550
    },
    {
      "epoch": 1.3754805638615977,
      "grad_norm": 0.5068691968917847,
      "learning_rate": 0.00010838596324611805,
      "loss": 0.6653,
      "step": 96600
    },
    {
      "epoch": 1.3761925103232238,
      "grad_norm": 0.848801851272583,
      "learning_rate": 0.00010833847761052283,
      "loss": 0.6236,
      "step": 96650
    },
    {
      "epoch": 1.3769044567848499,
      "grad_norm": 0.6008706092834473,
      "learning_rate": 0.00010829099197492758,
      "loss": 0.6423,
      "step": 96700
    },
    {
      "epoch": 1.377616403246476,
      "grad_norm": 0.9070008397102356,
      "learning_rate": 0.00010824350633933236,
      "loss": 0.6584,
      "step": 96750
    },
    {
      "epoch": 1.378328349708102,
      "grad_norm": 0.43885621428489685,
      "learning_rate": 0.00010819602070373714,
      "loss": 0.6511,
      "step": 96800
    },
    {
      "epoch": 1.3790402961697281,
      "grad_norm": 0.47485294938087463,
      "learning_rate": 0.00010814853506814189,
      "loss": 0.6542,
      "step": 96850
    },
    {
      "epoch": 1.379752242631354,
      "grad_norm": 0.7213519811630249,
      "learning_rate": 0.00010810104943254667,
      "loss": 0.6757,
      "step": 96900
    },
    {
      "epoch": 1.3804641890929803,
      "grad_norm": 1.177606225013733,
      "learning_rate": 0.00010805356379695142,
      "loss": 0.6135,
      "step": 96950
    },
    {
      "epoch": 1.3811761355546062,
      "grad_norm": 0.5473514199256897,
      "learning_rate": 0.0001080060781613562,
      "loss": 0.6249,
      "step": 97000
    },
    {
      "epoch": 1.3818880820162325,
      "grad_norm": 0.3245040476322174,
      "learning_rate": 0.00010795859252576097,
      "loss": 0.6458,
      "step": 97050
    },
    {
      "epoch": 1.3826000284778583,
      "grad_norm": 0.7535378932952881,
      "learning_rate": 0.00010791110689016572,
      "loss": 0.7173,
      "step": 97100
    },
    {
      "epoch": 1.3833119749394847,
      "grad_norm": 0.4211274981498718,
      "learning_rate": 0.0001078636212545705,
      "loss": 0.5707,
      "step": 97150
    },
    {
      "epoch": 1.3840239214011105,
      "grad_norm": 0.9063445925712585,
      "learning_rate": 0.00010781613561897525,
      "loss": 0.6244,
      "step": 97200
    },
    {
      "epoch": 1.3847358678627368,
      "grad_norm": 1.1219756603240967,
      "learning_rate": 0.00010776864998338003,
      "loss": 0.6297,
      "step": 97250
    },
    {
      "epoch": 1.3854478143243627,
      "grad_norm": 0.5665091872215271,
      "learning_rate": 0.0001077211643477848,
      "loss": 0.71,
      "step": 97300
    },
    {
      "epoch": 1.386159760785989,
      "grad_norm": 0.37601107358932495,
      "learning_rate": 0.00010767367871218957,
      "loss": 0.6725,
      "step": 97350
    },
    {
      "epoch": 1.3868717072476149,
      "grad_norm": 0.6018768548965454,
      "learning_rate": 0.00010762619307659433,
      "loss": 0.6459,
      "step": 97400
    },
    {
      "epoch": 1.3875836537092412,
      "grad_norm": 0.5923898220062256,
      "learning_rate": 0.0001075787074409991,
      "loss": 0.6509,
      "step": 97450
    },
    {
      "epoch": 1.388295600170867,
      "grad_norm": 0.5069190263748169,
      "learning_rate": 0.00010753122180540388,
      "loss": 0.6485,
      "step": 97500
    },
    {
      "epoch": 1.3890075466324934,
      "grad_norm": 0.4669434428215027,
      "learning_rate": 0.00010748373616980865,
      "loss": 0.6421,
      "step": 97550
    },
    {
      "epoch": 1.3897194930941192,
      "grad_norm": 0.5612226724624634,
      "learning_rate": 0.0001074362505342134,
      "loss": 0.643,
      "step": 97600
    },
    {
      "epoch": 1.3904314395557453,
      "grad_norm": 0.6301531195640564,
      "learning_rate": 0.00010738876489861818,
      "loss": 0.6093,
      "step": 97650
    },
    {
      "epoch": 1.3911433860173714,
      "grad_norm": 0.7796798944473267,
      "learning_rate": 0.00010734127926302293,
      "loss": 0.65,
      "step": 97700
    },
    {
      "epoch": 1.3918553324789975,
      "grad_norm": 0.8723565936088562,
      "learning_rate": 0.00010729379362742771,
      "loss": 0.6426,
      "step": 97750
    },
    {
      "epoch": 1.3925672789406236,
      "grad_norm": 0.44933590292930603,
      "learning_rate": 0.00010724630799183249,
      "loss": 0.5868,
      "step": 97800
    },
    {
      "epoch": 1.3932792254022497,
      "grad_norm": 0.4479089379310608,
      "learning_rate": 0.00010719882235623724,
      "loss": 0.6818,
      "step": 97850
    },
    {
      "epoch": 1.3939911718638758,
      "grad_norm": 0.7132862210273743,
      "learning_rate": 0.00010715133672064201,
      "loss": 0.6471,
      "step": 97900
    },
    {
      "epoch": 1.3947031183255019,
      "grad_norm": 0.6747190356254578,
      "learning_rate": 0.00010710385108504677,
      "loss": 0.6479,
      "step": 97950
    },
    {
      "epoch": 1.395415064787128,
      "grad_norm": 0.5979632139205933,
      "learning_rate": 0.00010705636544945154,
      "loss": 0.6337,
      "step": 98000
    },
    {
      "epoch": 1.396127011248754,
      "grad_norm": 0.6525061726570129,
      "learning_rate": 0.00010700887981385632,
      "loss": 0.6293,
      "step": 98050
    },
    {
      "epoch": 1.3968389577103801,
      "grad_norm": 0.6088407039642334,
      "learning_rate": 0.00010696139417826107,
      "loss": 0.5934,
      "step": 98100
    },
    {
      "epoch": 1.3975509041720062,
      "grad_norm": 0.30562150478363037,
      "learning_rate": 0.00010691390854266585,
      "loss": 0.6115,
      "step": 98150
    },
    {
      "epoch": 1.3982628506336323,
      "grad_norm": 0.3706558346748352,
      "learning_rate": 0.00010686642290707061,
      "loss": 0.613,
      "step": 98200
    },
    {
      "epoch": 1.3989747970952584,
      "grad_norm": 0.5562979578971863,
      "learning_rate": 0.00010681893727147538,
      "loss": 0.6782,
      "step": 98250
    },
    {
      "epoch": 1.3996867435568845,
      "grad_norm": 0.44859543442726135,
      "learning_rate": 0.00010677145163588015,
      "loss": 0.5439,
      "step": 98300
    },
    {
      "epoch": 1.4003986900185106,
      "grad_norm": 0.4991416335105896,
      "learning_rate": 0.00010672396600028492,
      "loss": 0.6002,
      "step": 98350
    },
    {
      "epoch": 1.4011106364801367,
      "grad_norm": 0.39261528849601746,
      "learning_rate": 0.0001066764803646897,
      "loss": 0.5916,
      "step": 98400
    },
    {
      "epoch": 1.4018225829417628,
      "grad_norm": 0.3785182237625122,
      "learning_rate": 0.00010662899472909445,
      "loss": 0.6393,
      "step": 98450
    },
    {
      "epoch": 1.4025345294033889,
      "grad_norm": 0.39622947573661804,
      "learning_rate": 0.00010658150909349922,
      "loss": 0.6157,
      "step": 98500
    },
    {
      "epoch": 1.403246475865015,
      "grad_norm": 0.5196048021316528,
      "learning_rate": 0.000106534023457904,
      "loss": 0.5608,
      "step": 98550
    },
    {
      "epoch": 1.403958422326641,
      "grad_norm": 0.5469505190849304,
      "learning_rate": 0.00010648653782230875,
      "loss": 0.6703,
      "step": 98600
    },
    {
      "epoch": 1.4046703687882671,
      "grad_norm": 0.8071326613426208,
      "learning_rate": 0.00010643905218671353,
      "loss": 0.6553,
      "step": 98650
    },
    {
      "epoch": 1.4053823152498932,
      "grad_norm": 0.35755687952041626,
      "learning_rate": 0.00010639156655111828,
      "loss": 0.6345,
      "step": 98700
    },
    {
      "epoch": 1.4060942617115193,
      "grad_norm": 0.5736192464828491,
      "learning_rate": 0.00010634408091552306,
      "loss": 0.6601,
      "step": 98750
    },
    {
      "epoch": 1.4068062081731454,
      "grad_norm": 0.5216793417930603,
      "learning_rate": 0.00010629659527992784,
      "loss": 0.6563,
      "step": 98800
    },
    {
      "epoch": 1.4075181546347715,
      "grad_norm": 0.8426513075828552,
      "learning_rate": 0.00010624910964433259,
      "loss": 0.6294,
      "step": 98850
    },
    {
      "epoch": 1.4082301010963976,
      "grad_norm": 0.4802648723125458,
      "learning_rate": 0.00010620162400873736,
      "loss": 0.6494,
      "step": 98900
    },
    {
      "epoch": 1.4089420475580237,
      "grad_norm": 0.7355090975761414,
      "learning_rate": 0.00010615413837314211,
      "loss": 0.6373,
      "step": 98950
    },
    {
      "epoch": 1.4096539940196497,
      "grad_norm": 0.3931352496147156,
      "learning_rate": 0.00010610665273754689,
      "loss": 0.6298,
      "step": 99000
    },
    {
      "epoch": 1.4103659404812758,
      "grad_norm": 0.8096128106117249,
      "learning_rate": 0.00010605916710195167,
      "loss": 0.6006,
      "step": 99050
    },
    {
      "epoch": 1.411077886942902,
      "grad_norm": 0.40369948744773865,
      "learning_rate": 0.00010601168146635643,
      "loss": 0.5708,
      "step": 99100
    },
    {
      "epoch": 1.411789833404528,
      "grad_norm": 0.500850260257721,
      "learning_rate": 0.0001059641958307612,
      "loss": 0.5779,
      "step": 99150
    },
    {
      "epoch": 1.412501779866154,
      "grad_norm": 0.49368372559547424,
      "learning_rate": 0.00010591671019516596,
      "loss": 0.6082,
      "step": 99200
    },
    {
      "epoch": 1.4132137263277802,
      "grad_norm": 0.5650104284286499,
      "learning_rate": 0.00010586922455957074,
      "loss": 0.6429,
      "step": 99250
    },
    {
      "epoch": 1.4139256727894063,
      "grad_norm": 0.31970950961112976,
      "learning_rate": 0.00010582173892397552,
      "loss": 0.6353,
      "step": 99300
    },
    {
      "epoch": 1.4146376192510324,
      "grad_norm": 0.6587854027748108,
      "learning_rate": 0.00010577425328838027,
      "loss": 0.7152,
      "step": 99350
    },
    {
      "epoch": 1.4153495657126585,
      "grad_norm": 0.709313154220581,
      "learning_rate": 0.00010572676765278505,
      "loss": 0.6888,
      "step": 99400
    },
    {
      "epoch": 1.4160615121742846,
      "grad_norm": 0.5335348844528198,
      "learning_rate": 0.0001056792820171898,
      "loss": 0.684,
      "step": 99450
    },
    {
      "epoch": 1.4167734586359106,
      "grad_norm": 0.5235198140144348,
      "learning_rate": 0.00010563179638159457,
      "loss": 0.5962,
      "step": 99500
    },
    {
      "epoch": 1.4174854050975367,
      "grad_norm": 0.686234176158905,
      "learning_rate": 0.00010558431074599935,
      "loss": 0.6468,
      "step": 99550
    },
    {
      "epoch": 1.4181973515591628,
      "grad_norm": 0.49651214480400085,
      "learning_rate": 0.0001055368251104041,
      "loss": 0.6146,
      "step": 99600
    },
    {
      "epoch": 1.418909298020789,
      "grad_norm": 0.2978188991546631,
      "learning_rate": 0.00010548933947480888,
      "loss": 0.6459,
      "step": 99650
    },
    {
      "epoch": 1.419621244482415,
      "grad_norm": 0.47125834226608276,
      "learning_rate": 0.00010544185383921363,
      "loss": 0.6436,
      "step": 99700
    },
    {
      "epoch": 1.420333190944041,
      "grad_norm": 0.707500159740448,
      "learning_rate": 0.00010539436820361841,
      "loss": 0.6804,
      "step": 99750
    },
    {
      "epoch": 1.4210451374056672,
      "grad_norm": 0.5413402318954468,
      "learning_rate": 0.00010534688256802318,
      "loss": 0.6813,
      "step": 99800
    },
    {
      "epoch": 1.421757083867293,
      "grad_norm": 0.514997124671936,
      "learning_rate": 0.00010529939693242794,
      "loss": 0.6857,
      "step": 99850
    },
    {
      "epoch": 1.4224690303289194,
      "grad_norm": 0.6765477657318115,
      "learning_rate": 0.00010525191129683271,
      "loss": 0.6083,
      "step": 99900
    },
    {
      "epoch": 1.4231809767905452,
      "grad_norm": 0.498853474855423,
      "learning_rate": 0.00010520442566123748,
      "loss": 0.6554,
      "step": 99950
    },
    {
      "epoch": 1.4238929232521715,
      "grad_norm": 0.436470627784729,
      "learning_rate": 0.00010515694002564225,
      "loss": 0.6824,
      "step": 100000
    },
    {
      "epoch": 1.4246048697137974,
      "grad_norm": 0.47672662138938904,
      "learning_rate": 0.00010510945439004702,
      "loss": 0.6403,
      "step": 100050
    },
    {
      "epoch": 1.4253168161754237,
      "grad_norm": 0.6668485999107361,
      "learning_rate": 0.00010506196875445178,
      "loss": 0.6489,
      "step": 100100
    },
    {
      "epoch": 1.4260287626370496,
      "grad_norm": 0.6781304478645325,
      "learning_rate": 0.00010501448311885656,
      "loss": 0.5998,
      "step": 100150
    },
    {
      "epoch": 1.426740709098676,
      "grad_norm": 0.42612287402153015,
      "learning_rate": 0.00010496699748326131,
      "loss": 0.639,
      "step": 100200
    },
    {
      "epoch": 1.4274526555603018,
      "grad_norm": 0.4116414785385132,
      "learning_rate": 0.00010491951184766609,
      "loss": 0.6223,
      "step": 100250
    },
    {
      "epoch": 1.428164602021928,
      "grad_norm": 0.6503821015357971,
      "learning_rate": 0.00010487202621207087,
      "loss": 0.5927,
      "step": 100300
    },
    {
      "epoch": 1.428876548483554,
      "grad_norm": 0.7729104161262512,
      "learning_rate": 0.00010482454057647562,
      "loss": 0.6698,
      "step": 100350
    },
    {
      "epoch": 1.4295884949451803,
      "grad_norm": 0.5770069360733032,
      "learning_rate": 0.0001047770549408804,
      "loss": 0.5808,
      "step": 100400
    },
    {
      "epoch": 1.4303004414068061,
      "grad_norm": 0.4248085021972656,
      "learning_rate": 0.00010472956930528514,
      "loss": 0.6011,
      "step": 100450
    },
    {
      "epoch": 1.4310123878684322,
      "grad_norm": 0.535477876663208,
      "learning_rate": 0.00010468208366968992,
      "loss": 0.6513,
      "step": 100500
    },
    {
      "epoch": 1.4317243343300583,
      "grad_norm": 0.6481120586395264,
      "learning_rate": 0.0001046345980340947,
      "loss": 0.6196,
      "step": 100550
    },
    {
      "epoch": 1.4324362807916844,
      "grad_norm": 0.5211168527603149,
      "learning_rate": 0.00010458711239849945,
      "loss": 0.6393,
      "step": 100600
    },
    {
      "epoch": 1.4331482272533105,
      "grad_norm": 0.5569695830345154,
      "learning_rate": 0.00010454057647561613,
      "loss": 0.6291,
      "step": 100650
    },
    {
      "epoch": 1.4338601737149366,
      "grad_norm": 0.5276851058006287,
      "learning_rate": 0.00010449309084002091,
      "loss": 0.6768,
      "step": 100700
    },
    {
      "epoch": 1.4345721201765627,
      "grad_norm": 0.6780301928520203,
      "learning_rate": 0.00010444560520442566,
      "loss": 0.6894,
      "step": 100750
    },
    {
      "epoch": 1.4352840666381887,
      "grad_norm": 0.7349186539649963,
      "learning_rate": 0.00010439811956883044,
      "loss": 0.6529,
      "step": 100800
    },
    {
      "epoch": 1.4359960130998148,
      "grad_norm": 0.4806784987449646,
      "learning_rate": 0.00010435063393323521,
      "loss": 0.6871,
      "step": 100850
    },
    {
      "epoch": 1.436707959561441,
      "grad_norm": 0.49921515583992004,
      "learning_rate": 0.00010430314829763996,
      "loss": 0.6732,
      "step": 100900
    },
    {
      "epoch": 1.437419906023067,
      "grad_norm": 0.5146073698997498,
      "learning_rate": 0.00010425566266204474,
      "loss": 0.5479,
      "step": 100950
    },
    {
      "epoch": 1.438131852484693,
      "grad_norm": 0.5557993650436401,
      "learning_rate": 0.00010420817702644949,
      "loss": 0.6069,
      "step": 101000
    },
    {
      "epoch": 1.4388437989463192,
      "grad_norm": 0.5724456906318665,
      "learning_rate": 0.00010416069139085427,
      "loss": 0.6681,
      "step": 101050
    },
    {
      "epoch": 1.4395557454079453,
      "grad_norm": 0.6175346374511719,
      "learning_rate": 0.00010411320575525905,
      "loss": 0.699,
      "step": 101100
    },
    {
      "epoch": 1.4402676918695714,
      "grad_norm": 0.3989124894142151,
      "learning_rate": 0.0001040657201196638,
      "loss": 0.6847,
      "step": 101150
    },
    {
      "epoch": 1.4409796383311975,
      "grad_norm": 0.5277543067932129,
      "learning_rate": 0.00010401823448406857,
      "loss": 0.6376,
      "step": 101200
    },
    {
      "epoch": 1.4416915847928236,
      "grad_norm": 0.5586369633674622,
      "learning_rate": 0.00010397074884847333,
      "loss": 0.6399,
      "step": 101250
    },
    {
      "epoch": 1.4424035312544496,
      "grad_norm": 0.42919671535491943,
      "learning_rate": 0.0001039232632128781,
      "loss": 0.6276,
      "step": 101300
    },
    {
      "epoch": 1.4431154777160757,
      "grad_norm": 0.6229014992713928,
      "learning_rate": 0.00010387577757728288,
      "loss": 0.672,
      "step": 101350
    },
    {
      "epoch": 1.4438274241777018,
      "grad_norm": 0.5041776299476624,
      "learning_rate": 0.00010382829194168764,
      "loss": 0.5758,
      "step": 101400
    },
    {
      "epoch": 1.444539370639328,
      "grad_norm": 0.46011611819267273,
      "learning_rate": 0.00010378080630609241,
      "loss": 0.6121,
      "step": 101450
    },
    {
      "epoch": 1.445251317100954,
      "grad_norm": 0.7055173516273499,
      "learning_rate": 0.00010373332067049717,
      "loss": 0.6497,
      "step": 101500
    },
    {
      "epoch": 1.44596326356258,
      "grad_norm": 0.5128423571586609,
      "learning_rate": 0.00010368583503490195,
      "loss": 0.6656,
      "step": 101550
    },
    {
      "epoch": 1.4466752100242062,
      "grad_norm": 0.48816627264022827,
      "learning_rate": 0.00010363834939930673,
      "loss": 0.5893,
      "step": 101600
    },
    {
      "epoch": 1.4473871564858323,
      "grad_norm": 0.47995343804359436,
      "learning_rate": 0.00010359086376371148,
      "loss": 0.6226,
      "step": 101650
    },
    {
      "epoch": 1.4480991029474584,
      "grad_norm": 0.47084519267082214,
      "learning_rate": 0.00010354337812811626,
      "loss": 0.6204,
      "step": 101700
    },
    {
      "epoch": 1.4488110494090845,
      "grad_norm": 0.5001727938652039,
      "learning_rate": 0.000103495892492521,
      "loss": 0.6154,
      "step": 101750
    },
    {
      "epoch": 1.4495229958707105,
      "grad_norm": 0.6417458653450012,
      "learning_rate": 0.00010344840685692578,
      "loss": 0.6639,
      "step": 101800
    },
    {
      "epoch": 1.4502349423323366,
      "grad_norm": 0.7171569466590881,
      "learning_rate": 0.00010340092122133056,
      "loss": 0.6647,
      "step": 101850
    },
    {
      "epoch": 1.4509468887939627,
      "grad_norm": 0.3817019462585449,
      "learning_rate": 0.00010335343558573531,
      "loss": 0.5587,
      "step": 101900
    },
    {
      "epoch": 1.4516588352555888,
      "grad_norm": 0.5094404220581055,
      "learning_rate": 0.00010330594995014009,
      "loss": 0.6375,
      "step": 101950
    },
    {
      "epoch": 1.452370781717215,
      "grad_norm": 0.4957675635814667,
      "learning_rate": 0.00010325846431454484,
      "loss": 0.6859,
      "step": 102000
    },
    {
      "epoch": 1.453082728178841,
      "grad_norm": 0.7240200638771057,
      "learning_rate": 0.00010321097867894962,
      "loss": 0.6402,
      "step": 102050
    },
    {
      "epoch": 1.453794674640467,
      "grad_norm": 0.7038822174072266,
      "learning_rate": 0.0001031634930433544,
      "loss": 0.6489,
      "step": 102100
    },
    {
      "epoch": 1.4545066211020932,
      "grad_norm": 0.5330342054367065,
      "learning_rate": 0.00010311600740775915,
      "loss": 0.6324,
      "step": 102150
    },
    {
      "epoch": 1.4552185675637193,
      "grad_norm": 0.5834181904792786,
      "learning_rate": 0.00010306852177216392,
      "loss": 0.5684,
      "step": 102200
    },
    {
      "epoch": 1.4559305140253453,
      "grad_norm": 0.7226995229721069,
      "learning_rate": 0.0001030219858492806,
      "loss": 0.6387,
      "step": 102250
    },
    {
      "epoch": 1.4566424604869714,
      "grad_norm": 0.5490807890892029,
      "learning_rate": 0.00010297450021368535,
      "loss": 0.6139,
      "step": 102300
    },
    {
      "epoch": 1.4573544069485975,
      "grad_norm": 0.6949341893196106,
      "learning_rate": 0.00010292701457809013,
      "loss": 0.617,
      "step": 102350
    },
    {
      "epoch": 1.4580663534102236,
      "grad_norm": 0.6655299663543701,
      "learning_rate": 0.00010287952894249491,
      "loss": 0.6187,
      "step": 102400
    },
    {
      "epoch": 1.4587782998718497,
      "grad_norm": 0.3227943181991577,
      "learning_rate": 0.00010283204330689966,
      "loss": 0.6289,
      "step": 102450
    },
    {
      "epoch": 1.4594902463334758,
      "grad_norm": 0.5854218602180481,
      "learning_rate": 0.00010278455767130444,
      "loss": 0.6434,
      "step": 102500
    },
    {
      "epoch": 1.4602021927951019,
      "grad_norm": 0.6343972682952881,
      "learning_rate": 0.00010273707203570919,
      "loss": 0.6536,
      "step": 102550
    },
    {
      "epoch": 1.460914139256728,
      "grad_norm": 0.4107784926891327,
      "learning_rate": 0.00010268958640011396,
      "loss": 0.641,
      "step": 102600
    },
    {
      "epoch": 1.461626085718354,
      "grad_norm": 0.46283575892448425,
      "learning_rate": 0.00010264210076451874,
      "loss": 0.6228,
      "step": 102650
    },
    {
      "epoch": 1.46233803217998,
      "grad_norm": 0.8181782364845276,
      "learning_rate": 0.00010259461512892349,
      "loss": 0.6661,
      "step": 102700
    },
    {
      "epoch": 1.4630499786416062,
      "grad_norm": 0.5668143033981323,
      "learning_rate": 0.00010254712949332827,
      "loss": 0.6748,
      "step": 102750
    },
    {
      "epoch": 1.463761925103232,
      "grad_norm": 0.5166815519332886,
      "learning_rate": 0.00010249964385773303,
      "loss": 0.6857,
      "step": 102800
    },
    {
      "epoch": 1.4644738715648584,
      "grad_norm": 0.2643095850944519,
      "learning_rate": 0.00010245215822213781,
      "loss": 0.6493,
      "step": 102850
    },
    {
      "epoch": 1.4651858180264843,
      "grad_norm": 0.5190829634666443,
      "learning_rate": 0.00010240467258654258,
      "loss": 0.6822,
      "step": 102900
    },
    {
      "epoch": 1.4658977644881106,
      "grad_norm": 0.5530035495758057,
      "learning_rate": 0.00010235718695094734,
      "loss": 0.6504,
      "step": 102950
    },
    {
      "epoch": 1.4666097109497365,
      "grad_norm": 0.8211770057678223,
      "learning_rate": 0.00010230970131535212,
      "loss": 0.6482,
      "step": 103000
    },
    {
      "epoch": 1.4673216574113628,
      "grad_norm": 0.5367410778999329,
      "learning_rate": 0.00010226221567975687,
      "loss": 0.661,
      "step": 103050
    },
    {
      "epoch": 1.4680336038729886,
      "grad_norm": 0.6507319211959839,
      "learning_rate": 0.00010221473004416165,
      "loss": 0.6233,
      "step": 103100
    },
    {
      "epoch": 1.468745550334615,
      "grad_norm": 0.4720230996608734,
      "learning_rate": 0.00010216724440856642,
      "loss": 0.6439,
      "step": 103150
    },
    {
      "epoch": 1.4694574967962408,
      "grad_norm": 0.6710729002952576,
      "learning_rate": 0.00010211975877297117,
      "loss": 0.6379,
      "step": 103200
    },
    {
      "epoch": 1.4701694432578671,
      "grad_norm": 0.643805742263794,
      "learning_rate": 0.00010207227313737595,
      "loss": 0.6693,
      "step": 103250
    },
    {
      "epoch": 1.470881389719493,
      "grad_norm": 0.6456025242805481,
      "learning_rate": 0.0001020247875017807,
      "loss": 0.653,
      "step": 103300
    },
    {
      "epoch": 1.4715933361811193,
      "grad_norm": 0.5043006539344788,
      "learning_rate": 0.00010197730186618548,
      "loss": 0.6825,
      "step": 103350
    },
    {
      "epoch": 1.4723052826427452,
      "grad_norm": 0.43122777342796326,
      "learning_rate": 0.00010192981623059026,
      "loss": 0.6559,
      "step": 103400
    },
    {
      "epoch": 1.4730172291043713,
      "grad_norm": 0.41741323471069336,
      "learning_rate": 0.00010188233059499501,
      "loss": 0.6462,
      "step": 103450
    },
    {
      "epoch": 1.4737291755659974,
      "grad_norm": 0.5205941796302795,
      "learning_rate": 0.00010183484495939979,
      "loss": 0.6414,
      "step": 103500
    },
    {
      "epoch": 1.4744411220276235,
      "grad_norm": 0.47672751545906067,
      "learning_rate": 0.00010178735932380455,
      "loss": 0.67,
      "step": 103550
    },
    {
      "epoch": 1.4751530684892495,
      "grad_norm": 0.6534926295280457,
      "learning_rate": 0.00010173987368820931,
      "loss": 0.6397,
      "step": 103600
    },
    {
      "epoch": 1.4758650149508756,
      "grad_norm": 0.7907444834709167,
      "learning_rate": 0.00010169238805261409,
      "loss": 0.6659,
      "step": 103650
    },
    {
      "epoch": 1.4765769614125017,
      "grad_norm": 0.7899671792984009,
      "learning_rate": 0.00010164490241701886,
      "loss": 0.6498,
      "step": 103700
    },
    {
      "epoch": 1.4772889078741278,
      "grad_norm": 0.7032619118690491,
      "learning_rate": 0.00010159741678142363,
      "loss": 0.6517,
      "step": 103750
    },
    {
      "epoch": 1.478000854335754,
      "grad_norm": 0.4549988806247711,
      "learning_rate": 0.00010154993114582838,
      "loss": 0.6312,
      "step": 103800
    },
    {
      "epoch": 1.47871280079738,
      "grad_norm": 0.5742658376693726,
      "learning_rate": 0.00010150244551023316,
      "loss": 0.6574,
      "step": 103850
    },
    {
      "epoch": 1.479424747259006,
      "grad_norm": 0.6515724658966064,
      "learning_rate": 0.00010145495987463794,
      "loss": 0.6311,
      "step": 103900
    },
    {
      "epoch": 1.4801366937206322,
      "grad_norm": 0.46506085991859436,
      "learning_rate": 0.00010140747423904269,
      "loss": 0.6277,
      "step": 103950
    },
    {
      "epoch": 1.4808486401822583,
      "grad_norm": 0.5756485462188721,
      "learning_rate": 0.00010135998860344747,
      "loss": 0.6149,
      "step": 104000
    },
    {
      "epoch": 1.4815605866438843,
      "grad_norm": 0.46274077892303467,
      "learning_rate": 0.00010131250296785222,
      "loss": 0.6632,
      "step": 104050
    },
    {
      "epoch": 1.4822725331055104,
      "grad_norm": 0.5322691202163696,
      "learning_rate": 0.000101265017332257,
      "loss": 0.6449,
      "step": 104100
    },
    {
      "epoch": 1.4829844795671365,
      "grad_norm": 0.4346505105495453,
      "learning_rate": 0.00010121753169666177,
      "loss": 0.5726,
      "step": 104150
    },
    {
      "epoch": 1.4836964260287626,
      "grad_norm": 0.7319326400756836,
      "learning_rate": 0.00010117004606106652,
      "loss": 0.6746,
      "step": 104200
    },
    {
      "epoch": 1.4844083724903887,
      "grad_norm": 0.4686991572380066,
      "learning_rate": 0.0001011225604254713,
      "loss": 0.7016,
      "step": 104250
    },
    {
      "epoch": 1.4851203189520148,
      "grad_norm": 0.4407646358013153,
      "learning_rate": 0.00010107507478987605,
      "loss": 0.7062,
      "step": 104300
    },
    {
      "epoch": 1.4858322654136409,
      "grad_norm": 0.4511789083480835,
      "learning_rate": 0.00010102758915428083,
      "loss": 0.6492,
      "step": 104350
    },
    {
      "epoch": 1.486544211875267,
      "grad_norm": 0.6056671738624573,
      "learning_rate": 0.0001009801035186856,
      "loss": 0.6394,
      "step": 104400
    },
    {
      "epoch": 1.487256158336893,
      "grad_norm": 0.7074532508850098,
      "learning_rate": 0.00010093261788309036,
      "loss": 0.6195,
      "step": 104450
    },
    {
      "epoch": 1.4879681047985192,
      "grad_norm": 0.6209703087806702,
      "learning_rate": 0.00010088513224749513,
      "loss": 0.6617,
      "step": 104500
    },
    {
      "epoch": 1.4886800512601452,
      "grad_norm": 0.8751618266105652,
      "learning_rate": 0.0001008376466118999,
      "loss": 0.6858,
      "step": 104550
    },
    {
      "epoch": 1.4893919977217713,
      "grad_norm": 0.6147716045379639,
      "learning_rate": 0.00010079016097630468,
      "loss": 0.6881,
      "step": 104600
    },
    {
      "epoch": 1.4901039441833974,
      "grad_norm": 0.5447062849998474,
      "learning_rate": 0.00010074267534070945,
      "loss": 0.634,
      "step": 104650
    },
    {
      "epoch": 1.4908158906450235,
      "grad_norm": 0.42026031017303467,
      "learning_rate": 0.0001006951897051142,
      "loss": 0.6781,
      "step": 104700
    },
    {
      "epoch": 1.4915278371066496,
      "grad_norm": 0.5954877734184265,
      "learning_rate": 0.00010064770406951898,
      "loss": 0.666,
      "step": 104750
    },
    {
      "epoch": 1.4922397835682757,
      "grad_norm": 0.5409018397331238,
      "learning_rate": 0.00010060021843392373,
      "loss": 0.6233,
      "step": 104800
    },
    {
      "epoch": 1.4929517300299018,
      "grad_norm": 0.667202353477478,
      "learning_rate": 0.00010055273279832851,
      "loss": 0.6123,
      "step": 104850
    },
    {
      "epoch": 1.4936636764915279,
      "grad_norm": 0.9169930219650269,
      "learning_rate": 0.00010050524716273329,
      "loss": 0.6182,
      "step": 104900
    },
    {
      "epoch": 1.494375622953154,
      "grad_norm": 0.5339242219924927,
      "learning_rate": 0.00010045776152713804,
      "loss": 0.6727,
      "step": 104950
    },
    {
      "epoch": 1.49508756941478,
      "grad_norm": 0.5269656777381897,
      "learning_rate": 0.00010041027589154282,
      "loss": 0.668,
      "step": 105000
    },
    {
      "epoch": 1.4957995158764061,
      "grad_norm": 0.786841094493866,
      "learning_rate": 0.00010036279025594757,
      "loss": 0.635,
      "step": 105050
    },
    {
      "epoch": 1.4965114623380322,
      "grad_norm": 0.4168139100074768,
      "learning_rate": 0.00010031530462035234,
      "loss": 0.6159,
      "step": 105100
    },
    {
      "epoch": 1.4972234087996583,
      "grad_norm": 0.45625168085098267,
      "learning_rate": 0.00010026781898475712,
      "loss": 0.6264,
      "step": 105150
    },
    {
      "epoch": 1.4979353552612844,
      "grad_norm": 0.5301780700683594,
      "learning_rate": 0.00010022033334916187,
      "loss": 0.6126,
      "step": 105200
    },
    {
      "epoch": 1.4986473017229105,
      "grad_norm": 0.799458920955658,
      "learning_rate": 0.00010017284771356665,
      "loss": 0.6924,
      "step": 105250
    },
    {
      "epoch": 1.4993592481845366,
      "grad_norm": 0.6546634435653687,
      "learning_rate": 0.00010012536207797141,
      "loss": 0.6766,
      "step": 105300
    },
    {
      "epoch": 1.5000711946461625,
      "grad_norm": 0.6472576856613159,
      "learning_rate": 0.00010007787644237618,
      "loss": 0.6346,
      "step": 105350
    },
    {
      "epoch": 1.5007831411077888,
      "grad_norm": 0.371454656124115,
      "learning_rate": 0.00010003039080678096,
      "loss": 0.6444,
      "step": 105400
    },
    {
      "epoch": 1.5014950875694146,
      "grad_norm": 0.46751946210861206,
      "learning_rate": 9.998290517118572e-05,
      "loss": 0.6103,
      "step": 105450
    },
    {
      "epoch": 1.502207034031041,
      "grad_norm": 0.5804412364959717,
      "learning_rate": 9.99354195355905e-05,
      "loss": 0.6883,
      "step": 105500
    },
    {
      "epoch": 1.5029189804926668,
      "grad_norm": 0.5857120752334595,
      "learning_rate": 9.988793389999526e-05,
      "loss": 0.6514,
      "step": 105550
    },
    {
      "epoch": 1.5036309269542931,
      "grad_norm": 0.49130624532699585,
      "learning_rate": 9.984044826440003e-05,
      "loss": 0.601,
      "step": 105600
    },
    {
      "epoch": 1.504342873415919,
      "grad_norm": 0.5109453797340393,
      "learning_rate": 9.979296262880479e-05,
      "loss": 0.6819,
      "step": 105650
    },
    {
      "epoch": 1.5050548198775453,
      "grad_norm": 0.3435531556606293,
      "learning_rate": 9.974547699320957e-05,
      "loss": 0.5472,
      "step": 105700
    },
    {
      "epoch": 1.5057667663391712,
      "grad_norm": 0.6203576922416687,
      "learning_rate": 9.969799135761433e-05,
      "loss": 0.6436,
      "step": 105750
    },
    {
      "epoch": 1.5064787128007975,
      "grad_norm": 0.5734279155731201,
      "learning_rate": 9.96505057220191e-05,
      "loss": 0.6126,
      "step": 105800
    },
    {
      "epoch": 1.5071906592624233,
      "grad_norm": 0.596210241317749,
      "learning_rate": 9.960302008642386e-05,
      "loss": 0.6386,
      "step": 105850
    },
    {
      "epoch": 1.5079026057240497,
      "grad_norm": 0.4243331253528595,
      "learning_rate": 9.955553445082862e-05,
      "loss": 0.6704,
      "step": 105900
    },
    {
      "epoch": 1.5086145521856755,
      "grad_norm": 0.518279492855072,
      "learning_rate": 9.95080488152334e-05,
      "loss": 0.6715,
      "step": 105950
    },
    {
      "epoch": 1.5093264986473018,
      "grad_norm": 0.318314790725708,
      "learning_rate": 9.946056317963816e-05,
      "loss": 0.6319,
      "step": 106000
    },
    {
      "epoch": 1.5100384451089277,
      "grad_norm": 0.8507799506187439,
      "learning_rate": 9.941307754404293e-05,
      "loss": 0.6392,
      "step": 106050
    },
    {
      "epoch": 1.510750391570554,
      "grad_norm": 0.6049284934997559,
      "learning_rate": 9.936654162115961e-05,
      "loss": 0.5742,
      "step": 106100
    },
    {
      "epoch": 1.5114623380321799,
      "grad_norm": 0.46449610590934753,
      "learning_rate": 9.931905598556437e-05,
      "loss": 0.5707,
      "step": 106150
    },
    {
      "epoch": 1.5121742844938062,
      "grad_norm": 0.5772040486335754,
      "learning_rate": 9.927157034996914e-05,
      "loss": 0.6593,
      "step": 106200
    },
    {
      "epoch": 1.512886230955432,
      "grad_norm": 0.7203744649887085,
      "learning_rate": 9.922408471437391e-05,
      "loss": 0.6472,
      "step": 106250
    },
    {
      "epoch": 1.5135981774170584,
      "grad_norm": 0.41938406229019165,
      "learning_rate": 9.917659907877868e-05,
      "loss": 0.6147,
      "step": 106300
    },
    {
      "epoch": 1.5143101238786842,
      "grad_norm": 0.3828001320362091,
      "learning_rate": 9.912911344318344e-05,
      "loss": 0.6417,
      "step": 106350
    },
    {
      "epoch": 1.5150220703403106,
      "grad_norm": 0.5085821151733398,
      "learning_rate": 9.90816278075882e-05,
      "loss": 0.6675,
      "step": 106400
    },
    {
      "epoch": 1.5157340168019364,
      "grad_norm": 0.41529953479766846,
      "learning_rate": 9.903414217199297e-05,
      "loss": 0.6727,
      "step": 106450
    },
    {
      "epoch": 1.5164459632635627,
      "grad_norm": 0.5222097039222717,
      "learning_rate": 9.898665653639775e-05,
      "loss": 0.5993,
      "step": 106500
    },
    {
      "epoch": 1.5171579097251886,
      "grad_norm": 0.5635049343109131,
      "learning_rate": 9.893917090080251e-05,
      "loss": 0.6411,
      "step": 106550
    },
    {
      "epoch": 1.517869856186815,
      "grad_norm": 0.6771787405014038,
      "learning_rate": 9.889168526520728e-05,
      "loss": 0.6966,
      "step": 106600
    },
    {
      "epoch": 1.5185818026484408,
      "grad_norm": 0.634580135345459,
      "learning_rate": 9.884419962961204e-05,
      "loss": 0.5902,
      "step": 106650
    },
    {
      "epoch": 1.5192937491100669,
      "grad_norm": 0.4043189585208893,
      "learning_rate": 9.87967139940168e-05,
      "loss": 0.5927,
      "step": 106700
    },
    {
      "epoch": 1.520005695571693,
      "grad_norm": 0.4970910847187042,
      "learning_rate": 9.874922835842158e-05,
      "loss": 0.6419,
      "step": 106750
    },
    {
      "epoch": 1.520717642033319,
      "grad_norm": 0.586296558380127,
      "learning_rate": 9.870174272282635e-05,
      "loss": 0.6081,
      "step": 106800
    },
    {
      "epoch": 1.5214295884949451,
      "grad_norm": 0.5665112733840942,
      "learning_rate": 9.865425708723112e-05,
      "loss": 0.6657,
      "step": 106850
    },
    {
      "epoch": 1.5221415349565712,
      "grad_norm": 0.47015276551246643,
      "learning_rate": 9.860677145163589e-05,
      "loss": 0.6959,
      "step": 106900
    },
    {
      "epoch": 1.5228534814181973,
      "grad_norm": 0.3735712766647339,
      "learning_rate": 9.855928581604065e-05,
      "loss": 0.6344,
      "step": 106950
    },
    {
      "epoch": 1.5235654278798234,
      "grad_norm": 0.44998741149902344,
      "learning_rate": 9.851180018044543e-05,
      "loss": 0.6536,
      "step": 107000
    },
    {
      "epoch": 1.5242773743414495,
      "grad_norm": 0.712478756904602,
      "learning_rate": 9.846431454485019e-05,
      "loss": 0.6485,
      "step": 107050
    },
    {
      "epoch": 1.5249893208030756,
      "grad_norm": 0.5061662793159485,
      "learning_rate": 9.841682890925496e-05,
      "loss": 0.6401,
      "step": 107100
    },
    {
      "epoch": 1.5257012672647017,
      "grad_norm": 0.6776825785636902,
      "learning_rate": 9.836934327365972e-05,
      "loss": 0.6625,
      "step": 107150
    },
    {
      "epoch": 1.5264132137263278,
      "grad_norm": 0.8722428679466248,
      "learning_rate": 9.832185763806448e-05,
      "loss": 0.6439,
      "step": 107200
    },
    {
      "epoch": 1.5271251601879539,
      "grad_norm": 0.31566667556762695,
      "learning_rate": 9.827437200246926e-05,
      "loss": 0.6227,
      "step": 107250
    },
    {
      "epoch": 1.52783710664958,
      "grad_norm": 0.7552012205123901,
      "learning_rate": 9.822688636687403e-05,
      "loss": 0.6444,
      "step": 107300
    },
    {
      "epoch": 1.528549053111206,
      "grad_norm": 0.4779445230960846,
      "learning_rate": 9.817940073127879e-05,
      "loss": 0.593,
      "step": 107350
    },
    {
      "epoch": 1.5292609995728321,
      "grad_norm": 0.5459274649620056,
      "learning_rate": 9.813191509568355e-05,
      "loss": 0.6236,
      "step": 107400
    },
    {
      "epoch": 1.5299729460344582,
      "grad_norm": 0.5801877379417419,
      "learning_rate": 9.808442946008832e-05,
      "loss": 0.664,
      "step": 107450
    },
    {
      "epoch": 1.5306848924960843,
      "grad_norm": 0.5585116744041443,
      "learning_rate": 9.80369438244931e-05,
      "loss": 0.6598,
      "step": 107500
    },
    {
      "epoch": 1.5313968389577104,
      "grad_norm": 0.571830689907074,
      "learning_rate": 9.798945818889786e-05,
      "loss": 0.6718,
      "step": 107550
    },
    {
      "epoch": 1.5321087854193365,
      "grad_norm": 0.37223026156425476,
      "learning_rate": 9.794197255330262e-05,
      "loss": 0.6558,
      "step": 107600
    },
    {
      "epoch": 1.5328207318809626,
      "grad_norm": 0.6123344302177429,
      "learning_rate": 9.789448691770739e-05,
      "loss": 0.6816,
      "step": 107650
    },
    {
      "epoch": 1.5335326783425887,
      "grad_norm": 0.3927672803401947,
      "learning_rate": 9.784700128211217e-05,
      "loss": 0.5939,
      "step": 107700
    },
    {
      "epoch": 1.5342446248042148,
      "grad_norm": 0.5493810176849365,
      "learning_rate": 9.779951564651694e-05,
      "loss": 0.62,
      "step": 107750
    },
    {
      "epoch": 1.5349565712658408,
      "grad_norm": 0.520818829536438,
      "learning_rate": 9.775203001092171e-05,
      "loss": 0.6199,
      "step": 107800
    },
    {
      "epoch": 1.535668517727467,
      "grad_norm": 0.7297322750091553,
      "learning_rate": 9.770454437532647e-05,
      "loss": 0.6034,
      "step": 107850
    },
    {
      "epoch": 1.536380464189093,
      "grad_norm": 0.5923416018486023,
      "learning_rate": 9.765705873973124e-05,
      "loss": 0.6369,
      "step": 107900
    },
    {
      "epoch": 1.537092410650719,
      "grad_norm": 0.7405582666397095,
      "learning_rate": 9.7609573104136e-05,
      "loss": 0.5775,
      "step": 107950
    },
    {
      "epoch": 1.5378043571123452,
      "grad_norm": 0.784044086933136,
      "learning_rate": 9.756208746854078e-05,
      "loss": 0.6607,
      "step": 108000
    },
    {
      "epoch": 1.5385163035739713,
      "grad_norm": 0.5276558995246887,
      "learning_rate": 9.751460183294554e-05,
      "loss": 0.626,
      "step": 108050
    },
    {
      "epoch": 1.5392282500355974,
      "grad_norm": 1.0682443380355835,
      "learning_rate": 9.74671161973503e-05,
      "loss": 0.6847,
      "step": 108100
    },
    {
      "epoch": 1.5399401964972235,
      "grad_norm": 0.3859612047672272,
      "learning_rate": 9.741963056175507e-05,
      "loss": 0.6292,
      "step": 108150
    },
    {
      "epoch": 1.5406521429588493,
      "grad_norm": 0.749793529510498,
      "learning_rate": 9.737214492615983e-05,
      "loss": 0.6359,
      "step": 108200
    },
    {
      "epoch": 1.5413640894204756,
      "grad_norm": 0.4797775447368622,
      "learning_rate": 9.732465929056461e-05,
      "loss": 0.6867,
      "step": 108250
    },
    {
      "epoch": 1.5420760358821015,
      "grad_norm": 0.5390109419822693,
      "learning_rate": 9.727717365496938e-05,
      "loss": 0.6853,
      "step": 108300
    },
    {
      "epoch": 1.5427879823437278,
      "grad_norm": 0.4352821409702301,
      "learning_rate": 9.722968801937414e-05,
      "loss": 0.6548,
      "step": 108350
    },
    {
      "epoch": 1.5434999288053537,
      "grad_norm": 0.37041395902633667,
      "learning_rate": 9.71822023837789e-05,
      "loss": 0.5821,
      "step": 108400
    },
    {
      "epoch": 1.54421187526698,
      "grad_norm": 0.8024674654006958,
      "learning_rate": 9.713471674818367e-05,
      "loss": 0.6185,
      "step": 108450
    },
    {
      "epoch": 1.5449238217286059,
      "grad_norm": 0.7465371489524841,
      "learning_rate": 9.708723111258845e-05,
      "loss": 0.6112,
      "step": 108500
    },
    {
      "epoch": 1.5456357681902322,
      "grad_norm": 0.5842902660369873,
      "learning_rate": 9.703974547699321e-05,
      "loss": 0.6406,
      "step": 108550
    },
    {
      "epoch": 1.546347714651858,
      "grad_norm": 0.4370427131652832,
      "learning_rate": 9.699225984139799e-05,
      "loss": 0.6108,
      "step": 108600
    },
    {
      "epoch": 1.5470596611134844,
      "grad_norm": 0.7084591388702393,
      "learning_rate": 9.694477420580275e-05,
      "loss": 0.6692,
      "step": 108650
    },
    {
      "epoch": 1.5477716075751102,
      "grad_norm": 0.47663402557373047,
      "learning_rate": 9.689728857020752e-05,
      "loss": 0.6895,
      "step": 108700
    },
    {
      "epoch": 1.5484835540367365,
      "grad_norm": 0.6048988103866577,
      "learning_rate": 9.684980293461229e-05,
      "loss": 0.6687,
      "step": 108750
    },
    {
      "epoch": 1.5491955004983624,
      "grad_norm": 0.5540804266929626,
      "learning_rate": 9.680231729901706e-05,
      "loss": 0.6324,
      "step": 108800
    },
    {
      "epoch": 1.5499074469599887,
      "grad_norm": 0.589713454246521,
      "learning_rate": 9.675483166342182e-05,
      "loss": 0.6461,
      "step": 108850
    },
    {
      "epoch": 1.5506193934216146,
      "grad_norm": 0.46267804503440857,
      "learning_rate": 9.670734602782658e-05,
      "loss": 0.6157,
      "step": 108900
    },
    {
      "epoch": 1.551331339883241,
      "grad_norm": 0.3708595037460327,
      "learning_rate": 9.665986039223135e-05,
      "loss": 0.6545,
      "step": 108950
    },
    {
      "epoch": 1.5520432863448668,
      "grad_norm": 0.5973511934280396,
      "learning_rate": 9.661237475663613e-05,
      "loss": 0.7004,
      "step": 109000
    },
    {
      "epoch": 1.552755232806493,
      "grad_norm": 0.5793595910072327,
      "learning_rate": 9.656488912104089e-05,
      "loss": 0.623,
      "step": 109050
    },
    {
      "epoch": 1.553467179268119,
      "grad_norm": 0.53514564037323,
      "learning_rate": 9.651740348544565e-05,
      "loss": 0.6053,
      "step": 109100
    },
    {
      "epoch": 1.5541791257297453,
      "grad_norm": 0.5655851364135742,
      "learning_rate": 9.646991784985042e-05,
      "loss": 0.6686,
      "step": 109150
    },
    {
      "epoch": 1.5548910721913711,
      "grad_norm": 0.28459301590919495,
      "learning_rate": 9.642243221425518e-05,
      "loss": 0.6401,
      "step": 109200
    },
    {
      "epoch": 1.5556030186529974,
      "grad_norm": 0.6446864604949951,
      "learning_rate": 9.637494657865996e-05,
      "loss": 0.6646,
      "step": 109250
    },
    {
      "epoch": 1.5563149651146233,
      "grad_norm": 0.8585646748542786,
      "learning_rate": 9.632746094306472e-05,
      "loss": 0.5965,
      "step": 109300
    },
    {
      "epoch": 1.5570269115762496,
      "grad_norm": 0.81138014793396,
      "learning_rate": 9.627997530746949e-05,
      "loss": 0.588,
      "step": 109350
    },
    {
      "epoch": 1.5577388580378755,
      "grad_norm": 0.4782446622848511,
      "learning_rate": 9.623248967187427e-05,
      "loss": 0.632,
      "step": 109400
    },
    {
      "epoch": 1.5584508044995018,
      "grad_norm": 0.5418070554733276,
      "learning_rate": 9.618500403627903e-05,
      "loss": 0.6175,
      "step": 109450
    },
    {
      "epoch": 1.5591627509611277,
      "grad_norm": 0.5715348720550537,
      "learning_rate": 9.613751840068381e-05,
      "loss": 0.6283,
      "step": 109500
    },
    {
      "epoch": 1.559874697422754,
      "grad_norm": 0.6147140264511108,
      "learning_rate": 9.609003276508857e-05,
      "loss": 0.5887,
      "step": 109550
    },
    {
      "epoch": 1.5605866438843798,
      "grad_norm": 0.5817490816116333,
      "learning_rate": 9.604254712949334e-05,
      "loss": 0.6085,
      "step": 109600
    },
    {
      "epoch": 1.561298590346006,
      "grad_norm": 0.8283625841140747,
      "learning_rate": 9.59950614938981e-05,
      "loss": 0.6244,
      "step": 109650
    },
    {
      "epoch": 1.562010536807632,
      "grad_norm": 0.5001276135444641,
      "learning_rate": 9.594757585830286e-05,
      "loss": 0.659,
      "step": 109700
    },
    {
      "epoch": 1.5627224832692581,
      "grad_norm": 0.47914546728134155,
      "learning_rate": 9.590009022270764e-05,
      "loss": 0.6705,
      "step": 109750
    },
    {
      "epoch": 1.5634344297308842,
      "grad_norm": 0.5574117302894592,
      "learning_rate": 9.58526045871124e-05,
      "loss": 0.6427,
      "step": 109800
    },
    {
      "epoch": 1.5641463761925103,
      "grad_norm": 0.9581164717674255,
      "learning_rate": 9.580511895151717e-05,
      "loss": 0.655,
      "step": 109850
    },
    {
      "epoch": 1.5648583226541364,
      "grad_norm": 0.3245406150817871,
      "learning_rate": 9.575763331592193e-05,
      "loss": 0.6587,
      "step": 109900
    },
    {
      "epoch": 1.5655702691157625,
      "grad_norm": 0.731289803981781,
      "learning_rate": 9.57101476803267e-05,
      "loss": 0.6487,
      "step": 109950
    },
    {
      "epoch": 1.5662822155773886,
      "grad_norm": 0.4365129768848419,
      "learning_rate": 9.566266204473148e-05,
      "loss": 0.6543,
      "step": 110000
    },
    {
      "epoch": 1.5669941620390146,
      "grad_norm": 0.36177191138267517,
      "learning_rate": 9.561517640913624e-05,
      "loss": 0.5971,
      "step": 110050
    },
    {
      "epoch": 1.5677061085006407,
      "grad_norm": 0.5029952526092529,
      "learning_rate": 9.5567690773541e-05,
      "loss": 0.6882,
      "step": 110100
    },
    {
      "epoch": 1.5684180549622668,
      "grad_norm": 0.7078908085823059,
      "learning_rate": 9.552020513794577e-05,
      "loss": 0.5865,
      "step": 110150
    },
    {
      "epoch": 1.569130001423893,
      "grad_norm": 0.7469948530197144,
      "learning_rate": 9.547271950235053e-05,
      "loss": 0.6297,
      "step": 110200
    },
    {
      "epoch": 1.569841947885519,
      "grad_norm": 0.582322895526886,
      "learning_rate": 9.542523386675531e-05,
      "loss": 0.6376,
      "step": 110250
    },
    {
      "epoch": 1.570553894347145,
      "grad_norm": 0.47756659984588623,
      "learning_rate": 9.537774823116009e-05,
      "loss": 0.6206,
      "step": 110300
    },
    {
      "epoch": 1.5712658408087712,
      "grad_norm": 0.5280044078826904,
      "learning_rate": 9.533026259556485e-05,
      "loss": 0.6109,
      "step": 110350
    },
    {
      "epoch": 1.5719777872703973,
      "grad_norm": 0.5691530704498291,
      "learning_rate": 9.528277695996961e-05,
      "loss": 0.6307,
      "step": 110400
    },
    {
      "epoch": 1.5726897337320234,
      "grad_norm": 0.4551903307437897,
      "learning_rate": 9.523529132437438e-05,
      "loss": 0.6542,
      "step": 110450
    },
    {
      "epoch": 1.5734016801936495,
      "grad_norm": 0.6143501996994019,
      "learning_rate": 9.518780568877916e-05,
      "loss": 0.6366,
      "step": 110500
    },
    {
      "epoch": 1.5741136266552755,
      "grad_norm": 0.5488156080245972,
      "learning_rate": 9.514032005318392e-05,
      "loss": 0.6012,
      "step": 110550
    },
    {
      "epoch": 1.5748255731169016,
      "grad_norm": 0.8528561592102051,
      "learning_rate": 9.509283441758868e-05,
      "loss": 0.708,
      "step": 110600
    },
    {
      "epoch": 1.5755375195785277,
      "grad_norm": 0.40247905254364014,
      "learning_rate": 9.504534878199345e-05,
      "loss": 0.6818,
      "step": 110650
    },
    {
      "epoch": 1.5762494660401538,
      "grad_norm": 0.6283948421478271,
      "learning_rate": 9.499786314639821e-05,
      "loss": 0.7026,
      "step": 110700
    },
    {
      "epoch": 1.57696141250178,
      "grad_norm": 0.7197996377944946,
      "learning_rate": 9.495037751080299e-05,
      "loss": 0.6157,
      "step": 110750
    },
    {
      "epoch": 1.577673358963406,
      "grad_norm": 0.47159940004348755,
      "learning_rate": 9.490384158791966e-05,
      "loss": 0.6355,
      "step": 110800
    },
    {
      "epoch": 1.578385305425032,
      "grad_norm": 0.6911907196044922,
      "learning_rate": 9.485635595232443e-05,
      "loss": 0.6483,
      "step": 110850
    },
    {
      "epoch": 1.5790972518866582,
      "grad_norm": 1.1002204418182373,
      "learning_rate": 9.48088703167292e-05,
      "loss": 0.6795,
      "step": 110900
    },
    {
      "epoch": 1.5798091983482843,
      "grad_norm": 0.4634875953197479,
      "learning_rate": 9.476138468113396e-05,
      "loss": 0.6317,
      "step": 110950
    },
    {
      "epoch": 1.5805211448099103,
      "grad_norm": 0.6498629450798035,
      "learning_rate": 9.471389904553874e-05,
      "loss": 0.6649,
      "step": 111000
    },
    {
      "epoch": 1.5812330912715362,
      "grad_norm": 0.49541643261909485,
      "learning_rate": 9.46664134099435e-05,
      "loss": 0.6566,
      "step": 111050
    },
    {
      "epoch": 1.5819450377331625,
      "grad_norm": 0.5422843098640442,
      "learning_rate": 9.461892777434827e-05,
      "loss": 0.6173,
      "step": 111100
    },
    {
      "epoch": 1.5826569841947884,
      "grad_norm": 0.6346476674079895,
      "learning_rate": 9.457144213875303e-05,
      "loss": 0.6491,
      "step": 111150
    },
    {
      "epoch": 1.5833689306564147,
      "grad_norm": 0.6485323309898376,
      "learning_rate": 9.45239565031578e-05,
      "loss": 0.6501,
      "step": 111200
    },
    {
      "epoch": 1.5840808771180406,
      "grad_norm": 0.8066803216934204,
      "learning_rate": 9.447647086756257e-05,
      "loss": 0.6322,
      "step": 111250
    },
    {
      "epoch": 1.5847928235796669,
      "grad_norm": 0.4709703326225281,
      "learning_rate": 9.442898523196734e-05,
      "loss": 0.6834,
      "step": 111300
    },
    {
      "epoch": 1.5855047700412928,
      "grad_norm": 0.7882567048072815,
      "learning_rate": 9.43814995963721e-05,
      "loss": 0.6701,
      "step": 111350
    },
    {
      "epoch": 1.586216716502919,
      "grad_norm": 2.1738860607147217,
      "learning_rate": 9.433401396077687e-05,
      "loss": 0.6409,
      "step": 111400
    },
    {
      "epoch": 1.586928662964545,
      "grad_norm": 0.5473303198814392,
      "learning_rate": 9.428652832518163e-05,
      "loss": 0.7205,
      "step": 111450
    },
    {
      "epoch": 1.5876406094261712,
      "grad_norm": 0.33809566497802734,
      "learning_rate": 9.423904268958641e-05,
      "loss": 0.6166,
      "step": 111500
    },
    {
      "epoch": 1.5883525558877971,
      "grad_norm": 0.2638775706291199,
      "learning_rate": 9.419155705399117e-05,
      "loss": 0.6245,
      "step": 111550
    },
    {
      "epoch": 1.5890645023494234,
      "grad_norm": 0.48559126257896423,
      "learning_rate": 9.414407141839594e-05,
      "loss": 0.6544,
      "step": 111600
    },
    {
      "epoch": 1.5897764488110493,
      "grad_norm": 0.5477055907249451,
      "learning_rate": 9.40965857828007e-05,
      "loss": 0.5631,
      "step": 111650
    },
    {
      "epoch": 1.5904883952726756,
      "grad_norm": 0.34808188676834106,
      "learning_rate": 9.404910014720548e-05,
      "loss": 0.6185,
      "step": 111700
    },
    {
      "epoch": 1.5912003417343015,
      "grad_norm": 0.4888686239719391,
      "learning_rate": 9.400161451161024e-05,
      "loss": 0.6647,
      "step": 111750
    },
    {
      "epoch": 1.5919122881959278,
      "grad_norm": 0.585509717464447,
      "learning_rate": 9.395412887601502e-05,
      "loss": 0.6415,
      "step": 111800
    },
    {
      "epoch": 1.5926242346575537,
      "grad_norm": 0.5327326655387878,
      "learning_rate": 9.390664324041978e-05,
      "loss": 0.7027,
      "step": 111850
    },
    {
      "epoch": 1.59333618111918,
      "grad_norm": 0.8277948498725891,
      "learning_rate": 9.385915760482455e-05,
      "loss": 0.6378,
      "step": 111900
    },
    {
      "epoch": 1.5940481275808058,
      "grad_norm": 0.46583354473114014,
      "learning_rate": 9.381167196922931e-05,
      "loss": 0.6444,
      "step": 111950
    },
    {
      "epoch": 1.5947600740424321,
      "grad_norm": 0.6887389421463013,
      "learning_rate": 9.376418633363409e-05,
      "loss": 0.6123,
      "step": 112000
    },
    {
      "epoch": 1.595472020504058,
      "grad_norm": 0.604973554611206,
      "learning_rate": 9.371670069803885e-05,
      "loss": 0.6156,
      "step": 112050
    },
    {
      "epoch": 1.5961839669656843,
      "grad_norm": 0.4841424226760864,
      "learning_rate": 9.366921506244362e-05,
      "loss": 0.6658,
      "step": 112100
    },
    {
      "epoch": 1.5968959134273102,
      "grad_norm": 0.6323766708374023,
      "learning_rate": 9.362172942684838e-05,
      "loss": 0.7214,
      "step": 112150
    },
    {
      "epoch": 1.5976078598889365,
      "grad_norm": 0.4213745594024658,
      "learning_rate": 9.357424379125314e-05,
      "loss": 0.6754,
      "step": 112200
    },
    {
      "epoch": 1.5983198063505624,
      "grad_norm": 0.5877965688705444,
      "learning_rate": 9.352675815565792e-05,
      "loss": 0.6051,
      "step": 112250
    },
    {
      "epoch": 1.5990317528121887,
      "grad_norm": 0.5180643200874329,
      "learning_rate": 9.347927252006269e-05,
      "loss": 0.6288,
      "step": 112300
    },
    {
      "epoch": 1.5997436992738145,
      "grad_norm": 0.5078686475753784,
      "learning_rate": 9.343178688446745e-05,
      "loss": 0.6962,
      "step": 112350
    },
    {
      "epoch": 1.6004556457354409,
      "grad_norm": 0.8448429703712463,
      "learning_rate": 9.338430124887221e-05,
      "loss": 0.6472,
      "step": 112400
    },
    {
      "epoch": 1.6011675921970667,
      "grad_norm": 0.5166000723838806,
      "learning_rate": 9.333681561327698e-05,
      "loss": 0.5865,
      "step": 112450
    },
    {
      "epoch": 1.6018795386586928,
      "grad_norm": 0.7763435244560242,
      "learning_rate": 9.328932997768176e-05,
      "loss": 0.6029,
      "step": 112500
    },
    {
      "epoch": 1.602591485120319,
      "grad_norm": 0.6956639885902405,
      "learning_rate": 9.324184434208652e-05,
      "loss": 0.7009,
      "step": 112550
    },
    {
      "epoch": 1.603303431581945,
      "grad_norm": 0.6854779720306396,
      "learning_rate": 9.31943587064913e-05,
      "loss": 0.6398,
      "step": 112600
    },
    {
      "epoch": 1.604015378043571,
      "grad_norm": 0.46337488293647766,
      "learning_rate": 9.314687307089606e-05,
      "loss": 0.6615,
      "step": 112650
    },
    {
      "epoch": 1.6047273245051972,
      "grad_norm": 0.369361937046051,
      "learning_rate": 9.309938743530083e-05,
      "loss": 0.6259,
      "step": 112700
    },
    {
      "epoch": 1.6054392709668233,
      "grad_norm": 0.496025949716568,
      "learning_rate": 9.30519017997056e-05,
      "loss": 0.6687,
      "step": 112750
    },
    {
      "epoch": 1.6061512174284494,
      "grad_norm": 0.7344862818717957,
      "learning_rate": 9.300441616411037e-05,
      "loss": 0.6731,
      "step": 112800
    },
    {
      "epoch": 1.6068631638900754,
      "grad_norm": 0.5023550987243652,
      "learning_rate": 9.295693052851513e-05,
      "loss": 0.599,
      "step": 112850
    },
    {
      "epoch": 1.6075751103517015,
      "grad_norm": 0.4530620276927948,
      "learning_rate": 9.29094448929199e-05,
      "loss": 0.5763,
      "step": 112900
    },
    {
      "epoch": 1.6082870568133276,
      "grad_norm": 0.7504833936691284,
      "learning_rate": 9.286195925732466e-05,
      "loss": 0.6394,
      "step": 112950
    },
    {
      "epoch": 1.6089990032749537,
      "grad_norm": 0.7419711351394653,
      "learning_rate": 9.281447362172944e-05,
      "loss": 0.6666,
      "step": 113000
    },
    {
      "epoch": 1.6097109497365798,
      "grad_norm": 0.5845373868942261,
      "learning_rate": 9.27669879861342e-05,
      "loss": 0.6641,
      "step": 113050
    },
    {
      "epoch": 1.6104228961982059,
      "grad_norm": 0.539705753326416,
      "learning_rate": 9.271950235053897e-05,
      "loss": 0.6885,
      "step": 113100
    },
    {
      "epoch": 1.611134842659832,
      "grad_norm": 0.8765894174575806,
      "learning_rate": 9.267201671494373e-05,
      "loss": 0.6712,
      "step": 113150
    },
    {
      "epoch": 1.611846789121458,
      "grad_norm": 0.5428410172462463,
      "learning_rate": 9.26245310793485e-05,
      "loss": 0.598,
      "step": 113200
    },
    {
      "epoch": 1.6125587355830842,
      "grad_norm": 0.4781325161457062,
      "learning_rate": 9.257704544375327e-05,
      "loss": 0.7208,
      "step": 113250
    },
    {
      "epoch": 1.6132706820447102,
      "grad_norm": 0.41617313027381897,
      "learning_rate": 9.252955980815804e-05,
      "loss": 0.6345,
      "step": 113300
    },
    {
      "epoch": 1.6139826285063363,
      "grad_norm": 0.5347797870635986,
      "learning_rate": 9.24820741725628e-05,
      "loss": 0.5834,
      "step": 113350
    },
    {
      "epoch": 1.6146945749679624,
      "grad_norm": 0.5349180102348328,
      "learning_rate": 9.243458853696758e-05,
      "loss": 0.6336,
      "step": 113400
    },
    {
      "epoch": 1.6154065214295885,
      "grad_norm": 0.5108951926231384,
      "learning_rate": 9.238710290137234e-05,
      "loss": 0.6664,
      "step": 113450
    },
    {
      "epoch": 1.6161184678912146,
      "grad_norm": 0.9372195601463318,
      "learning_rate": 9.233961726577712e-05,
      "loss": 0.6088,
      "step": 113500
    },
    {
      "epoch": 1.6168304143528407,
      "grad_norm": 0.7345455288887024,
      "learning_rate": 9.229213163018188e-05,
      "loss": 0.6407,
      "step": 113550
    },
    {
      "epoch": 1.6175423608144668,
      "grad_norm": 0.558177649974823,
      "learning_rate": 9.224464599458665e-05,
      "loss": 0.6275,
      "step": 113600
    },
    {
      "epoch": 1.6182543072760929,
      "grad_norm": 0.5928058624267578,
      "learning_rate": 9.219716035899141e-05,
      "loss": 0.6499,
      "step": 113650
    },
    {
      "epoch": 1.618966253737719,
      "grad_norm": 0.32902956008911133,
      "learning_rate": 9.215062443610808e-05,
      "loss": 0.6277,
      "step": 113700
    },
    {
      "epoch": 1.619678200199345,
      "grad_norm": 0.560913622379303,
      "learning_rate": 9.210313880051284e-05,
      "loss": 0.7039,
      "step": 113750
    },
    {
      "epoch": 1.6203901466609711,
      "grad_norm": 0.5587758421897888,
      "learning_rate": 9.205565316491762e-05,
      "loss": 0.7269,
      "step": 113800
    },
    {
      "epoch": 1.6211020931225972,
      "grad_norm": 0.6686050891876221,
      "learning_rate": 9.200816752932238e-05,
      "loss": 0.6474,
      "step": 113850
    },
    {
      "epoch": 1.6218140395842233,
      "grad_norm": 0.36602717638015747,
      "learning_rate": 9.196068189372715e-05,
      "loss": 0.6095,
      "step": 113900
    },
    {
      "epoch": 1.6225259860458494,
      "grad_norm": 0.7869421243667603,
      "learning_rate": 9.191319625813192e-05,
      "loss": 0.647,
      "step": 113950
    },
    {
      "epoch": 1.6232379325074753,
      "grad_norm": 0.7419140338897705,
      "learning_rate": 9.186571062253669e-05,
      "loss": 0.6846,
      "step": 114000
    },
    {
      "epoch": 1.6239498789691016,
      "grad_norm": 0.6112070679664612,
      "learning_rate": 9.181822498694147e-05,
      "loss": 0.5836,
      "step": 114050
    },
    {
      "epoch": 1.6246618254307275,
      "grad_norm": 0.4595598578453064,
      "learning_rate": 9.177073935134623e-05,
      "loss": 0.6434,
      "step": 114100
    },
    {
      "epoch": 1.6253737718923538,
      "grad_norm": 0.5286964774131775,
      "learning_rate": 9.1723253715751e-05,
      "loss": 0.5996,
      "step": 114150
    },
    {
      "epoch": 1.6260857183539796,
      "grad_norm": 0.648982584476471,
      "learning_rate": 9.167576808015576e-05,
      "loss": 0.6365,
      "step": 114200
    },
    {
      "epoch": 1.626797664815606,
      "grad_norm": 0.5457524061203003,
      "learning_rate": 9.162923215727242e-05,
      "loss": 0.6406,
      "step": 114250
    },
    {
      "epoch": 1.6275096112772318,
      "grad_norm": 0.686779797077179,
      "learning_rate": 9.158174652167719e-05,
      "loss": 0.6307,
      "step": 114300
    },
    {
      "epoch": 1.6282215577388581,
      "grad_norm": 0.6018388271331787,
      "learning_rate": 9.153426088608197e-05,
      "loss": 0.6348,
      "step": 114350
    },
    {
      "epoch": 1.628933504200484,
      "grad_norm": 0.6644502282142639,
      "learning_rate": 9.148677525048673e-05,
      "loss": 0.6196,
      "step": 114400
    },
    {
      "epoch": 1.6296454506621103,
      "grad_norm": 0.690304160118103,
      "learning_rate": 9.143928961489149e-05,
      "loss": 0.6138,
      "step": 114450
    },
    {
      "epoch": 1.6303573971237362,
      "grad_norm": 0.6048271656036377,
      "learning_rate": 9.139180397929627e-05,
      "loss": 0.6842,
      "step": 114500
    },
    {
      "epoch": 1.6310693435853625,
      "grad_norm": 0.49392127990722656,
      "learning_rate": 9.134431834370103e-05,
      "loss": 0.6556,
      "step": 114550
    },
    {
      "epoch": 1.6317812900469884,
      "grad_norm": 0.5422094464302063,
      "learning_rate": 9.129683270810581e-05,
      "loss": 0.6095,
      "step": 114600
    },
    {
      "epoch": 1.6324932365086147,
      "grad_norm": 0.4145653247833252,
      "learning_rate": 9.124934707251058e-05,
      "loss": 0.6272,
      "step": 114650
    },
    {
      "epoch": 1.6332051829702405,
      "grad_norm": 0.607204020023346,
      "learning_rate": 9.120186143691534e-05,
      "loss": 0.6676,
      "step": 114700
    },
    {
      "epoch": 1.6339171294318668,
      "grad_norm": 0.7043821215629578,
      "learning_rate": 9.11543758013201e-05,
      "loss": 0.6452,
      "step": 114750
    },
    {
      "epoch": 1.6346290758934927,
      "grad_norm": 0.5238395929336548,
      "learning_rate": 9.110689016572487e-05,
      "loss": 0.6444,
      "step": 114800
    },
    {
      "epoch": 1.635341022355119,
      "grad_norm": 0.5944644212722778,
      "learning_rate": 9.105940453012965e-05,
      "loss": 0.6466,
      "step": 114850
    },
    {
      "epoch": 1.636052968816745,
      "grad_norm": 0.645121157169342,
      "learning_rate": 9.101191889453441e-05,
      "loss": 0.6362,
      "step": 114900
    },
    {
      "epoch": 1.6367649152783712,
      "grad_norm": 0.4229893684387207,
      "learning_rate": 9.096443325893917e-05,
      "loss": 0.6148,
      "step": 114950
    },
    {
      "epoch": 1.637476861739997,
      "grad_norm": 0.4857901930809021,
      "learning_rate": 9.091694762334394e-05,
      "loss": 0.6636,
      "step": 115000
    },
    {
      "epoch": 1.6381888082016234,
      "grad_norm": 0.6199926733970642,
      "learning_rate": 9.08694619877487e-05,
      "loss": 0.5989,
      "step": 115050
    },
    {
      "epoch": 1.6389007546632492,
      "grad_norm": 0.5364912152290344,
      "learning_rate": 9.082197635215348e-05,
      "loss": 0.5973,
      "step": 115100
    },
    {
      "epoch": 1.6396127011248756,
      "grad_norm": 0.41407510638237,
      "learning_rate": 9.077449071655824e-05,
      "loss": 0.5949,
      "step": 115150
    },
    {
      "epoch": 1.6403246475865014,
      "grad_norm": 0.7842774987220764,
      "learning_rate": 9.072700508096301e-05,
      "loss": 0.5883,
      "step": 115200
    },
    {
      "epoch": 1.6410365940481277,
      "grad_norm": 0.4580443501472473,
      "learning_rate": 9.067951944536777e-05,
      "loss": 0.6712,
      "step": 115250
    },
    {
      "epoch": 1.6417485405097536,
      "grad_norm": 0.4131211042404175,
      "learning_rate": 9.063203380977254e-05,
      "loss": 0.5962,
      "step": 115300
    },
    {
      "epoch": 1.64246048697138,
      "grad_norm": 0.5482211112976074,
      "learning_rate": 9.058454817417731e-05,
      "loss": 0.6201,
      "step": 115350
    },
    {
      "epoch": 1.6431724334330058,
      "grad_norm": 0.7256815433502197,
      "learning_rate": 9.053706253858208e-05,
      "loss": 0.6789,
      "step": 115400
    },
    {
      "epoch": 1.6438843798946319,
      "grad_norm": 0.6033368706703186,
      "learning_rate": 9.048957690298686e-05,
      "loss": 0.631,
      "step": 115450
    },
    {
      "epoch": 1.644596326356258,
      "grad_norm": 0.3986751139163971,
      "learning_rate": 9.044209126739162e-05,
      "loss": 0.6435,
      "step": 115500
    },
    {
      "epoch": 1.645308272817884,
      "grad_norm": 0.6879646182060242,
      "learning_rate": 9.039460563179638e-05,
      "loss": 0.6209,
      "step": 115550
    },
    {
      "epoch": 1.6460202192795101,
      "grad_norm": 0.7386277318000793,
      "learning_rate": 9.034711999620116e-05,
      "loss": 0.6465,
      "step": 115600
    },
    {
      "epoch": 1.6467321657411362,
      "grad_norm": 0.6477791666984558,
      "learning_rate": 9.029963436060593e-05,
      "loss": 0.6446,
      "step": 115650
    },
    {
      "epoch": 1.6474441122027623,
      "grad_norm": 0.5172969102859497,
      "learning_rate": 9.025214872501069e-05,
      "loss": 0.6611,
      "step": 115700
    },
    {
      "epoch": 1.6481560586643884,
      "grad_norm": 0.6497918367385864,
      "learning_rate": 9.020466308941545e-05,
      "loss": 0.6448,
      "step": 115750
    },
    {
      "epoch": 1.6488680051260145,
      "grad_norm": 0.5530678629875183,
      "learning_rate": 9.015717745382022e-05,
      "loss": 0.6419,
      "step": 115800
    },
    {
      "epoch": 1.6495799515876406,
      "grad_norm": 0.6616988182067871,
      "learning_rate": 9.0109691818225e-05,
      "loss": 0.6749,
      "step": 115850
    },
    {
      "epoch": 1.6502918980492667,
      "grad_norm": 0.4836616814136505,
      "learning_rate": 9.006220618262976e-05,
      "loss": 0.6571,
      "step": 115900
    },
    {
      "epoch": 1.6510038445108928,
      "grad_norm": 0.458547443151474,
      "learning_rate": 9.001472054703452e-05,
      "loss": 0.6142,
      "step": 115950
    },
    {
      "epoch": 1.6517157909725189,
      "grad_norm": 0.4796895682811737,
      "learning_rate": 8.996723491143929e-05,
      "loss": 0.6503,
      "step": 116000
    },
    {
      "epoch": 1.652427737434145,
      "grad_norm": 0.521084189414978,
      "learning_rate": 8.991974927584405e-05,
      "loss": 0.6232,
      "step": 116050
    },
    {
      "epoch": 1.653139683895771,
      "grad_norm": 0.4094080924987793,
      "learning_rate": 8.987226364024883e-05,
      "loss": 0.6682,
      "step": 116100
    },
    {
      "epoch": 1.6538516303573971,
      "grad_norm": 0.48606494069099426,
      "learning_rate": 8.982477800465359e-05,
      "loss": 0.6282,
      "step": 116150
    },
    {
      "epoch": 1.6545635768190232,
      "grad_norm": 0.5104541182518005,
      "learning_rate": 8.977729236905836e-05,
      "loss": 0.6929,
      "step": 116200
    },
    {
      "epoch": 1.6552755232806493,
      "grad_norm": 0.3670310378074646,
      "learning_rate": 8.972980673346313e-05,
      "loss": 0.6536,
      "step": 116250
    },
    {
      "epoch": 1.6559874697422754,
      "grad_norm": 0.5967168211936951,
      "learning_rate": 8.96823210978679e-05,
      "loss": 0.6642,
      "step": 116300
    },
    {
      "epoch": 1.6566994162039015,
      "grad_norm": 0.6992875933647156,
      "learning_rate": 8.963483546227268e-05,
      "loss": 0.5739,
      "step": 116350
    },
    {
      "epoch": 1.6574113626655276,
      "grad_norm": 0.35802769660949707,
      "learning_rate": 8.958734982667744e-05,
      "loss": 0.6144,
      "step": 116400
    },
    {
      "epoch": 1.6581233091271537,
      "grad_norm": 0.4771137237548828,
      "learning_rate": 8.95408139037941e-05,
      "loss": 0.6513,
      "step": 116450
    },
    {
      "epoch": 1.6588352555887798,
      "grad_norm": 0.7054080367088318,
      "learning_rate": 8.949332826819887e-05,
      "loss": 0.6435,
      "step": 116500
    },
    {
      "epoch": 1.6595472020504058,
      "grad_norm": 0.5045498609542847,
      "learning_rate": 8.944584263260363e-05,
      "loss": 0.6746,
      "step": 116550
    },
    {
      "epoch": 1.660259148512032,
      "grad_norm": 0.575502336025238,
      "learning_rate": 8.939835699700841e-05,
      "loss": 0.5966,
      "step": 116600
    },
    {
      "epoch": 1.660971094973658,
      "grad_norm": 0.5181990265846252,
      "learning_rate": 8.935087136141318e-05,
      "loss": 0.5977,
      "step": 116650
    },
    {
      "epoch": 1.6616830414352841,
      "grad_norm": 0.6218400001525879,
      "learning_rate": 8.930338572581794e-05,
      "loss": 0.6349,
      "step": 116700
    },
    {
      "epoch": 1.6623949878969102,
      "grad_norm": 0.4785463511943817,
      "learning_rate": 8.92559000902227e-05,
      "loss": 0.6319,
      "step": 116750
    },
    {
      "epoch": 1.6631069343585363,
      "grad_norm": 0.47006484866142273,
      "learning_rate": 8.920841445462748e-05,
      "loss": 0.6839,
      "step": 116800
    },
    {
      "epoch": 1.6638188808201622,
      "grad_norm": 0.495252400636673,
      "learning_rate": 8.916092881903225e-05,
      "loss": 0.6728,
      "step": 116850
    },
    {
      "epoch": 1.6645308272817885,
      "grad_norm": 0.7704550623893738,
      "learning_rate": 8.911344318343702e-05,
      "loss": 0.6501,
      "step": 116900
    },
    {
      "epoch": 1.6652427737434143,
      "grad_norm": 0.7709336876869202,
      "learning_rate": 8.906595754784179e-05,
      "loss": 0.6403,
      "step": 116950
    },
    {
      "epoch": 1.6659547202050407,
      "grad_norm": 0.5688784122467041,
      "learning_rate": 8.901847191224655e-05,
      "loss": 0.6266,
      "step": 117000
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.6077351570129395,
      "learning_rate": 8.897098627665132e-05,
      "loss": 0.6281,
      "step": 117050
    },
    {
      "epoch": 1.6673786131282928,
      "grad_norm": 0.7677366733551025,
      "learning_rate": 8.892350064105609e-05,
      "loss": 0.6348,
      "step": 117100
    },
    {
      "epoch": 1.6680905595899187,
      "grad_norm": 0.5520159006118774,
      "learning_rate": 8.887601500546086e-05,
      "loss": 0.6401,
      "step": 117150
    },
    {
      "epoch": 1.668802506051545,
      "grad_norm": 0.6822079420089722,
      "learning_rate": 8.882852936986562e-05,
      "loss": 0.6544,
      "step": 117200
    },
    {
      "epoch": 1.6695144525131709,
      "grad_norm": 0.4759795665740967,
      "learning_rate": 8.878104373427039e-05,
      "loss": 0.6724,
      "step": 117250
    },
    {
      "epoch": 1.6702263989747972,
      "grad_norm": 0.5242370367050171,
      "learning_rate": 8.873355809867515e-05,
      "loss": 0.6858,
      "step": 117300
    },
    {
      "epoch": 1.670938345436423,
      "grad_norm": 0.566231369972229,
      "learning_rate": 8.868607246307993e-05,
      "loss": 0.6427,
      "step": 117350
    },
    {
      "epoch": 1.6716502918980494,
      "grad_norm": 0.6460756063461304,
      "learning_rate": 8.863858682748469e-05,
      "loss": 0.636,
      "step": 117400
    },
    {
      "epoch": 1.6723622383596752,
      "grad_norm": 0.29916349053382874,
      "learning_rate": 8.859110119188946e-05,
      "loss": 0.6617,
      "step": 117450
    },
    {
      "epoch": 1.6730741848213015,
      "grad_norm": 0.6950064897537231,
      "learning_rate": 8.854361555629422e-05,
      "loss": 0.6132,
      "step": 117500
    },
    {
      "epoch": 1.6737861312829274,
      "grad_norm": 0.6762996912002563,
      "learning_rate": 8.849612992069898e-05,
      "loss": 0.661,
      "step": 117550
    },
    {
      "epoch": 1.6744980777445537,
      "grad_norm": 0.5652352571487427,
      "learning_rate": 8.844864428510376e-05,
      "loss": 0.6608,
      "step": 117600
    },
    {
      "epoch": 1.6752100242061796,
      "grad_norm": 0.7926605343818665,
      "learning_rate": 8.840115864950852e-05,
      "loss": 0.6028,
      "step": 117650
    },
    {
      "epoch": 1.675921970667806,
      "grad_norm": 0.5372695326805115,
      "learning_rate": 8.83536730139133e-05,
      "loss": 0.6287,
      "step": 117700
    },
    {
      "epoch": 1.6766339171294318,
      "grad_norm": 0.5090484619140625,
      "learning_rate": 8.830618737831807e-05,
      "loss": 0.6715,
      "step": 117750
    },
    {
      "epoch": 1.677345863591058,
      "grad_norm": 0.41716036200523376,
      "learning_rate": 8.825870174272283e-05,
      "loss": 0.6254,
      "step": 117800
    },
    {
      "epoch": 1.678057810052684,
      "grad_norm": 0.44204768538475037,
      "learning_rate": 8.821121610712761e-05,
      "loss": 0.6035,
      "step": 117850
    },
    {
      "epoch": 1.6787697565143103,
      "grad_norm": 0.3803543150424957,
      "learning_rate": 8.816373047153237e-05,
      "loss": 0.6972,
      "step": 117900
    },
    {
      "epoch": 1.6794817029759361,
      "grad_norm": 0.508154571056366,
      "learning_rate": 8.811624483593714e-05,
      "loss": 0.6476,
      "step": 117950
    },
    {
      "epoch": 1.6801936494375624,
      "grad_norm": 0.7904713153839111,
      "learning_rate": 8.80687592003419e-05,
      "loss": 0.6906,
      "step": 118000
    },
    {
      "epoch": 1.6809055958991883,
      "grad_norm": 0.582869291305542,
      "learning_rate": 8.802127356474666e-05,
      "loss": 0.6337,
      "step": 118050
    },
    {
      "epoch": 1.6816175423608146,
      "grad_norm": 0.4355394244194031,
      "learning_rate": 8.797378792915144e-05,
      "loss": 0.6802,
      "step": 118100
    },
    {
      "epoch": 1.6823294888224405,
      "grad_norm": 0.5941400527954102,
      "learning_rate": 8.79263022935562e-05,
      "loss": 0.6373,
      "step": 118150
    },
    {
      "epoch": 1.6830414352840668,
      "grad_norm": 0.6460815072059631,
      "learning_rate": 8.787881665796097e-05,
      "loss": 0.6589,
      "step": 118200
    },
    {
      "epoch": 1.6837533817456927,
      "grad_norm": 0.7533362507820129,
      "learning_rate": 8.783133102236573e-05,
      "loss": 0.6837,
      "step": 118250
    },
    {
      "epoch": 1.6844653282073188,
      "grad_norm": 0.6839137077331543,
      "learning_rate": 8.77838453867705e-05,
      "loss": 0.6382,
      "step": 118300
    },
    {
      "epoch": 1.6851772746689448,
      "grad_norm": 0.7244088053703308,
      "learning_rate": 8.773635975117528e-05,
      "loss": 0.6869,
      "step": 118350
    },
    {
      "epoch": 1.685889221130571,
      "grad_norm": 0.5257143974304199,
      "learning_rate": 8.768887411558004e-05,
      "loss": 0.648,
      "step": 118400
    },
    {
      "epoch": 1.686601167592197,
      "grad_norm": 0.6631689071655273,
      "learning_rate": 8.76413884799848e-05,
      "loss": 0.6735,
      "step": 118450
    },
    {
      "epoch": 1.6873131140538231,
      "grad_norm": 0.4854840934276581,
      "learning_rate": 8.759390284438958e-05,
      "loss": 0.6343,
      "step": 118500
    },
    {
      "epoch": 1.6880250605154492,
      "grad_norm": 0.5982183218002319,
      "learning_rate": 8.754641720879435e-05,
      "loss": 0.619,
      "step": 118550
    },
    {
      "epoch": 1.6887370069770753,
      "grad_norm": 0.6428471803665161,
      "learning_rate": 8.749893157319912e-05,
      "loss": 0.6329,
      "step": 118600
    },
    {
      "epoch": 1.6894489534387014,
      "grad_norm": 0.5215801000595093,
      "learning_rate": 8.745144593760389e-05,
      "loss": 0.5869,
      "step": 118650
    },
    {
      "epoch": 1.6901608999003275,
      "grad_norm": 0.617715060710907,
      "learning_rate": 8.740396030200865e-05,
      "loss": 0.6528,
      "step": 118700
    },
    {
      "epoch": 1.6908728463619536,
      "grad_norm": 0.5986242294311523,
      "learning_rate": 8.735647466641342e-05,
      "loss": 0.6087,
      "step": 118750
    },
    {
      "epoch": 1.6915847928235797,
      "grad_norm": 0.5596445798873901,
      "learning_rate": 8.730898903081818e-05,
      "loss": 0.6496,
      "step": 118800
    },
    {
      "epoch": 1.6922967392852057,
      "grad_norm": 0.7501165270805359,
      "learning_rate": 8.726150339522296e-05,
      "loss": 0.6106,
      "step": 118850
    },
    {
      "epoch": 1.6930086857468318,
      "grad_norm": 0.5431445837020874,
      "learning_rate": 8.721401775962772e-05,
      "loss": 0.6227,
      "step": 118900
    },
    {
      "epoch": 1.693720632208458,
      "grad_norm": 0.46617963910102844,
      "learning_rate": 8.716653212403249e-05,
      "loss": 0.6411,
      "step": 118950
    },
    {
      "epoch": 1.694432578670084,
      "grad_norm": 0.5569748282432556,
      "learning_rate": 8.711999620114915e-05,
      "loss": 0.6871,
      "step": 119000
    },
    {
      "epoch": 1.69514452513171,
      "grad_norm": 0.4415375292301178,
      "learning_rate": 8.707251056555393e-05,
      "loss": 0.5888,
      "step": 119050
    },
    {
      "epoch": 1.6958564715933362,
      "grad_norm": 0.571386992931366,
      "learning_rate": 8.702502492995869e-05,
      "loss": 0.6078,
      "step": 119100
    },
    {
      "epoch": 1.6965684180549623,
      "grad_norm": 0.6027116179466248,
      "learning_rate": 8.697753929436347e-05,
      "loss": 0.6205,
      "step": 119150
    },
    {
      "epoch": 1.6972803645165884,
      "grad_norm": 0.5349936485290527,
      "learning_rate": 8.693005365876823e-05,
      "loss": 0.6232,
      "step": 119200
    },
    {
      "epoch": 1.6979923109782145,
      "grad_norm": 0.5438159704208374,
      "learning_rate": 8.6882568023173e-05,
      "loss": 0.6819,
      "step": 119250
    },
    {
      "epoch": 1.6987042574398405,
      "grad_norm": 0.720363974571228,
      "learning_rate": 8.683508238757776e-05,
      "loss": 0.6119,
      "step": 119300
    },
    {
      "epoch": 1.6994162039014666,
      "grad_norm": 0.5369024276733398,
      "learning_rate": 8.678759675198253e-05,
      "loss": 0.6438,
      "step": 119350
    },
    {
      "epoch": 1.7001281503630927,
      "grad_norm": 0.5200456380844116,
      "learning_rate": 8.67401111163873e-05,
      "loss": 0.617,
      "step": 119400
    },
    {
      "epoch": 1.7008400968247188,
      "grad_norm": 0.4409809112548828,
      "learning_rate": 8.669262548079207e-05,
      "loss": 0.684,
      "step": 119450
    },
    {
      "epoch": 1.701552043286345,
      "grad_norm": 0.4397864043712616,
      "learning_rate": 8.664513984519683e-05,
      "loss": 0.6259,
      "step": 119500
    },
    {
      "epoch": 1.702263989747971,
      "grad_norm": 0.48468995094299316,
      "learning_rate": 8.65976542096016e-05,
      "loss": 0.6272,
      "step": 119550
    },
    {
      "epoch": 1.702975936209597,
      "grad_norm": 0.5462748408317566,
      "learning_rate": 8.655016857400636e-05,
      "loss": 0.6113,
      "step": 119600
    },
    {
      "epoch": 1.7036878826712232,
      "grad_norm": 0.520273745059967,
      "learning_rate": 8.650268293841114e-05,
      "loss": 0.6835,
      "step": 119650
    },
    {
      "epoch": 1.7043998291328493,
      "grad_norm": 0.560842752456665,
      "learning_rate": 8.64551973028159e-05,
      "loss": 0.6552,
      "step": 119700
    },
    {
      "epoch": 1.7051117755944754,
      "grad_norm": 0.696249783039093,
      "learning_rate": 8.640771166722067e-05,
      "loss": 0.6414,
      "step": 119750
    },
    {
      "epoch": 1.7058237220561012,
      "grad_norm": 0.6534599661827087,
      "learning_rate": 8.636022603162543e-05,
      "loss": 0.7182,
      "step": 119800
    },
    {
      "epoch": 1.7065356685177275,
      "grad_norm": 0.5934247374534607,
      "learning_rate": 8.63127403960302e-05,
      "loss": 0.6549,
      "step": 119850
    },
    {
      "epoch": 1.7072476149793534,
      "grad_norm": 0.5281415581703186,
      "learning_rate": 8.626525476043497e-05,
      "loss": 0.7034,
      "step": 119900
    },
    {
      "epoch": 1.7079595614409797,
      "grad_norm": 0.5102788805961609,
      "learning_rate": 8.621776912483974e-05,
      "loss": 0.6581,
      "step": 119950
    },
    {
      "epoch": 1.7086715079026056,
      "grad_norm": 0.5024833083152771,
      "learning_rate": 8.617028348924451e-05,
      "loss": 0.6637,
      "step": 120000
    },
    {
      "epoch": 1.709383454364232,
      "grad_norm": 0.5864351987838745,
      "learning_rate": 8.612279785364928e-05,
      "loss": 0.6296,
      "step": 120050
    },
    {
      "epoch": 1.7100954008258578,
      "grad_norm": 0.5121029019355774,
      "learning_rate": 8.607531221805404e-05,
      "loss": 0.6671,
      "step": 120100
    },
    {
      "epoch": 1.710807347287484,
      "grad_norm": 0.35602104663848877,
      "learning_rate": 8.602782658245882e-05,
      "loss": 0.6167,
      "step": 120150
    },
    {
      "epoch": 1.71151929374911,
      "grad_norm": 0.7153968811035156,
      "learning_rate": 8.598034094686358e-05,
      "loss": 0.6492,
      "step": 120200
    },
    {
      "epoch": 1.7122312402107362,
      "grad_norm": 0.6816487312316895,
      "learning_rate": 8.593285531126835e-05,
      "loss": 0.6396,
      "step": 120250
    },
    {
      "epoch": 1.7129431866723621,
      "grad_norm": 0.6584382653236389,
      "learning_rate": 8.588536967567311e-05,
      "loss": 0.6386,
      "step": 120300
    },
    {
      "epoch": 1.7136551331339884,
      "grad_norm": 0.3033919930458069,
      "learning_rate": 8.583788404007788e-05,
      "loss": 0.6322,
      "step": 120350
    },
    {
      "epoch": 1.7143670795956143,
      "grad_norm": 0.417285680770874,
      "learning_rate": 8.579039840448265e-05,
      "loss": 0.5995,
      "step": 120400
    },
    {
      "epoch": 1.7150790260572406,
      "grad_norm": 0.551588237285614,
      "learning_rate": 8.574291276888742e-05,
      "loss": 0.6952,
      "step": 120450
    },
    {
      "epoch": 1.7157909725188665,
      "grad_norm": 0.7495619654655457,
      "learning_rate": 8.569542713329218e-05,
      "loss": 0.6801,
      "step": 120500
    },
    {
      "epoch": 1.7165029189804928,
      "grad_norm": 0.7647082805633545,
      "learning_rate": 8.564794149769695e-05,
      "loss": 0.6083,
      "step": 120550
    },
    {
      "epoch": 1.7172148654421187,
      "grad_norm": 0.6849957704544067,
      "learning_rate": 8.560045586210171e-05,
      "loss": 0.6018,
      "step": 120600
    },
    {
      "epoch": 1.717926811903745,
      "grad_norm": 0.7615816593170166,
      "learning_rate": 8.555297022650649e-05,
      "loss": 0.6808,
      "step": 120650
    },
    {
      "epoch": 1.7186387583653708,
      "grad_norm": 0.46804943680763245,
      "learning_rate": 8.550548459091125e-05,
      "loss": 0.6105,
      "step": 120700
    },
    {
      "epoch": 1.7193507048269971,
      "grad_norm": 0.4019971787929535,
      "learning_rate": 8.545799895531601e-05,
      "loss": 0.6389,
      "step": 120750
    },
    {
      "epoch": 1.720062651288623,
      "grad_norm": 0.4986862540245056,
      "learning_rate": 8.541051331972079e-05,
      "loss": 0.6182,
      "step": 120800
    },
    {
      "epoch": 1.7207745977502493,
      "grad_norm": 0.46607983112335205,
      "learning_rate": 8.536302768412556e-05,
      "loss": 0.6151,
      "step": 120850
    },
    {
      "epoch": 1.7214865442118752,
      "grad_norm": 0.45372751355171204,
      "learning_rate": 8.531554204853033e-05,
      "loss": 0.6608,
      "step": 120900
    },
    {
      "epoch": 1.7221984906735015,
      "grad_norm": 0.7623787522315979,
      "learning_rate": 8.52680564129351e-05,
      "loss": 0.6597,
      "step": 120950
    },
    {
      "epoch": 1.7229104371351274,
      "grad_norm": 0.5748289823532104,
      "learning_rate": 8.522057077733986e-05,
      "loss": 0.6472,
      "step": 121000
    },
    {
      "epoch": 1.7236223835967537,
      "grad_norm": 0.518332839012146,
      "learning_rate": 8.517308514174463e-05,
      "loss": 0.6298,
      "step": 121050
    },
    {
      "epoch": 1.7243343300583795,
      "grad_norm": 0.6216704845428467,
      "learning_rate": 8.512559950614939e-05,
      "loss": 0.6294,
      "step": 121100
    },
    {
      "epoch": 1.7250462765200059,
      "grad_norm": 0.3715609312057495,
      "learning_rate": 8.507811387055417e-05,
      "loss": 0.6154,
      "step": 121150
    },
    {
      "epoch": 1.7257582229816317,
      "grad_norm": 0.5693736672401428,
      "learning_rate": 8.503062823495893e-05,
      "loss": 0.6228,
      "step": 121200
    },
    {
      "epoch": 1.7264701694432578,
      "grad_norm": 0.6356095671653748,
      "learning_rate": 8.49831425993637e-05,
      "loss": 0.7017,
      "step": 121250
    },
    {
      "epoch": 1.727182115904884,
      "grad_norm": 0.6334056854248047,
      "learning_rate": 8.493565696376846e-05,
      "loss": 0.6434,
      "step": 121300
    },
    {
      "epoch": 1.72789406236651,
      "grad_norm": 0.48259541392326355,
      "learning_rate": 8.488817132817322e-05,
      "loss": 0.6721,
      "step": 121350
    },
    {
      "epoch": 1.728606008828136,
      "grad_norm": 0.5386264324188232,
      "learning_rate": 8.4840685692578e-05,
      "loss": 0.6784,
      "step": 121400
    },
    {
      "epoch": 1.7293179552897622,
      "grad_norm": 0.7618592977523804,
      "learning_rate": 8.479320005698277e-05,
      "loss": 0.633,
      "step": 121450
    },
    {
      "epoch": 1.7300299017513883,
      "grad_norm": 0.5797138810157776,
      "learning_rate": 8.474571442138753e-05,
      "loss": 0.6145,
      "step": 121500
    },
    {
      "epoch": 1.7307418482130144,
      "grad_norm": 0.5228675603866577,
      "learning_rate": 8.46982287857923e-05,
      "loss": 0.6123,
      "step": 121550
    },
    {
      "epoch": 1.7314537946746404,
      "grad_norm": 0.730922281742096,
      "learning_rate": 8.465074315019707e-05,
      "loss": 0.6587,
      "step": 121600
    },
    {
      "epoch": 1.7321657411362665,
      "grad_norm": 0.5388955473899841,
      "learning_rate": 8.460325751460184e-05,
      "loss": 0.6779,
      "step": 121650
    },
    {
      "epoch": 1.7328776875978926,
      "grad_norm": 0.47140318155288696,
      "learning_rate": 8.455577187900661e-05,
      "loss": 0.6388,
      "step": 121700
    },
    {
      "epoch": 1.7335896340595187,
      "grad_norm": 0.4640016555786133,
      "learning_rate": 8.450828624341138e-05,
      "loss": 0.6633,
      "step": 121750
    },
    {
      "epoch": 1.7343015805211448,
      "grad_norm": 0.7348268628120422,
      "learning_rate": 8.446080060781614e-05,
      "loss": 0.5944,
      "step": 121800
    },
    {
      "epoch": 1.735013526982771,
      "grad_norm": 0.4517475962638855,
      "learning_rate": 8.44133149722209e-05,
      "loss": 0.6389,
      "step": 121850
    },
    {
      "epoch": 1.735725473444397,
      "grad_norm": 0.4993293285369873,
      "learning_rate": 8.436582933662568e-05,
      "loss": 0.6168,
      "step": 121900
    },
    {
      "epoch": 1.736437419906023,
      "grad_norm": 0.6983484029769897,
      "learning_rate": 8.431834370103045e-05,
      "loss": 0.6558,
      "step": 121950
    },
    {
      "epoch": 1.7371493663676492,
      "grad_norm": 0.469756156206131,
      "learning_rate": 8.427085806543521e-05,
      "loss": 0.6212,
      "step": 122000
    },
    {
      "epoch": 1.7378613128292753,
      "grad_norm": 0.4770670533180237,
      "learning_rate": 8.422337242983998e-05,
      "loss": 0.6969,
      "step": 122050
    },
    {
      "epoch": 1.7385732592909013,
      "grad_norm": 0.8046549558639526,
      "learning_rate": 8.417588679424474e-05,
      "loss": 0.5983,
      "step": 122100
    },
    {
      "epoch": 1.7392852057525274,
      "grad_norm": 0.5927556157112122,
      "learning_rate": 8.412840115864952e-05,
      "loss": 0.6518,
      "step": 122150
    },
    {
      "epoch": 1.7399971522141535,
      "grad_norm": 0.5391789078712463,
      "learning_rate": 8.408091552305428e-05,
      "loss": 0.5889,
      "step": 122200
    },
    {
      "epoch": 1.7407090986757796,
      "grad_norm": 0.598961591720581,
      "learning_rate": 8.403342988745904e-05,
      "loss": 0.5803,
      "step": 122250
    },
    {
      "epoch": 1.7414210451374057,
      "grad_norm": 0.7699038982391357,
      "learning_rate": 8.398594425186381e-05,
      "loss": 0.5863,
      "step": 122300
    },
    {
      "epoch": 1.7421329915990318,
      "grad_norm": 0.4863971471786499,
      "learning_rate": 8.393845861626857e-05,
      "loss": 0.6418,
      "step": 122350
    },
    {
      "epoch": 1.7428449380606579,
      "grad_norm": 0.5422697067260742,
      "learning_rate": 8.389097298067335e-05,
      "loss": 0.6664,
      "step": 122400
    },
    {
      "epoch": 1.743556884522284,
      "grad_norm": 0.6352231502532959,
      "learning_rate": 8.384348734507811e-05,
      "loss": 0.6406,
      "step": 122450
    },
    {
      "epoch": 1.74426883098391,
      "grad_norm": 0.8848286271095276,
      "learning_rate": 8.379600170948288e-05,
      "loss": 0.6448,
      "step": 122500
    },
    {
      "epoch": 1.7449807774455361,
      "grad_norm": 0.5836386680603027,
      "learning_rate": 8.374851607388766e-05,
      "loss": 0.6679,
      "step": 122550
    },
    {
      "epoch": 1.7456927239071622,
      "grad_norm": 0.5745699405670166,
      "learning_rate": 8.370103043829242e-05,
      "loss": 0.6887,
      "step": 122600
    },
    {
      "epoch": 1.746404670368788,
      "grad_norm": 0.5610843300819397,
      "learning_rate": 8.36535448026972e-05,
      "loss": 0.6825,
      "step": 122650
    },
    {
      "epoch": 1.7471166168304144,
      "grad_norm": 0.77784264087677,
      "learning_rate": 8.360605916710196e-05,
      "loss": 0.6808,
      "step": 122700
    },
    {
      "epoch": 1.7478285632920403,
      "grad_norm": 0.5509790182113647,
      "learning_rate": 8.355857353150673e-05,
      "loss": 0.6367,
      "step": 122750
    },
    {
      "epoch": 1.7485405097536666,
      "grad_norm": 0.6401721239089966,
      "learning_rate": 8.351108789591149e-05,
      "loss": 0.6385,
      "step": 122800
    },
    {
      "epoch": 1.7492524562152925,
      "grad_norm": 0.544183611869812,
      "learning_rate": 8.346360226031625e-05,
      "loss": 0.6517,
      "step": 122850
    },
    {
      "epoch": 1.7499644026769188,
      "grad_norm": 0.801080048084259,
      "learning_rate": 8.341611662472103e-05,
      "loss": 0.6804,
      "step": 122900
    },
    {
      "epoch": 1.7506763491385446,
      "grad_norm": 0.5910623669624329,
      "learning_rate": 8.33686309891258e-05,
      "loss": 0.5596,
      "step": 122950
    },
    {
      "epoch": 1.751388295600171,
      "grad_norm": 0.6944693922996521,
      "learning_rate": 8.332114535353056e-05,
      "loss": 0.5569,
      "step": 123000
    },
    {
      "epoch": 1.7521002420617968,
      "grad_norm": Infinity,
      "learning_rate": 8.327460943064723e-05,
      "loss": 0.654,
      "step": 123050
    },
    {
      "epoch": 1.7528121885234231,
      "grad_norm": 0.5472902059555054,
      "learning_rate": 8.3227123795052e-05,
      "loss": 0.6457,
      "step": 123100
    },
    {
      "epoch": 1.753524134985049,
      "grad_norm": 0.7704774737358093,
      "learning_rate": 8.317963815945677e-05,
      "loss": 0.6233,
      "step": 123150
    },
    {
      "epoch": 1.7542360814466753,
      "grad_norm": 1.1395111083984375,
      "learning_rate": 8.313215252386155e-05,
      "loss": 0.6141,
      "step": 123200
    },
    {
      "epoch": 1.7549480279083012,
      "grad_norm": 0.4759853184223175,
      "learning_rate": 8.308466688826631e-05,
      "loss": 0.6801,
      "step": 123250
    },
    {
      "epoch": 1.7556599743699275,
      "grad_norm": 0.6155783534049988,
      "learning_rate": 8.303718125267107e-05,
      "loss": 0.7154,
      "step": 123300
    },
    {
      "epoch": 1.7563719208315534,
      "grad_norm": 0.47975990176200867,
      "learning_rate": 8.298969561707584e-05,
      "loss": 0.6296,
      "step": 123350
    },
    {
      "epoch": 1.7570838672931797,
      "grad_norm": 0.5694675445556641,
      "learning_rate": 8.29422099814806e-05,
      "loss": 0.6942,
      "step": 123400
    },
    {
      "epoch": 1.7577958137548055,
      "grad_norm": 0.47339925169944763,
      "learning_rate": 8.289472434588538e-05,
      "loss": 0.6431,
      "step": 123450
    },
    {
      "epoch": 1.7585077602164318,
      "grad_norm": 0.585513710975647,
      "learning_rate": 8.284723871029014e-05,
      "loss": 0.6082,
      "step": 123500
    },
    {
      "epoch": 1.7592197066780577,
      "grad_norm": 0.522605836391449,
      "learning_rate": 8.279975307469491e-05,
      "loss": 0.6653,
      "step": 123550
    },
    {
      "epoch": 1.759931653139684,
      "grad_norm": 0.6458973288536072,
      "learning_rate": 8.275226743909967e-05,
      "loss": 0.6205,
      "step": 123600
    },
    {
      "epoch": 1.76064359960131,
      "grad_norm": 0.35187914967536926,
      "learning_rate": 8.270478180350444e-05,
      "loss": 0.609,
      "step": 123650
    },
    {
      "epoch": 1.7613555460629362,
      "grad_norm": 0.5261726975440979,
      "learning_rate": 8.265729616790921e-05,
      "loss": 0.632,
      "step": 123700
    },
    {
      "epoch": 1.762067492524562,
      "grad_norm": 0.733101487159729,
      "learning_rate": 8.260981053231398e-05,
      "loss": 0.6604,
      "step": 123750
    },
    {
      "epoch": 1.7627794389861884,
      "grad_norm": 0.5045645833015442,
      "learning_rate": 8.256232489671874e-05,
      "loss": 0.6374,
      "step": 123800
    },
    {
      "epoch": 1.7634913854478143,
      "grad_norm": 0.4426218867301941,
      "learning_rate": 8.25148392611235e-05,
      "loss": 0.6052,
      "step": 123850
    },
    {
      "epoch": 1.7642033319094406,
      "grad_norm": 0.4540708363056183,
      "learning_rate": 8.246735362552828e-05,
      "loss": 0.6401,
      "step": 123900
    },
    {
      "epoch": 1.7649152783710664,
      "grad_norm": 0.7858185172080994,
      "learning_rate": 8.241986798993305e-05,
      "loss": 0.6381,
      "step": 123950
    },
    {
      "epoch": 1.7656272248326927,
      "grad_norm": 0.5499414205551147,
      "learning_rate": 8.237238235433782e-05,
      "loss": 0.685,
      "step": 124000
    },
    {
      "epoch": 1.7663391712943186,
      "grad_norm": 0.7395838499069214,
      "learning_rate": 8.232489671874259e-05,
      "loss": 0.6322,
      "step": 124050
    },
    {
      "epoch": 1.7670511177559447,
      "grad_norm": 0.5000165104866028,
      "learning_rate": 8.227741108314735e-05,
      "loss": 0.6416,
      "step": 124100
    },
    {
      "epoch": 1.7677630642175708,
      "grad_norm": 0.4839576780796051,
      "learning_rate": 8.222992544755212e-05,
      "loss": 0.653,
      "step": 124150
    },
    {
      "epoch": 1.7684750106791969,
      "grad_norm": 0.7163050174713135,
      "learning_rate": 8.21824398119569e-05,
      "loss": 0.6185,
      "step": 124200
    },
    {
      "epoch": 1.769186957140823,
      "grad_norm": 0.5274282097816467,
      "learning_rate": 8.213495417636166e-05,
      "loss": 0.642,
      "step": 124250
    },
    {
      "epoch": 1.769898903602449,
      "grad_norm": 0.7628419995307922,
      "learning_rate": 8.208746854076642e-05,
      "loss": 0.6551,
      "step": 124300
    },
    {
      "epoch": 1.7706108500640751,
      "grad_norm": 0.8834076523780823,
      "learning_rate": 8.203998290517119e-05,
      "loss": 0.6374,
      "step": 124350
    },
    {
      "epoch": 1.7713227965257012,
      "grad_norm": 0.47569164633750916,
      "learning_rate": 8.199249726957595e-05,
      "loss": 0.6148,
      "step": 124400
    },
    {
      "epoch": 1.7720347429873273,
      "grad_norm": 0.5975772738456726,
      "learning_rate": 8.194501163398073e-05,
      "loss": 0.6434,
      "step": 124450
    },
    {
      "epoch": 1.7727466894489534,
      "grad_norm": 0.5143668055534363,
      "learning_rate": 8.189752599838549e-05,
      "loss": 0.6417,
      "step": 124500
    },
    {
      "epoch": 1.7734586359105795,
      "grad_norm": 0.5808954834938049,
      "learning_rate": 8.185004036279026e-05,
      "loss": 0.6127,
      "step": 124550
    },
    {
      "epoch": 1.7741705823722056,
      "grad_norm": 0.5689244866371155,
      "learning_rate": 8.180255472719502e-05,
      "loss": 0.6632,
      "step": 124600
    },
    {
      "epoch": 1.7748825288338317,
      "grad_norm": 0.5130940079689026,
      "learning_rate": 8.175506909159978e-05,
      "loss": 0.618,
      "step": 124650
    },
    {
      "epoch": 1.7755944752954578,
      "grad_norm": 0.4417826533317566,
      "learning_rate": 8.170758345600456e-05,
      "loss": 0.6306,
      "step": 124700
    },
    {
      "epoch": 1.7763064217570839,
      "grad_norm": 0.581004798412323,
      "learning_rate": 8.166009782040933e-05,
      "loss": 0.704,
      "step": 124750
    },
    {
      "epoch": 1.77701836821871,
      "grad_norm": 0.5644629597663879,
      "learning_rate": 8.16126121848141e-05,
      "loss": 0.7272,
      "step": 124800
    },
    {
      "epoch": 1.777730314680336,
      "grad_norm": 0.5508917570114136,
      "learning_rate": 8.156512654921887e-05,
      "loss": 0.671,
      "step": 124850
    },
    {
      "epoch": 1.7784422611419621,
      "grad_norm": 0.40405765175819397,
      "learning_rate": 8.151764091362363e-05,
      "loss": 0.5934,
      "step": 124900
    },
    {
      "epoch": 1.7791542076035882,
      "grad_norm": 0.6018610596656799,
      "learning_rate": 8.147015527802841e-05,
      "loss": 0.6851,
      "step": 124950
    },
    {
      "epoch": 1.7798661540652143,
      "grad_norm": 0.4724537134170532,
      "learning_rate": 8.142266964243317e-05,
      "loss": 0.5984,
      "step": 125000
    },
    {
      "epoch": 1.7805781005268404,
      "grad_norm": 0.903264582157135,
      "learning_rate": 8.137518400683794e-05,
      "loss": 0.6276,
      "step": 125050
    },
    {
      "epoch": 1.7812900469884665,
      "grad_norm": 0.43976977467536926,
      "learning_rate": 8.13286480839546e-05,
      "loss": 0.6056,
      "step": 125100
    },
    {
      "epoch": 1.7820019934500926,
      "grad_norm": 0.6361299157142639,
      "learning_rate": 8.128116244835937e-05,
      "loss": 0.6237,
      "step": 125150
    },
    {
      "epoch": 1.7827139399117187,
      "grad_norm": 0.49994805455207825,
      "learning_rate": 8.123367681276414e-05,
      "loss": 0.602,
      "step": 125200
    },
    {
      "epoch": 1.7834258863733448,
      "grad_norm": 0.45679575204849243,
      "learning_rate": 8.118619117716891e-05,
      "loss": 0.7238,
      "step": 125250
    },
    {
      "epoch": 1.7841378328349708,
      "grad_norm": 0.5596612095832825,
      "learning_rate": 8.113870554157367e-05,
      "loss": 0.7251,
      "step": 125300
    },
    {
      "epoch": 1.784849779296597,
      "grad_norm": 0.6387584805488586,
      "learning_rate": 8.109121990597845e-05,
      "loss": 0.677,
      "step": 125350
    },
    {
      "epoch": 1.785561725758223,
      "grad_norm": 0.6431001424789429,
      "learning_rate": 8.104373427038321e-05,
      "loss": 0.5633,
      "step": 125400
    },
    {
      "epoch": 1.7862736722198491,
      "grad_norm": 0.4324103891849518,
      "learning_rate": 8.099624863478799e-05,
      "loss": 0.6849,
      "step": 125450
    },
    {
      "epoch": 1.7869856186814752,
      "grad_norm": 0.62691330909729,
      "learning_rate": 8.094876299919276e-05,
      "loss": 0.6591,
      "step": 125500
    },
    {
      "epoch": 1.7876975651431013,
      "grad_norm": 0.4254171550273895,
      "learning_rate": 8.090127736359752e-05,
      "loss": 0.6608,
      "step": 125550
    },
    {
      "epoch": 1.7884095116047272,
      "grad_norm": 0.48250341415405273,
      "learning_rate": 8.085379172800228e-05,
      "loss": 0.6157,
      "step": 125600
    },
    {
      "epoch": 1.7891214580663535,
      "grad_norm": 1.00311279296875,
      "learning_rate": 8.080630609240705e-05,
      "loss": 0.5878,
      "step": 125650
    },
    {
      "epoch": 1.7898334045279793,
      "grad_norm": 0.6481460928916931,
      "learning_rate": 8.075882045681183e-05,
      "loss": 0.618,
      "step": 125700
    },
    {
      "epoch": 1.7905453509896057,
      "grad_norm": 0.5940829515457153,
      "learning_rate": 8.071133482121659e-05,
      "loss": 0.6481,
      "step": 125750
    },
    {
      "epoch": 1.7912572974512315,
      "grad_norm": 0.3990165889263153,
      "learning_rate": 8.066384918562135e-05,
      "loss": 0.6883,
      "step": 125800
    },
    {
      "epoch": 1.7919692439128578,
      "grad_norm": 0.5199148058891296,
      "learning_rate": 8.061636355002612e-05,
      "loss": 0.6381,
      "step": 125850
    },
    {
      "epoch": 1.7926811903744837,
      "grad_norm": 0.4340420365333557,
      "learning_rate": 8.056887791443088e-05,
      "loss": 0.6345,
      "step": 125900
    },
    {
      "epoch": 1.79339313683611,
      "grad_norm": 0.5615243911743164,
      "learning_rate": 8.052139227883566e-05,
      "loss": 0.6161,
      "step": 125950
    },
    {
      "epoch": 1.7941050832977359,
      "grad_norm": 0.47983890771865845,
      "learning_rate": 8.047390664324042e-05,
      "loss": 0.6482,
      "step": 126000
    },
    {
      "epoch": 1.7948170297593622,
      "grad_norm": 0.5939720273017883,
      "learning_rate": 8.042642100764519e-05,
      "loss": 0.6926,
      "step": 126050
    },
    {
      "epoch": 1.795528976220988,
      "grad_norm": 0.5345467925071716,
      "learning_rate": 8.037893537204995e-05,
      "loss": 0.5985,
      "step": 126100
    },
    {
      "epoch": 1.7962409226826144,
      "grad_norm": 0.47460588812828064,
      "learning_rate": 8.033144973645472e-05,
      "loss": 0.63,
      "step": 126150
    },
    {
      "epoch": 1.7969528691442402,
      "grad_norm": 0.751849889755249,
      "learning_rate": 8.02839641008595e-05,
      "loss": 0.6362,
      "step": 126200
    },
    {
      "epoch": 1.7976648156058666,
      "grad_norm": 0.3694990873336792,
      "learning_rate": 8.023647846526427e-05,
      "loss": 0.6393,
      "step": 126250
    },
    {
      "epoch": 1.7983767620674924,
      "grad_norm": 0.4928182065486908,
      "learning_rate": 8.018899282966904e-05,
      "loss": 0.6648,
      "step": 126300
    },
    {
      "epoch": 1.7990887085291187,
      "grad_norm": 0.5939902067184448,
      "learning_rate": 8.01415071940738e-05,
      "loss": 0.6245,
      "step": 126350
    },
    {
      "epoch": 1.7998006549907446,
      "grad_norm": 0.35663342475891113,
      "learning_rate": 8.009402155847856e-05,
      "loss": 0.6385,
      "step": 126400
    },
    {
      "epoch": 1.800512601452371,
      "grad_norm": 0.5372258424758911,
      "learning_rate": 8.004653592288334e-05,
      "loss": 0.6387,
      "step": 126450
    },
    {
      "epoch": 1.8012245479139968,
      "grad_norm": 0.7671939730644226,
      "learning_rate": 7.99990502872881e-05,
      "loss": 0.6394,
      "step": 126500
    },
    {
      "epoch": 1.801936494375623,
      "grad_norm": 0.6957143545150757,
      "learning_rate": 7.995156465169287e-05,
      "loss": 0.6167,
      "step": 126550
    },
    {
      "epoch": 1.802648440837249,
      "grad_norm": 0.674317479133606,
      "learning_rate": 7.990407901609763e-05,
      "loss": 0.6005,
      "step": 126600
    },
    {
      "epoch": 1.8033603872988753,
      "grad_norm": 0.6446442604064941,
      "learning_rate": 7.98565933805024e-05,
      "loss": 0.6185,
      "step": 126650
    },
    {
      "epoch": 1.8040723337605011,
      "grad_norm": 0.49591565132141113,
      "learning_rate": 7.980910774490717e-05,
      "loss": 0.6442,
      "step": 126700
    },
    {
      "epoch": 1.8047842802221274,
      "grad_norm": 0.6102425456047058,
      "learning_rate": 7.976162210931194e-05,
      "loss": 0.6518,
      "step": 126750
    },
    {
      "epoch": 1.8054962266837533,
      "grad_norm": 0.6398125290870667,
      "learning_rate": 7.97141364737167e-05,
      "loss": 0.7096,
      "step": 126800
    },
    {
      "epoch": 1.8062081731453796,
      "grad_norm": 0.5203426480293274,
      "learning_rate": 7.966665083812147e-05,
      "loss": 0.6625,
      "step": 126850
    },
    {
      "epoch": 1.8069201196070055,
      "grad_norm": 0.4099302887916565,
      "learning_rate": 7.961916520252623e-05,
      "loss": 0.569,
      "step": 126900
    },
    {
      "epoch": 1.8076320660686318,
      "grad_norm": 0.4883516728878021,
      "learning_rate": 7.957167956693101e-05,
      "loss": 0.6108,
      "step": 126950
    },
    {
      "epoch": 1.8083440125302577,
      "grad_norm": 0.4284033179283142,
      "learning_rate": 7.952419393133577e-05,
      "loss": 0.6331,
      "step": 127000
    },
    {
      "epoch": 1.8090559589918838,
      "grad_norm": 0.7089796662330627,
      "learning_rate": 7.947670829574054e-05,
      "loss": 0.5967,
      "step": 127050
    },
    {
      "epoch": 1.8097679054535099,
      "grad_norm": 0.43805038928985596,
      "learning_rate": 7.942922266014531e-05,
      "loss": 0.5805,
      "step": 127100
    },
    {
      "epoch": 1.810479851915136,
      "grad_norm": 0.4009203612804413,
      "learning_rate": 7.938173702455008e-05,
      "loss": 0.6505,
      "step": 127150
    },
    {
      "epoch": 1.811191798376762,
      "grad_norm": 0.5330110788345337,
      "learning_rate": 7.933425138895486e-05,
      "loss": 0.6507,
      "step": 127200
    },
    {
      "epoch": 1.8119037448383881,
      "grad_norm": 0.5339565873146057,
      "learning_rate": 7.928676575335962e-05,
      "loss": 0.5854,
      "step": 127250
    },
    {
      "epoch": 1.8126156913000142,
      "grad_norm": 0.5375736951828003,
      "learning_rate": 7.924022983047629e-05,
      "loss": 0.6427,
      "step": 127300
    },
    {
      "epoch": 1.8133276377616403,
      "grad_norm": 0.37269723415374756,
      "learning_rate": 7.919274419488105e-05,
      "loss": 0.6267,
      "step": 127350
    },
    {
      "epoch": 1.8140395842232664,
      "grad_norm": 0.7443631291389465,
      "learning_rate": 7.914525855928581e-05,
      "loss": 0.6453,
      "step": 127400
    },
    {
      "epoch": 1.8147515306848925,
      "grad_norm": 0.5834788084030151,
      "learning_rate": 7.909872263640249e-05,
      "loss": 0.7089,
      "step": 127450
    },
    {
      "epoch": 1.8154634771465186,
      "grad_norm": 0.7730966210365295,
      "learning_rate": 7.905123700080726e-05,
      "loss": 0.6268,
      "step": 127500
    },
    {
      "epoch": 1.8161754236081447,
      "grad_norm": 0.6451268792152405,
      "learning_rate": 7.900375136521203e-05,
      "loss": 0.6035,
      "step": 127550
    },
    {
      "epoch": 1.8168873700697707,
      "grad_norm": 0.5839315056800842,
      "learning_rate": 7.89562657296168e-05,
      "loss": 0.5966,
      "step": 127600
    },
    {
      "epoch": 1.8175993165313968,
      "grad_norm": 0.5516251921653748,
      "learning_rate": 7.890878009402156e-05,
      "loss": 0.6492,
      "step": 127650
    },
    {
      "epoch": 1.818311262993023,
      "grad_norm": 0.29781249165534973,
      "learning_rate": 7.886129445842633e-05,
      "loss": 0.6648,
      "step": 127700
    },
    {
      "epoch": 1.819023209454649,
      "grad_norm": 0.5046635866165161,
      "learning_rate": 7.881380882283109e-05,
      "loss": 0.6148,
      "step": 127750
    },
    {
      "epoch": 1.819735155916275,
      "grad_norm": 0.6392180919647217,
      "learning_rate": 7.876632318723587e-05,
      "loss": 0.6816,
      "step": 127800
    },
    {
      "epoch": 1.8204471023779012,
      "grad_norm": 0.3678489625453949,
      "learning_rate": 7.871883755164063e-05,
      "loss": 0.6493,
      "step": 127850
    },
    {
      "epoch": 1.8211590488395273,
      "grad_norm": 0.400981068611145,
      "learning_rate": 7.86713519160454e-05,
      "loss": 0.7126,
      "step": 127900
    },
    {
      "epoch": 1.8218709953011534,
      "grad_norm": 0.4084971249103546,
      "learning_rate": 7.862386628045016e-05,
      "loss": 0.6179,
      "step": 127950
    },
    {
      "epoch": 1.8225829417627795,
      "grad_norm": 0.6145727634429932,
      "learning_rate": 7.857638064485492e-05,
      "loss": 0.6816,
      "step": 128000
    },
    {
      "epoch": 1.8232948882244056,
      "grad_norm": 0.5498477220535278,
      "learning_rate": 7.85288950092597e-05,
      "loss": 0.5487,
      "step": 128050
    },
    {
      "epoch": 1.8240068346860316,
      "grad_norm": 0.8334426879882812,
      "learning_rate": 7.848140937366447e-05,
      "loss": 0.6012,
      "step": 128100
    },
    {
      "epoch": 1.8247187811476577,
      "grad_norm": 0.45877769589424133,
      "learning_rate": 7.843392373806923e-05,
      "loss": 0.6342,
      "step": 128150
    },
    {
      "epoch": 1.8254307276092838,
      "grad_norm": 0.6372392773628235,
      "learning_rate": 7.838643810247401e-05,
      "loss": 0.6322,
      "step": 128200
    },
    {
      "epoch": 1.82614267407091,
      "grad_norm": 1.0195307731628418,
      "learning_rate": 7.833895246687877e-05,
      "loss": 0.5771,
      "step": 128250
    },
    {
      "epoch": 1.826854620532536,
      "grad_norm": 1.0448551177978516,
      "learning_rate": 7.829146683128355e-05,
      "loss": 0.6018,
      "step": 128300
    },
    {
      "epoch": 1.827566566994162,
      "grad_norm": 0.6137702465057373,
      "learning_rate": 7.824398119568831e-05,
      "loss": 0.6098,
      "step": 128350
    },
    {
      "epoch": 1.8282785134557882,
      "grad_norm": 0.4813481569290161,
      "learning_rate": 7.819649556009308e-05,
      "loss": 0.6671,
      "step": 128400
    },
    {
      "epoch": 1.828990459917414,
      "grad_norm": 0.5013079047203064,
      "learning_rate": 7.814900992449784e-05,
      "loss": 0.7039,
      "step": 128450
    },
    {
      "epoch": 1.8297024063790404,
      "grad_norm": 0.37790271639823914,
      "learning_rate": 7.81015242889026e-05,
      "loss": 0.6757,
      "step": 128500
    },
    {
      "epoch": 1.8304143528406662,
      "grad_norm": 0.7195703983306885,
      "learning_rate": 7.805403865330738e-05,
      "loss": 0.6309,
      "step": 128550
    },
    {
      "epoch": 1.8311262993022925,
      "grad_norm": 0.38237813115119934,
      "learning_rate": 7.800655301771215e-05,
      "loss": 0.6351,
      "step": 128600
    },
    {
      "epoch": 1.8318382457639184,
      "grad_norm": 0.6151183247566223,
      "learning_rate": 7.795906738211691e-05,
      "loss": 0.6767,
      "step": 128650
    },
    {
      "epoch": 1.8325501922255447,
      "grad_norm": 0.5622354745864868,
      "learning_rate": 7.791158174652168e-05,
      "loss": 0.6233,
      "step": 128700
    },
    {
      "epoch": 1.8332621386871706,
      "grad_norm": 0.7189283967018127,
      "learning_rate": 7.786409611092644e-05,
      "loss": 0.511,
      "step": 128750
    },
    {
      "epoch": 1.833974085148797,
      "grad_norm": 0.4761917293071747,
      "learning_rate": 7.781661047533122e-05,
      "loss": 0.6509,
      "step": 128800
    },
    {
      "epoch": 1.8346860316104228,
      "grad_norm": 0.7450451254844666,
      "learning_rate": 7.776912483973598e-05,
      "loss": 0.6958,
      "step": 128850
    },
    {
      "epoch": 1.835397978072049,
      "grad_norm": 0.6463017463684082,
      "learning_rate": 7.772163920414075e-05,
      "loss": 0.5918,
      "step": 128900
    },
    {
      "epoch": 1.836109924533675,
      "grad_norm": 0.7568352222442627,
      "learning_rate": 7.767415356854551e-05,
      "loss": 0.6358,
      "step": 128950
    },
    {
      "epoch": 1.8368218709953013,
      "grad_norm": 0.6847808957099915,
      "learning_rate": 7.762666793295029e-05,
      "loss": 0.6334,
      "step": 129000
    },
    {
      "epoch": 1.8375338174569271,
      "grad_norm": 0.4758342504501343,
      "learning_rate": 7.757918229735505e-05,
      "loss": 0.6043,
      "step": 129050
    },
    {
      "epoch": 1.8382457639185534,
      "grad_norm": 0.44895055890083313,
      "learning_rate": 7.753169666175983e-05,
      "loss": 0.6679,
      "step": 129100
    },
    {
      "epoch": 1.8389577103801793,
      "grad_norm": 0.4873775541782379,
      "learning_rate": 7.748421102616459e-05,
      "loss": 0.6177,
      "step": 129150
    },
    {
      "epoch": 1.8396696568418056,
      "grad_norm": 0.2748304307460785,
      "learning_rate": 7.743672539056936e-05,
      "loss": 0.625,
      "step": 129200
    },
    {
      "epoch": 1.8403816033034315,
      "grad_norm": 0.6410127282142639,
      "learning_rate": 7.738923975497412e-05,
      "loss": 0.6281,
      "step": 129250
    },
    {
      "epoch": 1.8410935497650578,
      "grad_norm": 0.5696552395820618,
      "learning_rate": 7.73417541193789e-05,
      "loss": 0.6636,
      "step": 129300
    },
    {
      "epoch": 1.8418054962266837,
      "grad_norm": 0.6895323395729065,
      "learning_rate": 7.729426848378366e-05,
      "loss": 0.6686,
      "step": 129350
    },
    {
      "epoch": 1.84251744268831,
      "grad_norm": 0.5436521768569946,
      "learning_rate": 7.724678284818843e-05,
      "loss": 0.6582,
      "step": 129400
    },
    {
      "epoch": 1.8432293891499358,
      "grad_norm": 0.43987956643104553,
      "learning_rate": 7.719929721259319e-05,
      "loss": 0.6407,
      "step": 129450
    },
    {
      "epoch": 1.8439413356115621,
      "grad_norm": 0.5893162488937378,
      "learning_rate": 7.715181157699795e-05,
      "loss": 0.6826,
      "step": 129500
    },
    {
      "epoch": 1.844653282073188,
      "grad_norm": 0.662041187286377,
      "learning_rate": 7.710432594140273e-05,
      "loss": 0.6808,
      "step": 129550
    },
    {
      "epoch": 1.8453652285348143,
      "grad_norm": 0.45494014024734497,
      "learning_rate": 7.70568403058075e-05,
      "loss": 0.6955,
      "step": 129600
    },
    {
      "epoch": 1.8460771749964402,
      "grad_norm": 0.4918586313724518,
      "learning_rate": 7.700935467021226e-05,
      "loss": 0.5916,
      "step": 129650
    },
    {
      "epoch": 1.8467891214580665,
      "grad_norm": 0.5383712649345398,
      "learning_rate": 7.696186903461702e-05,
      "loss": 0.5904,
      "step": 129700
    },
    {
      "epoch": 1.8475010679196924,
      "grad_norm": 0.6271942853927612,
      "learning_rate": 7.691438339902179e-05,
      "loss": 0.6088,
      "step": 129750
    },
    {
      "epoch": 1.8482130143813187,
      "grad_norm": 0.4812218248844147,
      "learning_rate": 7.686784747613848e-05,
      "loss": 0.6136,
      "step": 129800
    },
    {
      "epoch": 1.8489249608429446,
      "grad_norm": 0.5140223503112793,
      "learning_rate": 7.682036184054325e-05,
      "loss": 0.675,
      "step": 129850
    },
    {
      "epoch": 1.8496369073045706,
      "grad_norm": 0.7482432723045349,
      "learning_rate": 7.677287620494801e-05,
      "loss": 0.6032,
      "step": 129900
    },
    {
      "epoch": 1.8503488537661967,
      "grad_norm": 0.5854161977767944,
      "learning_rate": 7.672539056935277e-05,
      "loss": 0.6087,
      "step": 129950
    },
    {
      "epoch": 1.8510608002278228,
      "grad_norm": 0.5119589567184448,
      "learning_rate": 7.667790493375754e-05,
      "loss": 0.6269,
      "step": 130000
    },
    {
      "epoch": 1.851772746689449,
      "grad_norm": 0.8076304197311401,
      "learning_rate": 7.663041929816232e-05,
      "loss": 0.6087,
      "step": 130050
    },
    {
      "epoch": 1.852484693151075,
      "grad_norm": 0.7964643836021423,
      "learning_rate": 7.658293366256708e-05,
      "loss": 0.7103,
      "step": 130100
    },
    {
      "epoch": 1.853196639612701,
      "grad_norm": 0.5479966998100281,
      "learning_rate": 7.653544802697184e-05,
      "loss": 0.7026,
      "step": 130150
    },
    {
      "epoch": 1.8539085860743272,
      "grad_norm": 0.5355525016784668,
      "learning_rate": 7.648796239137661e-05,
      "loss": 0.6225,
      "step": 130200
    },
    {
      "epoch": 1.8546205325359533,
      "grad_norm": 1.2091386318206787,
      "learning_rate": 7.644047675578137e-05,
      "loss": 0.7078,
      "step": 130250
    },
    {
      "epoch": 1.8553324789975794,
      "grad_norm": 0.47288262844085693,
      "learning_rate": 7.639299112018615e-05,
      "loss": 0.6327,
      "step": 130300
    },
    {
      "epoch": 1.8560444254592054,
      "grad_norm": 0.39494243264198303,
      "learning_rate": 7.634550548459091e-05,
      "loss": 0.6377,
      "step": 130350
    },
    {
      "epoch": 1.8567563719208315,
      "grad_norm": 0.6457399725914001,
      "learning_rate": 7.629801984899568e-05,
      "loss": 0.7111,
      "step": 130400
    },
    {
      "epoch": 1.8574683183824576,
      "grad_norm": 0.48253652453422546,
      "learning_rate": 7.625053421340045e-05,
      "loss": 0.6224,
      "step": 130450
    },
    {
      "epoch": 1.8581802648440837,
      "grad_norm": 0.495115727186203,
      "learning_rate": 7.620304857780522e-05,
      "loss": 0.627,
      "step": 130500
    },
    {
      "epoch": 1.8588922113057098,
      "grad_norm": 0.5378795266151428,
      "learning_rate": 7.615556294221e-05,
      "loss": 0.6783,
      "step": 130550
    },
    {
      "epoch": 1.859604157767336,
      "grad_norm": 0.4817975163459778,
      "learning_rate": 7.610807730661476e-05,
      "loss": 0.6763,
      "step": 130600
    },
    {
      "epoch": 1.860316104228962,
      "grad_norm": 0.49880215525627136,
      "learning_rate": 7.606059167101952e-05,
      "loss": 0.6144,
      "step": 130650
    },
    {
      "epoch": 1.861028050690588,
      "grad_norm": 0.43671509623527527,
      "learning_rate": 7.601310603542429e-05,
      "loss": 0.5957,
      "step": 130700
    },
    {
      "epoch": 1.8617399971522142,
      "grad_norm": 0.5257977843284607,
      "learning_rate": 7.596562039982905e-05,
      "loss": 0.6045,
      "step": 130750
    },
    {
      "epoch": 1.8624519436138403,
      "grad_norm": 0.8644930124282837,
      "learning_rate": 7.591813476423383e-05,
      "loss": 0.597,
      "step": 130800
    },
    {
      "epoch": 1.8631638900754663,
      "grad_norm": 0.5935767292976379,
      "learning_rate": 7.58706491286386e-05,
      "loss": 0.6034,
      "step": 130850
    },
    {
      "epoch": 1.8638758365370924,
      "grad_norm": 0.48153647780418396,
      "learning_rate": 7.582316349304336e-05,
      "loss": 0.6492,
      "step": 130900
    },
    {
      "epoch": 1.8645877829987185,
      "grad_norm": 0.6540238261222839,
      "learning_rate": 7.577567785744812e-05,
      "loss": 0.6102,
      "step": 130950
    },
    {
      "epoch": 1.8652997294603446,
      "grad_norm": 0.5689482092857361,
      "learning_rate": 7.572819222185289e-05,
      "loss": 0.6453,
      "step": 131000
    },
    {
      "epoch": 1.8660116759219707,
      "grad_norm": 0.3470551073551178,
      "learning_rate": 7.568070658625766e-05,
      "loss": 0.6057,
      "step": 131050
    },
    {
      "epoch": 1.8667236223835968,
      "grad_norm": 0.7248697876930237,
      "learning_rate": 7.563322095066243e-05,
      "loss": 0.6184,
      "step": 131100
    },
    {
      "epoch": 1.8674355688452229,
      "grad_norm": 0.7791725993156433,
      "learning_rate": 7.558573531506719e-05,
      "loss": 0.656,
      "step": 131150
    },
    {
      "epoch": 1.868147515306849,
      "grad_norm": 0.7070616483688354,
      "learning_rate": 7.553824967947196e-05,
      "loss": 0.6199,
      "step": 131200
    },
    {
      "epoch": 1.868859461768475,
      "grad_norm": 0.4554155170917511,
      "learning_rate": 7.549076404387672e-05,
      "loss": 0.6611,
      "step": 131250
    },
    {
      "epoch": 1.8695714082301012,
      "grad_norm": 0.8265237212181091,
      "learning_rate": 7.54432784082815e-05,
      "loss": 0.6428,
      "step": 131300
    },
    {
      "epoch": 1.8702833546917272,
      "grad_norm": 0.5346150398254395,
      "learning_rate": 7.539579277268626e-05,
      "loss": 0.6153,
      "step": 131350
    },
    {
      "epoch": 1.870995301153353,
      "grad_norm": 0.5733276009559631,
      "learning_rate": 7.534830713709104e-05,
      "loss": 0.6717,
      "step": 131400
    },
    {
      "epoch": 1.8717072476149794,
      "grad_norm": 0.7408744096755981,
      "learning_rate": 7.53008215014958e-05,
      "loss": 0.684,
      "step": 131450
    },
    {
      "epoch": 1.8724191940766053,
      "grad_norm": 0.5123221278190613,
      "learning_rate": 7.525333586590057e-05,
      "loss": 0.6085,
      "step": 131500
    },
    {
      "epoch": 1.8731311405382316,
      "grad_norm": 0.6139206886291504,
      "learning_rate": 7.520585023030535e-05,
      "loss": 0.6873,
      "step": 131550
    },
    {
      "epoch": 1.8738430869998575,
      "grad_norm": 0.6497774720191956,
      "learning_rate": 7.515836459471011e-05,
      "loss": 0.588,
      "step": 131600
    },
    {
      "epoch": 1.8745550334614838,
      "grad_norm": 0.4523810148239136,
      "learning_rate": 7.511087895911487e-05,
      "loss": 0.6618,
      "step": 131650
    },
    {
      "epoch": 1.8752669799231096,
      "grad_norm": 0.47486960887908936,
      "learning_rate": 7.506339332351964e-05,
      "loss": 0.5908,
      "step": 131700
    },
    {
      "epoch": 1.875978926384736,
      "grad_norm": 0.8762133717536926,
      "learning_rate": 7.50159076879244e-05,
      "loss": 0.6724,
      "step": 131750
    },
    {
      "epoch": 1.8766908728463618,
      "grad_norm": 0.5036027431488037,
      "learning_rate": 7.496842205232918e-05,
      "loss": 0.6364,
      "step": 131800
    },
    {
      "epoch": 1.8774028193079881,
      "grad_norm": 0.6368247866630554,
      "learning_rate": 7.492093641673394e-05,
      "loss": 0.6196,
      "step": 131850
    },
    {
      "epoch": 1.878114765769614,
      "grad_norm": 0.5644044876098633,
      "learning_rate": 7.487345078113871e-05,
      "loss": 0.6347,
      "step": 131900
    },
    {
      "epoch": 1.8788267122312403,
      "grad_norm": 0.7100173234939575,
      "learning_rate": 7.482596514554347e-05,
      "loss": 0.6874,
      "step": 131950
    },
    {
      "epoch": 1.8795386586928662,
      "grad_norm": 0.8039681911468506,
      "learning_rate": 7.477847950994824e-05,
      "loss": 0.678,
      "step": 132000
    },
    {
      "epoch": 1.8802506051544925,
      "grad_norm": 0.4085339903831482,
      "learning_rate": 7.473099387435301e-05,
      "loss": 0.6547,
      "step": 132050
    },
    {
      "epoch": 1.8809625516161184,
      "grad_norm": 0.813929557800293,
      "learning_rate": 7.468350823875778e-05,
      "loss": 0.6661,
      "step": 132100
    },
    {
      "epoch": 1.8816744980777447,
      "grad_norm": 0.5725425481796265,
      "learning_rate": 7.463602260316254e-05,
      "loss": 0.6337,
      "step": 132150
    },
    {
      "epoch": 1.8823864445393705,
      "grad_norm": 0.6621997356414795,
      "learning_rate": 7.458853696756732e-05,
      "loss": 0.6162,
      "step": 132200
    },
    {
      "epoch": 1.8830983910009969,
      "grad_norm": 0.42578160762786865,
      "learning_rate": 7.454105133197208e-05,
      "loss": 0.6395,
      "step": 132250
    },
    {
      "epoch": 1.8838103374626227,
      "grad_norm": 0.5869613885879517,
      "learning_rate": 7.449356569637686e-05,
      "loss": 0.6424,
      "step": 132300
    },
    {
      "epoch": 1.884522283924249,
      "grad_norm": 0.5140199065208435,
      "learning_rate": 7.444608006078162e-05,
      "loss": 0.6706,
      "step": 132350
    },
    {
      "epoch": 1.885234230385875,
      "grad_norm": 0.7340917587280273,
      "learning_rate": 7.439859442518639e-05,
      "loss": 0.6349,
      "step": 132400
    },
    {
      "epoch": 1.8859461768475012,
      "grad_norm": 0.4175563156604767,
      "learning_rate": 7.435110878959115e-05,
      "loss": 0.6788,
      "step": 132450
    },
    {
      "epoch": 1.886658123309127,
      "grad_norm": 0.7144019603729248,
      "learning_rate": 7.430362315399592e-05,
      "loss": 0.6627,
      "step": 132500
    },
    {
      "epoch": 1.8873700697707534,
      "grad_norm": 0.49120619893074036,
      "learning_rate": 7.42561375184007e-05,
      "loss": 0.5794,
      "step": 132550
    },
    {
      "epoch": 1.8880820162323793,
      "grad_norm": 0.39319515228271484,
      "learning_rate": 7.420960159551736e-05,
      "loss": 0.6293,
      "step": 132600
    },
    {
      "epoch": 1.8887939626940056,
      "grad_norm": 0.48852571845054626,
      "learning_rate": 7.416211595992212e-05,
      "loss": 0.6753,
      "step": 132650
    },
    {
      "epoch": 1.8895059091556314,
      "grad_norm": 0.5193912386894226,
      "learning_rate": 7.411463032432689e-05,
      "loss": 0.6348,
      "step": 132700
    },
    {
      "epoch": 1.8902178556172577,
      "grad_norm": 0.6536937952041626,
      "learning_rate": 7.406714468873167e-05,
      "loss": 0.6391,
      "step": 132750
    },
    {
      "epoch": 1.8909298020788836,
      "grad_norm": 0.6786468029022217,
      "learning_rate": 7.401965905313643e-05,
      "loss": 0.6412,
      "step": 132800
    },
    {
      "epoch": 1.8916417485405097,
      "grad_norm": 0.6775460839271545,
      "learning_rate": 7.397217341754121e-05,
      "loss": 0.6367,
      "step": 132850
    },
    {
      "epoch": 1.8923536950021358,
      "grad_norm": 0.6303451061248779,
      "learning_rate": 7.392468778194597e-05,
      "loss": 0.5981,
      "step": 132900
    },
    {
      "epoch": 1.8930656414637619,
      "grad_norm": 0.524027943611145,
      "learning_rate": 7.387720214635074e-05,
      "loss": 0.6282,
      "step": 132950
    },
    {
      "epoch": 1.893777587925388,
      "grad_norm": 0.5841896533966064,
      "learning_rate": 7.38297165107555e-05,
      "loss": 0.6296,
      "step": 133000
    },
    {
      "epoch": 1.894489534387014,
      "grad_norm": 0.671493649482727,
      "learning_rate": 7.378223087516026e-05,
      "loss": 0.5943,
      "step": 133050
    },
    {
      "epoch": 1.8952014808486402,
      "grad_norm": 0.6291784048080444,
      "learning_rate": 7.373474523956504e-05,
      "loss": 0.6641,
      "step": 133100
    },
    {
      "epoch": 1.8959134273102662,
      "grad_norm": 0.8624507188796997,
      "learning_rate": 7.36872596039698e-05,
      "loss": 0.7008,
      "step": 133150
    },
    {
      "epoch": 1.8966253737718923,
      "grad_norm": 0.718771755695343,
      "learning_rate": 7.363977396837457e-05,
      "loss": 0.6028,
      "step": 133200
    },
    {
      "epoch": 1.8973373202335184,
      "grad_norm": 0.3744354546070099,
      "learning_rate": 7.359228833277933e-05,
      "loss": 0.6487,
      "step": 133250
    },
    {
      "epoch": 1.8980492666951445,
      "grad_norm": 0.5228320956230164,
      "learning_rate": 7.35448026971841e-05,
      "loss": 0.6802,
      "step": 133300
    },
    {
      "epoch": 1.8987612131567706,
      "grad_norm": 0.6114022135734558,
      "learning_rate": 7.349731706158888e-05,
      "loss": 0.6292,
      "step": 133350
    },
    {
      "epoch": 1.8994731596183967,
      "grad_norm": 0.733128547668457,
      "learning_rate": 7.344983142599364e-05,
      "loss": 0.6556,
      "step": 133400
    },
    {
      "epoch": 1.9001851060800228,
      "grad_norm": 0.7289406061172485,
      "learning_rate": 7.34023457903984e-05,
      "loss": 0.6458,
      "step": 133450
    },
    {
      "epoch": 1.9008970525416489,
      "grad_norm": 0.5289369821548462,
      "learning_rate": 7.335486015480317e-05,
      "loss": 0.6271,
      "step": 133500
    },
    {
      "epoch": 1.901608999003275,
      "grad_norm": 0.6292904615402222,
      "learning_rate": 7.330737451920794e-05,
      "loss": 0.6716,
      "step": 133550
    },
    {
      "epoch": 1.902320945464901,
      "grad_norm": 0.34935474395751953,
      "learning_rate": 7.325988888361271e-05,
      "loss": 0.6262,
      "step": 133600
    },
    {
      "epoch": 1.9030328919265271,
      "grad_norm": 0.6429250836372375,
      "learning_rate": 7.321240324801749e-05,
      "loss": 0.6614,
      "step": 133650
    },
    {
      "epoch": 1.9037448383881532,
      "grad_norm": 0.6801185607910156,
      "learning_rate": 7.316491761242225e-05,
      "loss": 0.6221,
      "step": 133700
    },
    {
      "epoch": 1.9044567848497793,
      "grad_norm": 0.6387725472450256,
      "learning_rate": 7.311743197682701e-05,
      "loss": 0.6461,
      "step": 133750
    },
    {
      "epoch": 1.9051687313114054,
      "grad_norm": 0.6680633425712585,
      "learning_rate": 7.306994634123178e-05,
      "loss": 0.5882,
      "step": 133800
    },
    {
      "epoch": 1.9058806777730315,
      "grad_norm": 0.6409697532653809,
      "learning_rate": 7.302246070563656e-05,
      "loss": 0.6535,
      "step": 133850
    },
    {
      "epoch": 1.9065926242346576,
      "grad_norm": 0.29672399163246155,
      "learning_rate": 7.297497507004132e-05,
      "loss": 0.5809,
      "step": 133900
    },
    {
      "epoch": 1.9073045706962837,
      "grad_norm": 0.7803894281387329,
      "learning_rate": 7.292748943444608e-05,
      "loss": 0.6212,
      "step": 133950
    },
    {
      "epoch": 1.9080165171579098,
      "grad_norm": 0.7387920618057251,
      "learning_rate": 7.288000379885085e-05,
      "loss": 0.6518,
      "step": 134000
    },
    {
      "epoch": 1.9087284636195359,
      "grad_norm": 0.3597351312637329,
      "learning_rate": 7.283251816325561e-05,
      "loss": 0.6333,
      "step": 134050
    },
    {
      "epoch": 1.909440410081162,
      "grad_norm": 0.5008895397186279,
      "learning_rate": 7.278503252766039e-05,
      "loss": 0.5735,
      "step": 134100
    },
    {
      "epoch": 1.910152356542788,
      "grad_norm": 0.5994754433631897,
      "learning_rate": 7.273754689206515e-05,
      "loss": 0.6551,
      "step": 134150
    },
    {
      "epoch": 1.9108643030044141,
      "grad_norm": 0.5682177543640137,
      "learning_rate": 7.269006125646992e-05,
      "loss": 0.648,
      "step": 134200
    },
    {
      "epoch": 1.91157624946604,
      "grad_norm": 0.7765544056892395,
      "learning_rate": 7.264257562087468e-05,
      "loss": 0.6279,
      "step": 134250
    },
    {
      "epoch": 1.9122881959276663,
      "grad_norm": 0.5033139586448669,
      "learning_rate": 7.259508998527945e-05,
      "loss": 0.5876,
      "step": 134300
    },
    {
      "epoch": 1.9130001423892922,
      "grad_norm": 0.7803922891616821,
      "learning_rate": 7.254760434968422e-05,
      "loss": 0.6401,
      "step": 134350
    },
    {
      "epoch": 1.9137120888509185,
      "grad_norm": 0.5318769812583923,
      "learning_rate": 7.250011871408899e-05,
      "loss": 0.6297,
      "step": 134400
    },
    {
      "epoch": 1.9144240353125443,
      "grad_norm": 0.5891567468643188,
      "learning_rate": 7.245263307849377e-05,
      "loss": 0.6918,
      "step": 134450
    },
    {
      "epoch": 1.9151359817741707,
      "grad_norm": 0.3011590540409088,
      "learning_rate": 7.240514744289853e-05,
      "loss": 0.6155,
      "step": 134500
    },
    {
      "epoch": 1.9158479282357965,
      "grad_norm": 0.4607192575931549,
      "learning_rate": 7.23576618073033e-05,
      "loss": 0.6341,
      "step": 134550
    },
    {
      "epoch": 1.9165598746974228,
      "grad_norm": 0.8217857480049133,
      "learning_rate": 7.231017617170807e-05,
      "loss": 0.63,
      "step": 134600
    },
    {
      "epoch": 1.9172718211590487,
      "grad_norm": 0.8347583413124084,
      "learning_rate": 7.226269053611284e-05,
      "loss": 0.6325,
      "step": 134650
    },
    {
      "epoch": 1.917983767620675,
      "grad_norm": 0.44533461332321167,
      "learning_rate": 7.22152049005176e-05,
      "loss": 0.6286,
      "step": 134700
    },
    {
      "epoch": 1.9186957140823009,
      "grad_norm": 0.7159672379493713,
      "learning_rate": 7.216771926492236e-05,
      "loss": 0.6211,
      "step": 134750
    },
    {
      "epoch": 1.9194076605439272,
      "grad_norm": 0.7846629023551941,
      "learning_rate": 7.212023362932713e-05,
      "loss": 0.5982,
      "step": 134800
    },
    {
      "epoch": 1.920119607005553,
      "grad_norm": 0.41853588819503784,
      "learning_rate": 7.20727479937319e-05,
      "loss": 0.6861,
      "step": 134850
    },
    {
      "epoch": 1.9208315534671794,
      "grad_norm": 0.46463945508003235,
      "learning_rate": 7.202526235813667e-05,
      "loss": 0.5835,
      "step": 134900
    },
    {
      "epoch": 1.9215434999288052,
      "grad_norm": 1.332497477531433,
      "learning_rate": 7.197777672254143e-05,
      "loss": 0.697,
      "step": 134950
    },
    {
      "epoch": 1.9222554463904316,
      "grad_norm": 0.6277145743370056,
      "learning_rate": 7.19302910869462e-05,
      "loss": 0.6743,
      "step": 135000
    },
    {
      "epoch": 1.9229673928520574,
      "grad_norm": 0.606452465057373,
      "learning_rate": 7.188280545135096e-05,
      "loss": 0.5922,
      "step": 135050
    },
    {
      "epoch": 1.9236793393136837,
      "grad_norm": 0.416291743516922,
      "learning_rate": 7.183531981575574e-05,
      "loss": 0.6401,
      "step": 135100
    },
    {
      "epoch": 1.9243912857753096,
      "grad_norm": 0.32415667176246643,
      "learning_rate": 7.17878341801605e-05,
      "loss": 0.5919,
      "step": 135150
    },
    {
      "epoch": 1.925103232236936,
      "grad_norm": 0.47973060607910156,
      "learning_rate": 7.174034854456527e-05,
      "loss": 0.6205,
      "step": 135200
    },
    {
      "epoch": 1.9258151786985618,
      "grad_norm": 0.36161816120147705,
      "learning_rate": 7.169286290897003e-05,
      "loss": 0.6194,
      "step": 135250
    },
    {
      "epoch": 1.926527125160188,
      "grad_norm": 0.3429439067840576,
      "learning_rate": 7.164537727337481e-05,
      "loss": 0.7186,
      "step": 135300
    },
    {
      "epoch": 1.927239071621814,
      "grad_norm": 0.6693495512008667,
      "learning_rate": 7.159789163777957e-05,
      "loss": 0.5795,
      "step": 135350
    },
    {
      "epoch": 1.9279510180834403,
      "grad_norm": 0.4658605754375458,
      "learning_rate": 7.155040600218435e-05,
      "loss": 0.6454,
      "step": 135400
    },
    {
      "epoch": 1.9286629645450661,
      "grad_norm": 1.0148050785064697,
      "learning_rate": 7.150292036658911e-05,
      "loss": 0.6282,
      "step": 135450
    },
    {
      "epoch": 1.9293749110066925,
      "grad_norm": 0.6026776432991028,
      "learning_rate": 7.145543473099388e-05,
      "loss": 0.6302,
      "step": 135500
    },
    {
      "epoch": 1.9300868574683183,
      "grad_norm": 0.4497699439525604,
      "learning_rate": 7.140794909539864e-05,
      "loss": 0.6638,
      "step": 135550
    },
    {
      "epoch": 1.9307988039299446,
      "grad_norm": 0.6328054070472717,
      "learning_rate": 7.136046345980342e-05,
      "loss": 0.663,
      "step": 135600
    },
    {
      "epoch": 1.9315107503915705,
      "grad_norm": 0.7355612516403198,
      "learning_rate": 7.131297782420818e-05,
      "loss": 0.6316,
      "step": 135650
    },
    {
      "epoch": 1.9322226968531966,
      "grad_norm": 0.714386522769928,
      "learning_rate": 7.126549218861295e-05,
      "loss": 0.6976,
      "step": 135700
    },
    {
      "epoch": 1.9329346433148227,
      "grad_norm": 0.5184441804885864,
      "learning_rate": 7.121800655301771e-05,
      "loss": 0.6077,
      "step": 135750
    },
    {
      "epoch": 1.9336465897764488,
      "grad_norm": 0.6290624737739563,
      "learning_rate": 7.117052091742248e-05,
      "loss": 0.6505,
      "step": 135800
    },
    {
      "epoch": 1.9343585362380749,
      "grad_norm": 0.6769830584526062,
      "learning_rate": 7.112303528182725e-05,
      "loss": 0.6501,
      "step": 135850
    },
    {
      "epoch": 1.935070482699701,
      "grad_norm": 0.4675461947917938,
      "learning_rate": 7.107554964623202e-05,
      "loss": 0.6513,
      "step": 135900
    },
    {
      "epoch": 1.935782429161327,
      "grad_norm": 0.6454716920852661,
      "learning_rate": 7.102806401063678e-05,
      "loss": 0.6495,
      "step": 135950
    },
    {
      "epoch": 1.9364943756229531,
      "grad_norm": 0.35932856798171997,
      "learning_rate": 7.098057837504155e-05,
      "loss": 0.6138,
      "step": 136000
    },
    {
      "epoch": 1.9372063220845792,
      "grad_norm": 0.412530779838562,
      "learning_rate": 7.093309273944631e-05,
      "loss": 0.6993,
      "step": 136050
    },
    {
      "epoch": 1.9379182685462053,
      "grad_norm": 0.7418189644813538,
      "learning_rate": 7.088560710385109e-05,
      "loss": 0.6856,
      "step": 136100
    },
    {
      "epoch": 1.9386302150078314,
      "grad_norm": 0.3934200704097748,
      "learning_rate": 7.083812146825585e-05,
      "loss": 0.6072,
      "step": 136150
    },
    {
      "epoch": 1.9393421614694575,
      "grad_norm": 0.5669942498207092,
      "learning_rate": 7.079063583266063e-05,
      "loss": 0.7074,
      "step": 136200
    },
    {
      "epoch": 1.9400541079310836,
      "grad_norm": 0.46750274300575256,
      "learning_rate": 7.07431501970654e-05,
      "loss": 0.5907,
      "step": 136250
    },
    {
      "epoch": 1.9407660543927097,
      "grad_norm": 0.48017916083335876,
      "learning_rate": 7.069566456147016e-05,
      "loss": 0.6314,
      "step": 136300
    },
    {
      "epoch": 1.9414780008543358,
      "grad_norm": 1.0862011909484863,
      "learning_rate": 7.064817892587494e-05,
      "loss": 0.6679,
      "step": 136350
    },
    {
      "epoch": 1.9421899473159618,
      "grad_norm": 0.3555888235569,
      "learning_rate": 7.06006932902797e-05,
      "loss": 0.666,
      "step": 136400
    },
    {
      "epoch": 1.942901893777588,
      "grad_norm": 0.7775619626045227,
      "learning_rate": 7.055320765468446e-05,
      "loss": 0.6891,
      "step": 136450
    },
    {
      "epoch": 1.943613840239214,
      "grad_norm": 0.6586139798164368,
      "learning_rate": 7.050572201908923e-05,
      "loss": 0.6367,
      "step": 136500
    },
    {
      "epoch": 1.94432578670084,
      "grad_norm": 0.4565053880214691,
      "learning_rate": 7.045823638349399e-05,
      "loss": 0.6242,
      "step": 136550
    },
    {
      "epoch": 1.9450377331624662,
      "grad_norm": 0.4776025712490082,
      "learning_rate": 7.041075074789877e-05,
      "loss": 0.632,
      "step": 136600
    },
    {
      "epoch": 1.9457496796240923,
      "grad_norm": 0.5530105829238892,
      "learning_rate": 7.036326511230353e-05,
      "loss": 0.6204,
      "step": 136650
    },
    {
      "epoch": 1.9464616260857184,
      "grad_norm": 0.6425091028213501,
      "learning_rate": 7.03157794767083e-05,
      "loss": 0.6015,
      "step": 136700
    },
    {
      "epoch": 1.9471735725473445,
      "grad_norm": 0.4252552390098572,
      "learning_rate": 7.026924355382498e-05,
      "loss": 0.702,
      "step": 136750
    },
    {
      "epoch": 1.9478855190089706,
      "grad_norm": 0.4484148919582367,
      "learning_rate": 7.022175791822974e-05,
      "loss": 0.7117,
      "step": 136800
    },
    {
      "epoch": 1.9485974654705966,
      "grad_norm": 0.5945034027099609,
      "learning_rate": 7.01742722826345e-05,
      "loss": 0.6475,
      "step": 136850
    },
    {
      "epoch": 1.9493094119322227,
      "grad_norm": 0.460538774728775,
      "learning_rate": 7.012678664703928e-05,
      "loss": 0.6676,
      "step": 136900
    },
    {
      "epoch": 1.9500213583938488,
      "grad_norm": 0.48675814270973206,
      "learning_rate": 7.008025072415595e-05,
      "loss": 0.6457,
      "step": 136950
    },
    {
      "epoch": 1.950733304855475,
      "grad_norm": 0.6892966628074646,
      "learning_rate": 7.003276508856071e-05,
      "loss": 0.6303,
      "step": 137000
    },
    {
      "epoch": 1.951445251317101,
      "grad_norm": 0.4994211792945862,
      "learning_rate": 6.998527945296548e-05,
      "loss": 0.6503,
      "step": 137050
    },
    {
      "epoch": 1.952157197778727,
      "grad_norm": 0.4552849531173706,
      "learning_rate": 6.993779381737024e-05,
      "loss": 0.6389,
      "step": 137100
    },
    {
      "epoch": 1.9528691442403532,
      "grad_norm": 0.536431610584259,
      "learning_rate": 6.989030818177502e-05,
      "loss": 0.6776,
      "step": 137150
    },
    {
      "epoch": 1.953581090701979,
      "grad_norm": 0.4190056622028351,
      "learning_rate": 6.984282254617978e-05,
      "loss": 0.6515,
      "step": 137200
    },
    {
      "epoch": 1.9542930371636054,
      "grad_norm": 0.5202071666717529,
      "learning_rate": 6.979533691058455e-05,
      "loss": 0.6166,
      "step": 137250
    },
    {
      "epoch": 1.9550049836252312,
      "grad_norm": 0.8255980014801025,
      "learning_rate": 6.974785127498932e-05,
      "loss": 0.598,
      "step": 137300
    },
    {
      "epoch": 1.9557169300868575,
      "grad_norm": 0.51203852891922,
      "learning_rate": 6.970036563939409e-05,
      "loss": 0.6214,
      "step": 137350
    },
    {
      "epoch": 1.9564288765484834,
      "grad_norm": 0.32392558455467224,
      "learning_rate": 6.965288000379887e-05,
      "loss": 0.5572,
      "step": 137400
    },
    {
      "epoch": 1.9571408230101097,
      "grad_norm": 0.7108334302902222,
      "learning_rate": 6.960539436820363e-05,
      "loss": 0.6659,
      "step": 137450
    },
    {
      "epoch": 1.9578527694717356,
      "grad_norm": 0.7571433186531067,
      "learning_rate": 6.95579087326084e-05,
      "loss": 0.6364,
      "step": 137500
    },
    {
      "epoch": 1.958564715933362,
      "grad_norm": 0.5082988739013672,
      "learning_rate": 6.951042309701316e-05,
      "loss": 0.645,
      "step": 137550
    },
    {
      "epoch": 1.9592766623949878,
      "grad_norm": 0.6161609888076782,
      "learning_rate": 6.946293746141792e-05,
      "loss": 0.7189,
      "step": 137600
    },
    {
      "epoch": 1.959988608856614,
      "grad_norm": 0.40825799107551575,
      "learning_rate": 6.94154518258227e-05,
      "loss": 0.6557,
      "step": 137650
    },
    {
      "epoch": 1.96070055531824,
      "grad_norm": 0.6621101498603821,
      "learning_rate": 6.936796619022746e-05,
      "loss": 0.6006,
      "step": 137700
    },
    {
      "epoch": 1.9614125017798663,
      "grad_norm": 0.4975722134113312,
      "learning_rate": 6.932048055463223e-05,
      "loss": 0.6671,
      "step": 137750
    },
    {
      "epoch": 1.9621244482414921,
      "grad_norm": 1.2080522775650024,
      "learning_rate": 6.927299491903699e-05,
      "loss": 0.6605,
      "step": 137800
    },
    {
      "epoch": 1.9628363947031184,
      "grad_norm": 0.6717345714569092,
      "learning_rate": 6.922550928344176e-05,
      "loss": 0.5886,
      "step": 137850
    },
    {
      "epoch": 1.9635483411647443,
      "grad_norm": 0.8134428262710571,
      "learning_rate": 6.917802364784653e-05,
      "loss": 0.6397,
      "step": 137900
    },
    {
      "epoch": 1.9642602876263706,
      "grad_norm": 0.42031675577163696,
      "learning_rate": 6.91305380122513e-05,
      "loss": 0.6142,
      "step": 137950
    },
    {
      "epoch": 1.9649722340879965,
      "grad_norm": 0.553549587726593,
      "learning_rate": 6.908305237665606e-05,
      "loss": 0.6209,
      "step": 138000
    },
    {
      "epoch": 1.9656841805496228,
      "grad_norm": 0.4444754123687744,
      "learning_rate": 6.903556674106083e-05,
      "loss": 0.6444,
      "step": 138050
    },
    {
      "epoch": 1.9663961270112487,
      "grad_norm": 0.5550546646118164,
      "learning_rate": 6.89880811054656e-05,
      "loss": 0.5997,
      "step": 138100
    },
    {
      "epoch": 1.967108073472875,
      "grad_norm": 0.5096774697303772,
      "learning_rate": 6.894059546987037e-05,
      "loss": 0.6317,
      "step": 138150
    },
    {
      "epoch": 1.9678200199345008,
      "grad_norm": 0.5412968993186951,
      "learning_rate": 6.889310983427514e-05,
      "loss": 0.6508,
      "step": 138200
    },
    {
      "epoch": 1.9685319663961272,
      "grad_norm": 0.4393545985221863,
      "learning_rate": 6.884562419867991e-05,
      "loss": 0.6152,
      "step": 138250
    },
    {
      "epoch": 1.969243912857753,
      "grad_norm": 0.40973082184791565,
      "learning_rate": 6.879813856308467e-05,
      "loss": 0.6473,
      "step": 138300
    },
    {
      "epoch": 1.9699558593193793,
      "grad_norm": 0.44489726424217224,
      "learning_rate": 6.875065292748944e-05,
      "loss": 0.5444,
      "step": 138350
    },
    {
      "epoch": 1.9706678057810052,
      "grad_norm": 0.5239623785018921,
      "learning_rate": 6.870316729189421e-05,
      "loss": 0.6224,
      "step": 138400
    },
    {
      "epoch": 1.9713797522426315,
      "grad_norm": 0.4561986029148102,
      "learning_rate": 6.865568165629898e-05,
      "loss": 0.687,
      "step": 138450
    },
    {
      "epoch": 1.9720916987042574,
      "grad_norm": 0.48300909996032715,
      "learning_rate": 6.860819602070374e-05,
      "loss": 0.6943,
      "step": 138500
    },
    {
      "epoch": 1.9728036451658837,
      "grad_norm": 0.46957868337631226,
      "learning_rate": 6.85607103851085e-05,
      "loss": 0.6347,
      "step": 138550
    },
    {
      "epoch": 1.9735155916275096,
      "grad_norm": 0.5425602793693542,
      "learning_rate": 6.851322474951327e-05,
      "loss": 0.6022,
      "step": 138600
    },
    {
      "epoch": 1.9742275380891356,
      "grad_norm": 0.39760643243789673,
      "learning_rate": 6.846573911391805e-05,
      "loss": 0.6391,
      "step": 138650
    },
    {
      "epoch": 1.9749394845507617,
      "grad_norm": 0.48898956179618835,
      "learning_rate": 6.841825347832281e-05,
      "loss": 0.5581,
      "step": 138700
    },
    {
      "epoch": 1.9756514310123878,
      "grad_norm": 0.568426251411438,
      "learning_rate": 6.837076784272758e-05,
      "loss": 0.6269,
      "step": 138750
    },
    {
      "epoch": 1.976363377474014,
      "grad_norm": 0.6399205327033997,
      "learning_rate": 6.832328220713234e-05,
      "loss": 0.6388,
      "step": 138800
    },
    {
      "epoch": 1.97707532393564,
      "grad_norm": 0.6255491971969604,
      "learning_rate": 6.82757965715371e-05,
      "loss": 0.6698,
      "step": 138850
    },
    {
      "epoch": 1.977787270397266,
      "grad_norm": 0.5425181984901428,
      "learning_rate": 6.822831093594188e-05,
      "loss": 0.6309,
      "step": 138900
    },
    {
      "epoch": 1.9784992168588922,
      "grad_norm": 0.5664756894111633,
      "learning_rate": 6.818082530034665e-05,
      "loss": 0.6931,
      "step": 138950
    },
    {
      "epoch": 1.9792111633205183,
      "grad_norm": 0.801652193069458,
      "learning_rate": 6.813333966475141e-05,
      "loss": 0.6686,
      "step": 139000
    },
    {
      "epoch": 1.9799231097821444,
      "grad_norm": 0.7481991052627563,
      "learning_rate": 6.808585402915619e-05,
      "loss": 0.6896,
      "step": 139050
    },
    {
      "epoch": 1.9806350562437705,
      "grad_norm": 0.8305515050888062,
      "learning_rate": 6.803836839356095e-05,
      "loss": 0.6571,
      "step": 139100
    },
    {
      "epoch": 1.9813470027053965,
      "grad_norm": 0.359734445810318,
      "learning_rate": 6.799088275796573e-05,
      "loss": 0.5776,
      "step": 139150
    },
    {
      "epoch": 1.9820589491670226,
      "grad_norm": 0.6543527841567993,
      "learning_rate": 6.79433971223705e-05,
      "loss": 0.6625,
      "step": 139200
    },
    {
      "epoch": 1.9827708956286487,
      "grad_norm": 0.5913695096969604,
      "learning_rate": 6.789591148677526e-05,
      "loss": 0.646,
      "step": 139250
    },
    {
      "epoch": 1.9834828420902748,
      "grad_norm": 0.6615756154060364,
      "learning_rate": 6.784842585118002e-05,
      "loss": 0.6244,
      "step": 139300
    },
    {
      "epoch": 1.984194788551901,
      "grad_norm": 0.6252893209457397,
      "learning_rate": 6.780094021558479e-05,
      "loss": 0.6489,
      "step": 139350
    },
    {
      "epoch": 1.984906735013527,
      "grad_norm": 0.6639333367347717,
      "learning_rate": 6.775345457998956e-05,
      "loss": 0.6349,
      "step": 139400
    },
    {
      "epoch": 1.985618681475153,
      "grad_norm": 0.5501571893692017,
      "learning_rate": 6.770596894439433e-05,
      "loss": 0.648,
      "step": 139450
    },
    {
      "epoch": 1.9863306279367792,
      "grad_norm": 0.9960181713104248,
      "learning_rate": 6.765848330879909e-05,
      "loss": 0.6436,
      "step": 139500
    },
    {
      "epoch": 1.9870425743984053,
      "grad_norm": 0.5661091804504395,
      "learning_rate": 6.761099767320386e-05,
      "loss": 0.6172,
      "step": 139550
    },
    {
      "epoch": 1.9877545208600313,
      "grad_norm": 0.586143970489502,
      "learning_rate": 6.756351203760862e-05,
      "loss": 0.6897,
      "step": 139600
    },
    {
      "epoch": 1.9884664673216574,
      "grad_norm": 0.8545190691947937,
      "learning_rate": 6.75160264020134e-05,
      "loss": 0.6739,
      "step": 139650
    },
    {
      "epoch": 1.9891784137832835,
      "grad_norm": 0.5456956624984741,
      "learning_rate": 6.746854076641816e-05,
      "loss": 0.6376,
      "step": 139700
    },
    {
      "epoch": 1.9898903602449096,
      "grad_norm": 0.537717878818512,
      "learning_rate": 6.742105513082293e-05,
      "loss": 0.6081,
      "step": 139750
    },
    {
      "epoch": 1.9906023067065357,
      "grad_norm": 0.9329671859741211,
      "learning_rate": 6.737356949522769e-05,
      "loss": 0.6227,
      "step": 139800
    },
    {
      "epoch": 1.9913142531681618,
      "grad_norm": 0.7259423136711121,
      "learning_rate": 6.732608385963247e-05,
      "loss": 0.6276,
      "step": 139850
    },
    {
      "epoch": 1.9920261996297879,
      "grad_norm": 0.554739236831665,
      "learning_rate": 6.727859822403723e-05,
      "loss": 0.6308,
      "step": 139900
    },
    {
      "epoch": 1.992738146091414,
      "grad_norm": 0.7451350688934326,
      "learning_rate": 6.723111258844201e-05,
      "loss": 0.6365,
      "step": 139950
    },
    {
      "epoch": 1.99345009255304,
      "grad_norm": 0.5603907108306885,
      "learning_rate": 6.718362695284677e-05,
      "loss": 0.6856,
      "step": 140000
    },
    {
      "epoch": 1.994162039014666,
      "grad_norm": 0.39771050214767456,
      "learning_rate": 6.713614131725154e-05,
      "loss": 0.6247,
      "step": 140050
    },
    {
      "epoch": 1.9948739854762922,
      "grad_norm": 0.4724991023540497,
      "learning_rate": 6.70886556816563e-05,
      "loss": 0.6767,
      "step": 140100
    },
    {
      "epoch": 1.9955859319379181,
      "grad_norm": 0.7029452919960022,
      "learning_rate": 6.704117004606108e-05,
      "loss": 0.591,
      "step": 140150
    },
    {
      "epoch": 1.9962978783995444,
      "grad_norm": 0.46095335483551025,
      "learning_rate": 6.699368441046584e-05,
      "loss": 0.6624,
      "step": 140200
    },
    {
      "epoch": 1.9970098248611703,
      "grad_norm": 0.5027214884757996,
      "learning_rate": 6.69461987748706e-05,
      "loss": 0.6175,
      "step": 140250
    },
    {
      "epoch": 1.9977217713227966,
      "grad_norm": 0.7013969421386719,
      "learning_rate": 6.689871313927537e-05,
      "loss": 0.6125,
      "step": 140300
    },
    {
      "epoch": 1.9984337177844225,
      "grad_norm": 0.42757976055145264,
      "learning_rate": 6.685122750368013e-05,
      "loss": 0.6235,
      "step": 140350
    },
    {
      "epoch": 1.9991456642460488,
      "grad_norm": 0.9854925870895386,
      "learning_rate": 6.680374186808491e-05,
      "loss": 0.6502,
      "step": 140400
    },
    {
      "epoch": 1.9998576107076746,
      "grad_norm": 0.32692262530326843,
      "learning_rate": 6.675625623248968e-05,
      "loss": 0.6753,
      "step": 140450
    },
    {
      "epoch": 2.000569557169301,
      "grad_norm": 0.3669075667858124,
      "learning_rate": 6.670877059689444e-05,
      "loss": 0.6264,
      "step": 140500
    },
    {
      "epoch": 2.001281503630927,
      "grad_norm": 0.49833908677101135,
      "learning_rate": 6.66612849612992e-05,
      "loss": 0.6474,
      "step": 140550
    },
    {
      "epoch": 2.001993450092553,
      "grad_norm": 0.9368237257003784,
      "learning_rate": 6.661379932570397e-05,
      "loss": 0.6437,
      "step": 140600
    },
    {
      "epoch": 2.002705396554179,
      "grad_norm": 0.7920169830322266,
      "learning_rate": 6.656631369010875e-05,
      "loss": 0.5718,
      "step": 140650
    },
    {
      "epoch": 2.0034173430158053,
      "grad_norm": 0.48541563749313354,
      "learning_rate": 6.651882805451351e-05,
      "loss": 0.6755,
      "step": 140700
    },
    {
      "epoch": 2.004129289477431,
      "grad_norm": 0.8764333128929138,
      "learning_rate": 6.647134241891829e-05,
      "loss": 0.6259,
      "step": 140750
    },
    {
      "epoch": 2.0048412359390575,
      "grad_norm": 0.5175334811210632,
      "learning_rate": 6.642385678332305e-05,
      "loss": 0.6435,
      "step": 140800
    },
    {
      "epoch": 2.0055531824006834,
      "grad_norm": 0.2662852704524994,
      "learning_rate": 6.637637114772782e-05,
      "loss": 0.6395,
      "step": 140850
    },
    {
      "epoch": 2.0062651288623097,
      "grad_norm": 0.3508296608924866,
      "learning_rate": 6.632888551213259e-05,
      "loss": 0.628,
      "step": 140900
    },
    {
      "epoch": 2.0069770753239355,
      "grad_norm": 0.5598146319389343,
      "learning_rate": 6.628139987653736e-05,
      "loss": 0.5893,
      "step": 140950
    },
    {
      "epoch": 2.007689021785562,
      "grad_norm": 0.5556359887123108,
      "learning_rate": 6.623486395365402e-05,
      "loss": 0.615,
      "step": 141000
    },
    {
      "epoch": 2.0084009682471877,
      "grad_norm": 0.5408344864845276,
      "learning_rate": 6.618737831805879e-05,
      "loss": 0.6339,
      "step": 141050
    },
    {
      "epoch": 2.009112914708814,
      "grad_norm": 0.7564074397087097,
      "learning_rate": 6.613989268246355e-05,
      "loss": 0.6387,
      "step": 141100
    },
    {
      "epoch": 2.00982486117044,
      "grad_norm": 0.5152608156204224,
      "learning_rate": 6.609240704686832e-05,
      "loss": 0.6026,
      "step": 141150
    },
    {
      "epoch": 2.010536807632066,
      "grad_norm": 0.6106404066085815,
      "learning_rate": 6.604492141127309e-05,
      "loss": 0.6376,
      "step": 141200
    },
    {
      "epoch": 2.011248754093692,
      "grad_norm": 0.4782963693141937,
      "learning_rate": 6.599743577567786e-05,
      "loss": 0.5833,
      "step": 141250
    },
    {
      "epoch": 2.0119607005553184,
      "grad_norm": 0.7395342588424683,
      "learning_rate": 6.594995014008263e-05,
      "loss": 0.6552,
      "step": 141300
    },
    {
      "epoch": 2.0126726470169443,
      "grad_norm": 0.5163690447807312,
      "learning_rate": 6.59024645044874e-05,
      "loss": 0.6209,
      "step": 141350
    },
    {
      "epoch": 2.0133845934785706,
      "grad_norm": 0.5050812363624573,
      "learning_rate": 6.585497886889216e-05,
      "loss": 0.6034,
      "step": 141400
    },
    {
      "epoch": 2.0140965399401964,
      "grad_norm": 0.49929532408714294,
      "learning_rate": 6.580749323329694e-05,
      "loss": 0.6508,
      "step": 141450
    },
    {
      "epoch": 2.0148084864018228,
      "grad_norm": 0.5514919757843018,
      "learning_rate": 6.57600075977017e-05,
      "loss": 0.6092,
      "step": 141500
    },
    {
      "epoch": 2.0155204328634486,
      "grad_norm": 0.6213619112968445,
      "learning_rate": 6.571252196210647e-05,
      "loss": 0.617,
      "step": 141550
    },
    {
      "epoch": 2.016232379325075,
      "grad_norm": 0.7237138748168945,
      "learning_rate": 6.566503632651123e-05,
      "loss": 0.5925,
      "step": 141600
    },
    {
      "epoch": 2.016944325786701,
      "grad_norm": 0.605317234992981,
      "learning_rate": 6.5617550690916e-05,
      "loss": 0.6739,
      "step": 141650
    },
    {
      "epoch": 2.017656272248327,
      "grad_norm": 0.6835228204727173,
      "learning_rate": 6.557006505532077e-05,
      "loss": 0.643,
      "step": 141700
    },
    {
      "epoch": 2.018368218709953,
      "grad_norm": 0.5797150135040283,
      "learning_rate": 6.552257941972554e-05,
      "loss": 0.6297,
      "step": 141750
    },
    {
      "epoch": 2.0190801651715793,
      "grad_norm": 0.49242010712623596,
      "learning_rate": 6.54750937841303e-05,
      "loss": 0.6218,
      "step": 141800
    },
    {
      "epoch": 2.019792111633205,
      "grad_norm": 0.5811387300491333,
      "learning_rate": 6.542760814853507e-05,
      "loss": 0.5705,
      "step": 141850
    },
    {
      "epoch": 2.0205040580948315,
      "grad_norm": 0.4979524314403534,
      "learning_rate": 6.538012251293983e-05,
      "loss": 0.6665,
      "step": 141900
    },
    {
      "epoch": 2.0212160045564573,
      "grad_norm": 0.4649796783924103,
      "learning_rate": 6.533263687734461e-05,
      "loss": 0.5814,
      "step": 141950
    },
    {
      "epoch": 2.0219279510180836,
      "grad_norm": 0.5094455480575562,
      "learning_rate": 6.528515124174937e-05,
      "loss": 0.6769,
      "step": 142000
    },
    {
      "epoch": 2.0226398974797095,
      "grad_norm": 0.6086083650588989,
      "learning_rate": 6.523766560615414e-05,
      "loss": 0.6212,
      "step": 142050
    },
    {
      "epoch": 2.023351843941336,
      "grad_norm": 0.3948025703430176,
      "learning_rate": 6.51901799705589e-05,
      "loss": 0.6391,
      "step": 142100
    },
    {
      "epoch": 2.0240637904029617,
      "grad_norm": 0.6371327638626099,
      "learning_rate": 6.514269433496368e-05,
      "loss": 0.602,
      "step": 142150
    },
    {
      "epoch": 2.024775736864588,
      "grad_norm": 0.3919886648654938,
      "learning_rate": 6.509520869936846e-05,
      "loss": 0.5968,
      "step": 142200
    },
    {
      "epoch": 2.025487683326214,
      "grad_norm": 0.6428747773170471,
      "learning_rate": 6.504772306377322e-05,
      "loss": 0.602,
      "step": 142250
    },
    {
      "epoch": 2.0261996297878397,
      "grad_norm": 0.5943619012832642,
      "learning_rate": 6.500023742817798e-05,
      "loss": 0.611,
      "step": 142300
    },
    {
      "epoch": 2.026911576249466,
      "grad_norm": 0.6244820356369019,
      "learning_rate": 6.495275179258275e-05,
      "loss": 0.657,
      "step": 142350
    },
    {
      "epoch": 2.027623522711092,
      "grad_norm": 0.5664207339286804,
      "learning_rate": 6.490526615698751e-05,
      "loss": 0.5695,
      "step": 142400
    },
    {
      "epoch": 2.0283354691727182,
      "grad_norm": 0.40808793902397156,
      "learning_rate": 6.485778052139229e-05,
      "loss": 0.6407,
      "step": 142450
    },
    {
      "epoch": 2.029047415634344,
      "grad_norm": 0.5246437788009644,
      "learning_rate": 6.481029488579705e-05,
      "loss": 0.6599,
      "step": 142500
    },
    {
      "epoch": 2.0297593620959704,
      "grad_norm": 0.5499512553215027,
      "learning_rate": 6.476280925020182e-05,
      "loss": 0.6436,
      "step": 142550
    },
    {
      "epoch": 2.0304713085575963,
      "grad_norm": 0.7765212059020996,
      "learning_rate": 6.471532361460658e-05,
      "loss": 0.6151,
      "step": 142600
    },
    {
      "epoch": 2.0311832550192226,
      "grad_norm": 0.4861809313297272,
      "learning_rate": 6.466783797901135e-05,
      "loss": 0.6541,
      "step": 142650
    },
    {
      "epoch": 2.0318952014808485,
      "grad_norm": 0.6185715198516846,
      "learning_rate": 6.462035234341612e-05,
      "loss": 0.6444,
      "step": 142700
    },
    {
      "epoch": 2.0326071479424748,
      "grad_norm": 0.6554602384567261,
      "learning_rate": 6.457286670782089e-05,
      "loss": 0.5935,
      "step": 142750
    },
    {
      "epoch": 2.0333190944041006,
      "grad_norm": 0.5826871395111084,
      "learning_rate": 6.452538107222565e-05,
      "loss": 0.6633,
      "step": 142800
    },
    {
      "epoch": 2.034031040865727,
      "grad_norm": 0.7091745138168335,
      "learning_rate": 6.447789543663042e-05,
      "loss": 0.5801,
      "step": 142850
    },
    {
      "epoch": 2.034742987327353,
      "grad_norm": 0.5289116501808167,
      "learning_rate": 6.44313595137471e-05,
      "loss": 0.6586,
      "step": 142900
    },
    {
      "epoch": 2.035454933788979,
      "grad_norm": 0.5423290133476257,
      "learning_rate": 6.438387387815186e-05,
      "loss": 0.5993,
      "step": 142950
    },
    {
      "epoch": 2.036166880250605,
      "grad_norm": 0.7910626530647278,
      "learning_rate": 6.433638824255664e-05,
      "loss": 0.579,
      "step": 143000
    },
    {
      "epoch": 2.0368788267122313,
      "grad_norm": 0.5365270972251892,
      "learning_rate": 6.42889026069614e-05,
      "loss": 0.6614,
      "step": 143050
    },
    {
      "epoch": 2.037590773173857,
      "grad_norm": 0.5872582197189331,
      "learning_rate": 6.424141697136616e-05,
      "loss": 0.6028,
      "step": 143100
    },
    {
      "epoch": 2.0383027196354835,
      "grad_norm": 0.5620079040527344,
      "learning_rate": 6.419393133577093e-05,
      "loss": 0.6349,
      "step": 143150
    },
    {
      "epoch": 2.0390146660971094,
      "grad_norm": 0.6986063122749329,
      "learning_rate": 6.414644570017569e-05,
      "loss": 0.6664,
      "step": 143200
    },
    {
      "epoch": 2.0397266125587357,
      "grad_norm": 0.5142303109169006,
      "learning_rate": 6.409896006458047e-05,
      "loss": 0.624,
      "step": 143250
    },
    {
      "epoch": 2.0404385590203615,
      "grad_norm": 0.5532731413841248,
      "learning_rate": 6.405147442898523e-05,
      "loss": 0.5718,
      "step": 143300
    },
    {
      "epoch": 2.041150505481988,
      "grad_norm": 0.5065169930458069,
      "learning_rate": 6.400398879339e-05,
      "loss": 0.6262,
      "step": 143350
    },
    {
      "epoch": 2.0418624519436137,
      "grad_norm": 0.7303428053855896,
      "learning_rate": 6.395650315779476e-05,
      "loss": 0.5943,
      "step": 143400
    },
    {
      "epoch": 2.04257439840524,
      "grad_norm": 0.6146771311759949,
      "learning_rate": 6.390901752219953e-05,
      "loss": 0.6423,
      "step": 143450
    },
    {
      "epoch": 2.043286344866866,
      "grad_norm": 0.6726312041282654,
      "learning_rate": 6.38615318866043e-05,
      "loss": 0.5989,
      "step": 143500
    },
    {
      "epoch": 2.043998291328492,
      "grad_norm": 0.48454394936561584,
      "learning_rate": 6.381404625100907e-05,
      "loss": 0.6075,
      "step": 143550
    },
    {
      "epoch": 2.044710237790118,
      "grad_norm": 0.6072821021080017,
      "learning_rate": 6.376656061541385e-05,
      "loss": 0.6722,
      "step": 143600
    },
    {
      "epoch": 2.0454221842517444,
      "grad_norm": 0.7409032583236694,
      "learning_rate": 6.371907497981861e-05,
      "loss": 0.5862,
      "step": 143650
    },
    {
      "epoch": 2.0461341307133702,
      "grad_norm": 0.7121456861495972,
      "learning_rate": 6.367158934422337e-05,
      "loss": 0.5864,
      "step": 143700
    },
    {
      "epoch": 2.0468460771749966,
      "grad_norm": 0.6903724670410156,
      "learning_rate": 6.362410370862815e-05,
      "loss": 0.6379,
      "step": 143750
    },
    {
      "epoch": 2.0475580236366224,
      "grad_norm": 0.5141860842704773,
      "learning_rate": 6.357661807303292e-05,
      "loss": 0.6557,
      "step": 143800
    },
    {
      "epoch": 2.0482699700982487,
      "grad_norm": 0.8543063402175903,
      "learning_rate": 6.352913243743768e-05,
      "loss": 0.6599,
      "step": 143850
    },
    {
      "epoch": 2.0489819165598746,
      "grad_norm": 0.5162461996078491,
      "learning_rate": 6.348164680184244e-05,
      "loss": 0.584,
      "step": 143900
    },
    {
      "epoch": 2.049693863021501,
      "grad_norm": 0.4652576446533203,
      "learning_rate": 6.343416116624721e-05,
      "loss": 0.6269,
      "step": 143950
    },
    {
      "epoch": 2.050405809483127,
      "grad_norm": 0.8137010335922241,
      "learning_rate": 6.338667553065198e-05,
      "loss": 0.6675,
      "step": 144000
    },
    {
      "epoch": 2.051117755944753,
      "grad_norm": 0.3848890960216522,
      "learning_rate": 6.333918989505675e-05,
      "loss": 0.6256,
      "step": 144050
    },
    {
      "epoch": 2.051829702406379,
      "grad_norm": 0.7058213353157043,
      "learning_rate": 6.329170425946151e-05,
      "loss": 0.5697,
      "step": 144100
    },
    {
      "epoch": 2.0525416488680053,
      "grad_norm": 0.5169538855552673,
      "learning_rate": 6.324421862386628e-05,
      "loss": 0.642,
      "step": 144150
    },
    {
      "epoch": 2.053253595329631,
      "grad_norm": 0.3972679376602173,
      "learning_rate": 6.319673298827104e-05,
      "loss": 0.5656,
      "step": 144200
    },
    {
      "epoch": 2.0539655417912575,
      "grad_norm": 0.5859609246253967,
      "learning_rate": 6.314924735267582e-05,
      "loss": 0.6114,
      "step": 144250
    },
    {
      "epoch": 2.0546774882528833,
      "grad_norm": 0.8170880079269409,
      "learning_rate": 6.310176171708058e-05,
      "loss": 0.5946,
      "step": 144300
    },
    {
      "epoch": 2.0553894347145096,
      "grad_norm": 0.37693196535110474,
      "learning_rate": 6.305427608148535e-05,
      "loss": 0.596,
      "step": 144350
    },
    {
      "epoch": 2.0561013811761355,
      "grad_norm": 0.8982353806495667,
      "learning_rate": 6.300679044589012e-05,
      "loss": 0.6084,
      "step": 144400
    },
    {
      "epoch": 2.056813327637762,
      "grad_norm": 0.3880033493041992,
      "learning_rate": 6.295930481029489e-05,
      "loss": 0.661,
      "step": 144450
    },
    {
      "epoch": 2.0575252740993877,
      "grad_norm": 0.5120803117752075,
      "learning_rate": 6.291181917469967e-05,
      "loss": 0.6051,
      "step": 144500
    },
    {
      "epoch": 2.058237220561014,
      "grad_norm": 0.31724461913108826,
      "learning_rate": 6.286433353910443e-05,
      "loss": 0.6172,
      "step": 144550
    },
    {
      "epoch": 2.05894916702264,
      "grad_norm": 0.6398324966430664,
      "learning_rate": 6.28168479035092e-05,
      "loss": 0.6053,
      "step": 144600
    },
    {
      "epoch": 2.059661113484266,
      "grad_norm": 0.75981205701828,
      "learning_rate": 6.276936226791396e-05,
      "loss": 0.6549,
      "step": 144650
    },
    {
      "epoch": 2.060373059945892,
      "grad_norm": 0.5803637504577637,
      "learning_rate": 6.272187663231872e-05,
      "loss": 0.6236,
      "step": 144700
    },
    {
      "epoch": 2.0610850064075183,
      "grad_norm": 0.6529368758201599,
      "learning_rate": 6.26743909967235e-05,
      "loss": 0.5827,
      "step": 144750
    },
    {
      "epoch": 2.061796952869144,
      "grad_norm": 0.7181974053382874,
      "learning_rate": 6.262690536112826e-05,
      "loss": 0.6586,
      "step": 144800
    },
    {
      "epoch": 2.0625088993307705,
      "grad_norm": 0.41823771595954895,
      "learning_rate": 6.257941972553303e-05,
      "loss": 0.6672,
      "step": 144850
    },
    {
      "epoch": 2.0632208457923964,
      "grad_norm": 0.48724767565727234,
      "learning_rate": 6.253193408993779e-05,
      "loss": 0.6401,
      "step": 144900
    },
    {
      "epoch": 2.0639327922540227,
      "grad_norm": 0.6244952082633972,
      "learning_rate": 6.248444845434256e-05,
      "loss": 0.5927,
      "step": 144950
    },
    {
      "epoch": 2.0646447387156486,
      "grad_norm": 0.4372166097164154,
      "learning_rate": 6.243696281874733e-05,
      "loss": 0.6214,
      "step": 145000
    },
    {
      "epoch": 2.0653566851772744,
      "grad_norm": 0.5715600848197937,
      "learning_rate": 6.23894771831521e-05,
      "loss": 0.5759,
      "step": 145050
    },
    {
      "epoch": 2.0660686316389008,
      "grad_norm": 0.6825013756752014,
      "learning_rate": 6.234199154755686e-05,
      "loss": 0.6875,
      "step": 145100
    },
    {
      "epoch": 2.066780578100527,
      "grad_norm": 0.4447938799858093,
      "learning_rate": 6.229450591196163e-05,
      "loss": 0.6593,
      "step": 145150
    },
    {
      "epoch": 2.067492524562153,
      "grad_norm": 0.6862852573394775,
      "learning_rate": 6.22470202763664e-05,
      "loss": 0.6721,
      "step": 145200
    },
    {
      "epoch": 2.068204471023779,
      "grad_norm": 0.8865388035774231,
      "learning_rate": 6.219953464077117e-05,
      "loss": 0.5931,
      "step": 145250
    },
    {
      "epoch": 2.068916417485405,
      "grad_norm": 0.5485848784446716,
      "learning_rate": 6.215204900517595e-05,
      "loss": 0.61,
      "step": 145300
    },
    {
      "epoch": 2.069628363947031,
      "grad_norm": 0.7698894143104553,
      "learning_rate": 6.210456336958071e-05,
      "loss": 0.6597,
      "step": 145350
    },
    {
      "epoch": 2.0703403104086573,
      "grad_norm": 0.4836852252483368,
      "learning_rate": 6.205707773398547e-05,
      "loss": 0.5942,
      "step": 145400
    },
    {
      "epoch": 2.071052256870283,
      "grad_norm": 1.029529333114624,
      "learning_rate": 6.200959209839024e-05,
      "loss": 0.6441,
      "step": 145450
    },
    {
      "epoch": 2.0717642033319095,
      "grad_norm": 0.5760820508003235,
      "learning_rate": 6.196210646279501e-05,
      "loss": 0.6765,
      "step": 145500
    },
    {
      "epoch": 2.0724761497935353,
      "grad_norm": 0.6701491475105286,
      "learning_rate": 6.191462082719978e-05,
      "loss": 0.6443,
      "step": 145550
    },
    {
      "epoch": 2.0731880962551616,
      "grad_norm": 0.8514748811721802,
      "learning_rate": 6.186713519160454e-05,
      "loss": 0.5955,
      "step": 145600
    },
    {
      "epoch": 2.0739000427167875,
      "grad_norm": 0.57916259765625,
      "learning_rate": 6.181964955600931e-05,
      "loss": 0.6919,
      "step": 145650
    },
    {
      "epoch": 2.074611989178414,
      "grad_norm": 0.4252344071865082,
      "learning_rate": 6.177216392041407e-05,
      "loss": 0.6765,
      "step": 145700
    },
    {
      "epoch": 2.0753239356400397,
      "grad_norm": 0.6033434867858887,
      "learning_rate": 6.172467828481885e-05,
      "loss": 0.651,
      "step": 145750
    },
    {
      "epoch": 2.076035882101666,
      "grad_norm": 0.6558444499969482,
      "learning_rate": 6.167719264922361e-05,
      "loss": 0.6414,
      "step": 145800
    },
    {
      "epoch": 2.076747828563292,
      "grad_norm": 0.8531479835510254,
      "learning_rate": 6.162970701362838e-05,
      "loss": 0.5794,
      "step": 145850
    },
    {
      "epoch": 2.077459775024918,
      "grad_norm": 0.7643375992774963,
      "learning_rate": 6.158222137803314e-05,
      "loss": 0.6194,
      "step": 145900
    },
    {
      "epoch": 2.078171721486544,
      "grad_norm": 0.5689430236816406,
      "learning_rate": 6.15347357424379e-05,
      "loss": 0.5727,
      "step": 145950
    },
    {
      "epoch": 2.0788836679481704,
      "grad_norm": 0.5784677267074585,
      "learning_rate": 6.148725010684268e-05,
      "loss": 0.6567,
      "step": 146000
    },
    {
      "epoch": 2.0795956144097962,
      "grad_norm": 0.3643938899040222,
      "learning_rate": 6.143976447124745e-05,
      "loss": 0.628,
      "step": 146050
    },
    {
      "epoch": 2.0803075608714225,
      "grad_norm": 0.48748064041137695,
      "learning_rate": 6.139227883565221e-05,
      "loss": 0.6432,
      "step": 146100
    },
    {
      "epoch": 2.0810195073330484,
      "grad_norm": 0.8931548595428467,
      "learning_rate": 6.134479320005699e-05,
      "loss": 0.6066,
      "step": 146150
    },
    {
      "epoch": 2.0817314537946747,
      "grad_norm": 0.6831908822059631,
      "learning_rate": 6.129730756446175e-05,
      "loss": 0.6242,
      "step": 146200
    },
    {
      "epoch": 2.0824434002563006,
      "grad_norm": 0.3750481605529785,
      "learning_rate": 6.124982192886653e-05,
      "loss": 0.6151,
      "step": 146250
    },
    {
      "epoch": 2.083155346717927,
      "grad_norm": 0.4741310477256775,
      "learning_rate": 6.12023362932713e-05,
      "loss": 0.6226,
      "step": 146300
    },
    {
      "epoch": 2.0838672931795528,
      "grad_norm": 0.5205668210983276,
      "learning_rate": 6.115485065767606e-05,
      "loss": 0.6206,
      "step": 146350
    },
    {
      "epoch": 2.084579239641179,
      "grad_norm": 0.49780580401420593,
      "learning_rate": 6.110736502208082e-05,
      "loss": 0.5968,
      "step": 146400
    },
    {
      "epoch": 2.085291186102805,
      "grad_norm": 0.7058145403862,
      "learning_rate": 6.105987938648559e-05,
      "loss": 0.6348,
      "step": 146450
    },
    {
      "epoch": 2.0860031325644313,
      "grad_norm": 0.25367507338523865,
      "learning_rate": 6.1012393750890364e-05,
      "loss": 0.5824,
      "step": 146500
    },
    {
      "epoch": 2.086715079026057,
      "grad_norm": 0.4899112284183502,
      "learning_rate": 6.096490811529513e-05,
      "loss": 0.6544,
      "step": 146550
    },
    {
      "epoch": 2.0874270254876834,
      "grad_norm": 0.5500165820121765,
      "learning_rate": 6.091742247969989e-05,
      "loss": 0.6305,
      "step": 146600
    },
    {
      "epoch": 2.0881389719493093,
      "grad_norm": 0.410177618265152,
      "learning_rate": 6.0869936844104656e-05,
      "loss": 0.6315,
      "step": 146650
    },
    {
      "epoch": 2.0888509184109356,
      "grad_norm": 0.5237023234367371,
      "learning_rate": 6.082245120850942e-05,
      "loss": 0.6312,
      "step": 146700
    },
    {
      "epoch": 2.0895628648725615,
      "grad_norm": 0.6954662799835205,
      "learning_rate": 6.07749655729142e-05,
      "loss": 0.6375,
      "step": 146750
    },
    {
      "epoch": 2.090274811334188,
      "grad_norm": 0.7184659242630005,
      "learning_rate": 6.072747993731897e-05,
      "loss": 0.6912,
      "step": 146800
    },
    {
      "epoch": 2.0909867577958137,
      "grad_norm": 0.48260265588760376,
      "learning_rate": 6.067999430172373e-05,
      "loss": 0.5912,
      "step": 146850
    },
    {
      "epoch": 2.09169870425744,
      "grad_norm": 0.4652397334575653,
      "learning_rate": 6.0632508666128497e-05,
      "loss": 0.6343,
      "step": 146900
    },
    {
      "epoch": 2.092410650719066,
      "grad_norm": 1.0018056631088257,
      "learning_rate": 6.058502303053326e-05,
      "loss": 0.6748,
      "step": 146950
    },
    {
      "epoch": 2.093122597180692,
      "grad_norm": 0.8467209935188293,
      "learning_rate": 6.053753739493804e-05,
      "loss": 0.6255,
      "step": 147000
    },
    {
      "epoch": 2.093834543642318,
      "grad_norm": 0.5686942934989929,
      "learning_rate": 6.04900517593428e-05,
      "loss": 0.6127,
      "step": 147050
    },
    {
      "epoch": 2.0945464901039443,
      "grad_norm": 0.656711757183075,
      "learning_rate": 6.0442566123747566e-05,
      "loss": 0.5927,
      "step": 147100
    },
    {
      "epoch": 2.09525843656557,
      "grad_norm": 0.6713650226593018,
      "learning_rate": 6.039508048815233e-05,
      "loss": 0.607,
      "step": 147150
    },
    {
      "epoch": 2.0959703830271965,
      "grad_norm": 0.6043956875801086,
      "learning_rate": 6.03475948525571e-05,
      "loss": 0.5955,
      "step": 147200
    },
    {
      "epoch": 2.0966823294888224,
      "grad_norm": 0.4495769441127777,
      "learning_rate": 6.030010921696187e-05,
      "loss": 0.6258,
      "step": 147250
    },
    {
      "epoch": 2.0973942759504487,
      "grad_norm": 0.4409615993499756,
      "learning_rate": 6.025262358136664e-05,
      "loss": 0.6197,
      "step": 147300
    },
    {
      "epoch": 2.0981062224120746,
      "grad_norm": 0.4817388951778412,
      "learning_rate": 6.020513794577141e-05,
      "loss": 0.6182,
      "step": 147350
    },
    {
      "epoch": 2.098818168873701,
      "grad_norm": 0.8958005309104919,
      "learning_rate": 6.015860202288808e-05,
      "loss": 0.6502,
      "step": 147400
    },
    {
      "epoch": 2.0995301153353267,
      "grad_norm": 0.8514620065689087,
      "learning_rate": 6.0111116387292844e-05,
      "loss": 0.6156,
      "step": 147450
    },
    {
      "epoch": 2.100242061796953,
      "grad_norm": 0.5136885046958923,
      "learning_rate": 6.006363075169762e-05,
      "loss": 0.5837,
      "step": 147500
    },
    {
      "epoch": 2.100954008258579,
      "grad_norm": 0.569273054599762,
      "learning_rate": 6.0016145116102385e-05,
      "loss": 0.642,
      "step": 147550
    },
    {
      "epoch": 2.1016659547202052,
      "grad_norm": 0.5861627459526062,
      "learning_rate": 5.996865948050715e-05,
      "loss": 0.6273,
      "step": 147600
    },
    {
      "epoch": 2.102377901181831,
      "grad_norm": 0.7672524452209473,
      "learning_rate": 5.992117384491191e-05,
      "loss": 0.5991,
      "step": 147650
    },
    {
      "epoch": 2.1030898476434574,
      "grad_norm": 0.6207942366600037,
      "learning_rate": 5.987368820931668e-05,
      "loss": 0.6269,
      "step": 147700
    },
    {
      "epoch": 2.1038017941050833,
      "grad_norm": 0.6828775405883789,
      "learning_rate": 5.9826202573721455e-05,
      "loss": 0.6118,
      "step": 147750
    },
    {
      "epoch": 2.1045137405667096,
      "grad_norm": 0.592569887638092,
      "learning_rate": 5.977871693812622e-05,
      "loss": 0.6722,
      "step": 147800
    },
    {
      "epoch": 2.1052256870283355,
      "grad_norm": 0.4716261327266693,
      "learning_rate": 5.973123130253099e-05,
      "loss": 0.6598,
      "step": 147850
    },
    {
      "epoch": 2.1059376334899618,
      "grad_norm": 0.7176535725593567,
      "learning_rate": 5.9683745666935754e-05,
      "loss": 0.6534,
      "step": 147900
    },
    {
      "epoch": 2.1066495799515876,
      "grad_norm": 0.6406491994857788,
      "learning_rate": 5.963626003134052e-05,
      "loss": 0.58,
      "step": 147950
    },
    {
      "epoch": 2.1073615264132135,
      "grad_norm": 0.4914475083351135,
      "learning_rate": 5.9588774395745296e-05,
      "loss": 0.6423,
      "step": 148000
    },
    {
      "epoch": 2.10807347287484,
      "grad_norm": 0.8070460557937622,
      "learning_rate": 5.954128876015006e-05,
      "loss": 0.6456,
      "step": 148050
    },
    {
      "epoch": 2.1087854193364657,
      "grad_norm": 0.2902727425098419,
      "learning_rate": 5.9493803124554824e-05,
      "loss": 0.6462,
      "step": 148100
    },
    {
      "epoch": 2.109497365798092,
      "grad_norm": 0.5657382607460022,
      "learning_rate": 5.944631748895959e-05,
      "loss": 0.6003,
      "step": 148150
    },
    {
      "epoch": 2.110209312259718,
      "grad_norm": 0.356279194355011,
      "learning_rate": 5.939883185336436e-05,
      "loss": 0.6639,
      "step": 148200
    },
    {
      "epoch": 2.110921258721344,
      "grad_norm": 0.5210683345794678,
      "learning_rate": 5.935134621776913e-05,
      "loss": 0.5931,
      "step": 148250
    },
    {
      "epoch": 2.11163320518297,
      "grad_norm": 0.6327348351478577,
      "learning_rate": 5.93038605821739e-05,
      "loss": 0.6388,
      "step": 148300
    },
    {
      "epoch": 2.1123451516445964,
      "grad_norm": 0.48532217741012573,
      "learning_rate": 5.9256374946578664e-05,
      "loss": 0.6209,
      "step": 148350
    },
    {
      "epoch": 2.113057098106222,
      "grad_norm": 0.6315271258354187,
      "learning_rate": 5.920888931098343e-05,
      "loss": 0.5611,
      "step": 148400
    },
    {
      "epoch": 2.1137690445678485,
      "grad_norm": 0.4964018762111664,
      "learning_rate": 5.916140367538819e-05,
      "loss": 0.5842,
      "step": 148450
    },
    {
      "epoch": 2.1144809910294744,
      "grad_norm": 0.39405789971351624,
      "learning_rate": 5.911391803979297e-05,
      "loss": 0.6024,
      "step": 148500
    },
    {
      "epoch": 2.1151929374911007,
      "grad_norm": 0.4906402826309204,
      "learning_rate": 5.9066432404197734e-05,
      "loss": 0.6441,
      "step": 148550
    },
    {
      "epoch": 2.1159048839527266,
      "grad_norm": 1.1034377813339233,
      "learning_rate": 5.90189467686025e-05,
      "loss": 0.6535,
      "step": 148600
    },
    {
      "epoch": 2.116616830414353,
      "grad_norm": 0.4338870644569397,
      "learning_rate": 5.897146113300727e-05,
      "loss": 0.6067,
      "step": 148650
    },
    {
      "epoch": 2.1173287768759788,
      "grad_norm": 0.8888185024261475,
      "learning_rate": 5.892397549741203e-05,
      "loss": 0.648,
      "step": 148700
    },
    {
      "epoch": 2.118040723337605,
      "grad_norm": 0.6835840940475464,
      "learning_rate": 5.887648986181681e-05,
      "loss": 0.6469,
      "step": 148750
    },
    {
      "epoch": 2.118752669799231,
      "grad_norm": 0.5075716972351074,
      "learning_rate": 5.8829004226221575e-05,
      "loss": 0.6301,
      "step": 148800
    },
    {
      "epoch": 2.1194646162608572,
      "grad_norm": 0.730305552482605,
      "learning_rate": 5.878151859062634e-05,
      "loss": 0.5982,
      "step": 148850
    },
    {
      "epoch": 2.120176562722483,
      "grad_norm": 0.7464948892593384,
      "learning_rate": 5.87340329550311e-05,
      "loss": 0.5531,
      "step": 148900
    },
    {
      "epoch": 2.1208885091841094,
      "grad_norm": 0.6571973562240601,
      "learning_rate": 5.868654731943587e-05,
      "loss": 0.6688,
      "step": 148950
    },
    {
      "epoch": 2.1216004556457353,
      "grad_norm": 0.3539660573005676,
      "learning_rate": 5.8639061683840645e-05,
      "loss": 0.6213,
      "step": 149000
    },
    {
      "epoch": 2.1223124021073616,
      "grad_norm": 0.7065044641494751,
      "learning_rate": 5.859157604824541e-05,
      "loss": 0.651,
      "step": 149050
    },
    {
      "epoch": 2.1230243485689875,
      "grad_norm": 0.5686415433883667,
      "learning_rate": 5.854409041265018e-05,
      "loss": 0.594,
      "step": 149100
    },
    {
      "epoch": 2.123736295030614,
      "grad_norm": 0.5005928874015808,
      "learning_rate": 5.8496604777054943e-05,
      "loss": 0.5733,
      "step": 149150
    },
    {
      "epoch": 2.1244482414922397,
      "grad_norm": 0.45584213733673096,
      "learning_rate": 5.844911914145971e-05,
      "loss": 0.6319,
      "step": 149200
    },
    {
      "epoch": 2.125160187953866,
      "grad_norm": 0.6071564555168152,
      "learning_rate": 5.8401633505864485e-05,
      "loss": 0.6335,
      "step": 149250
    },
    {
      "epoch": 2.125872134415492,
      "grad_norm": 0.5330239534378052,
      "learning_rate": 5.835414787026925e-05,
      "loss": 0.5957,
      "step": 149300
    },
    {
      "epoch": 2.126584080877118,
      "grad_norm": 0.5990018248558044,
      "learning_rate": 5.830666223467401e-05,
      "loss": 0.6186,
      "step": 149350
    },
    {
      "epoch": 2.127296027338744,
      "grad_norm": 0.3139130175113678,
      "learning_rate": 5.825917659907878e-05,
      "loss": 0.565,
      "step": 149400
    },
    {
      "epoch": 2.1280079738003703,
      "grad_norm": 0.5285298824310303,
      "learning_rate": 5.821169096348355e-05,
      "loss": 0.638,
      "step": 149450
    },
    {
      "epoch": 2.128719920261996,
      "grad_norm": 0.5758006572723389,
      "learning_rate": 5.816420532788832e-05,
      "loss": 0.6101,
      "step": 149500
    },
    {
      "epoch": 2.1294318667236225,
      "grad_norm": 0.7548537850379944,
      "learning_rate": 5.811671969229309e-05,
      "loss": 0.6084,
      "step": 149550
    },
    {
      "epoch": 2.1301438131852484,
      "grad_norm": 0.538284420967102,
      "learning_rate": 5.8070183769409756e-05,
      "loss": 0.5923,
      "step": 149600
    },
    {
      "epoch": 2.1308557596468747,
      "grad_norm": 0.41330718994140625,
      "learning_rate": 5.8022698133814526e-05,
      "loss": 0.604,
      "step": 149650
    },
    {
      "epoch": 2.1315677061085005,
      "grad_norm": 0.6615563035011292,
      "learning_rate": 5.797521249821929e-05,
      "loss": 0.5619,
      "step": 149700
    },
    {
      "epoch": 2.132279652570127,
      "grad_norm": 0.31796130537986755,
      "learning_rate": 5.7927726862624055e-05,
      "loss": 0.5941,
      "step": 149750
    },
    {
      "epoch": 2.1329915990317527,
      "grad_norm": 0.6138759851455688,
      "learning_rate": 5.788024122702883e-05,
      "loss": 0.6326,
      "step": 149800
    },
    {
      "epoch": 2.133703545493379,
      "grad_norm": 0.8449473977088928,
      "learning_rate": 5.7832755591433596e-05,
      "loss": 0.5503,
      "step": 149850
    },
    {
      "epoch": 2.134415491955005,
      "grad_norm": 0.5671873092651367,
      "learning_rate": 5.778526995583836e-05,
      "loss": 0.6289,
      "step": 149900
    },
    {
      "epoch": 2.135127438416631,
      "grad_norm": 0.5129414796829224,
      "learning_rate": 5.7737784320243124e-05,
      "loss": 0.6383,
      "step": 149950
    },
    {
      "epoch": 2.135839384878257,
      "grad_norm": 0.4210037887096405,
      "learning_rate": 5.7690298684647895e-05,
      "loss": 0.6047,
      "step": 150000
    },
    {
      "epoch": 2.1365513313398834,
      "grad_norm": 0.722324788570404,
      "learning_rate": 5.7642813049052666e-05,
      "loss": 0.6268,
      "step": 150050
    },
    {
      "epoch": 2.1372632778015093,
      "grad_norm": 1.0427014827728271,
      "learning_rate": 5.759532741345744e-05,
      "loss": 0.5755,
      "step": 150100
    },
    {
      "epoch": 2.1379752242631356,
      "grad_norm": 0.6313612461090088,
      "learning_rate": 5.75478417778622e-05,
      "loss": 0.6253,
      "step": 150150
    },
    {
      "epoch": 2.1386871707247614,
      "grad_norm": 0.45870596170425415,
      "learning_rate": 5.7500356142266965e-05,
      "loss": 0.6599,
      "step": 150200
    },
    {
      "epoch": 2.1393991171863878,
      "grad_norm": 0.4046579897403717,
      "learning_rate": 5.745287050667173e-05,
      "loss": 0.6024,
      "step": 150250
    },
    {
      "epoch": 2.1401110636480136,
      "grad_norm": 0.3972684144973755,
      "learning_rate": 5.7405384871076507e-05,
      "loss": 0.5647,
      "step": 150300
    },
    {
      "epoch": 2.14082301010964,
      "grad_norm": 0.484793096780777,
      "learning_rate": 5.735789923548127e-05,
      "loss": 0.6117,
      "step": 150350
    },
    {
      "epoch": 2.141534956571266,
      "grad_norm": 0.48058003187179565,
      "learning_rate": 5.7310413599886035e-05,
      "loss": 0.6619,
      "step": 150400
    },
    {
      "epoch": 2.142246903032892,
      "grad_norm": 0.5621242523193359,
      "learning_rate": 5.7262927964290805e-05,
      "loss": 0.6161,
      "step": 150450
    },
    {
      "epoch": 2.142958849494518,
      "grad_norm": 0.4086088240146637,
      "learning_rate": 5.721544232869557e-05,
      "loss": 0.5857,
      "step": 150500
    },
    {
      "epoch": 2.1436707959561443,
      "grad_norm": 0.7589751482009888,
      "learning_rate": 5.716795669310035e-05,
      "loss": 0.6576,
      "step": 150550
    },
    {
      "epoch": 2.14438274241777,
      "grad_norm": 0.5216707587242126,
      "learning_rate": 5.712047105750511e-05,
      "loss": 0.5706,
      "step": 150600
    },
    {
      "epoch": 2.1450946888793965,
      "grad_norm": 0.7771121263504028,
      "learning_rate": 5.7072985421909875e-05,
      "loss": 0.6386,
      "step": 150650
    },
    {
      "epoch": 2.1458066353410223,
      "grad_norm": 0.4105930030345917,
      "learning_rate": 5.702549978631464e-05,
      "loss": 0.6097,
      "step": 150700
    },
    {
      "epoch": 2.146518581802648,
      "grad_norm": 0.5651963353157043,
      "learning_rate": 5.69780141507194e-05,
      "loss": 0.6525,
      "step": 150750
    },
    {
      "epoch": 2.1472305282642745,
      "grad_norm": 0.6510297656059265,
      "learning_rate": 5.693052851512418e-05,
      "loss": 0.6687,
      "step": 150800
    },
    {
      "epoch": 2.147942474725901,
      "grad_norm": 0.4951297342777252,
      "learning_rate": 5.6883042879528945e-05,
      "loss": 0.657,
      "step": 150850
    },
    {
      "epoch": 2.1486544211875267,
      "grad_norm": 0.399522989988327,
      "learning_rate": 5.6835557243933716e-05,
      "loss": 0.641,
      "step": 150900
    },
    {
      "epoch": 2.1493663676491526,
      "grad_norm": 0.40748950839042664,
      "learning_rate": 5.678807160833848e-05,
      "loss": 0.6813,
      "step": 150950
    },
    {
      "epoch": 2.150078314110779,
      "grad_norm": 0.6631960272789001,
      "learning_rate": 5.6740585972743244e-05,
      "loss": 0.6118,
      "step": 151000
    },
    {
      "epoch": 2.150790260572405,
      "grad_norm": 0.5707657933235168,
      "learning_rate": 5.669310033714802e-05,
      "loss": 0.6098,
      "step": 151050
    },
    {
      "epoch": 2.151502207034031,
      "grad_norm": 0.36264950037002563,
      "learning_rate": 5.6645614701552786e-05,
      "loss": 0.6353,
      "step": 151100
    },
    {
      "epoch": 2.152214153495657,
      "grad_norm": 0.3335471749305725,
      "learning_rate": 5.659812906595755e-05,
      "loss": 0.6218,
      "step": 151150
    },
    {
      "epoch": 2.1529260999572832,
      "grad_norm": 0.49013108015060425,
      "learning_rate": 5.6550643430362314e-05,
      "loss": 0.6519,
      "step": 151200
    },
    {
      "epoch": 2.153638046418909,
      "grad_norm": 0.7898183465003967,
      "learning_rate": 5.650315779476708e-05,
      "loss": 0.6631,
      "step": 151250
    },
    {
      "epoch": 2.1543499928805354,
      "grad_norm": 0.7423325181007385,
      "learning_rate": 5.6455672159171855e-05,
      "loss": 0.7312,
      "step": 151300
    },
    {
      "epoch": 2.1550619393421613,
      "grad_norm": 0.5231900215148926,
      "learning_rate": 5.640818652357662e-05,
      "loss": 0.5932,
      "step": 151350
    },
    {
      "epoch": 2.1557738858037876,
      "grad_norm": 0.8457293510437012,
      "learning_rate": 5.636070088798139e-05,
      "loss": 0.6439,
      "step": 151400
    },
    {
      "epoch": 2.1564858322654135,
      "grad_norm": 0.84354567527771,
      "learning_rate": 5.6313215252386154e-05,
      "loss": 0.5948,
      "step": 151450
    },
    {
      "epoch": 2.1571977787270398,
      "grad_norm": 0.5481840968132019,
      "learning_rate": 5.626572961679092e-05,
      "loss": 0.5895,
      "step": 151500
    },
    {
      "epoch": 2.1579097251886656,
      "grad_norm": 0.4818716049194336,
      "learning_rate": 5.6218243981195696e-05,
      "loss": 0.6619,
      "step": 151550
    },
    {
      "epoch": 2.158621671650292,
      "grad_norm": 0.4899216294288635,
      "learning_rate": 5.617170805831237e-05,
      "loss": 0.6094,
      "step": 151600
    },
    {
      "epoch": 2.159333618111918,
      "grad_norm": 0.4593103229999542,
      "learning_rate": 5.612517213542904e-05,
      "loss": 0.6137,
      "step": 151650
    },
    {
      "epoch": 2.160045564573544,
      "grad_norm": 0.6299600005149841,
      "learning_rate": 5.6077686499833805e-05,
      "loss": 0.5729,
      "step": 151700
    },
    {
      "epoch": 2.16075751103517,
      "grad_norm": 0.49090608954429626,
      "learning_rate": 5.603020086423857e-05,
      "loss": 0.6223,
      "step": 151750
    },
    {
      "epoch": 2.1614694574967963,
      "grad_norm": 0.7504133582115173,
      "learning_rate": 5.598271522864333e-05,
      "loss": 0.6535,
      "step": 151800
    },
    {
      "epoch": 2.162181403958422,
      "grad_norm": 0.9323362708091736,
      "learning_rate": 5.593522959304811e-05,
      "loss": 0.6158,
      "step": 151850
    },
    {
      "epoch": 2.1628933504200485,
      "grad_norm": 0.6853703260421753,
      "learning_rate": 5.5887743957452875e-05,
      "loss": 0.6525,
      "step": 151900
    },
    {
      "epoch": 2.1636052968816744,
      "grad_norm": 0.4412287771701813,
      "learning_rate": 5.584025832185764e-05,
      "loss": 0.5817,
      "step": 151950
    },
    {
      "epoch": 2.1643172433433007,
      "grad_norm": 0.6259970664978027,
      "learning_rate": 5.579277268626241e-05,
      "loss": 0.6046,
      "step": 152000
    },
    {
      "epoch": 2.1650291898049265,
      "grad_norm": 0.6463814377784729,
      "learning_rate": 5.5745287050667174e-05,
      "loss": 0.6534,
      "step": 152050
    },
    {
      "epoch": 2.165741136266553,
      "grad_norm": 0.6993852257728577,
      "learning_rate": 5.569780141507195e-05,
      "loss": 0.6029,
      "step": 152100
    },
    {
      "epoch": 2.1664530827281787,
      "grad_norm": 0.515244722366333,
      "learning_rate": 5.5650315779476716e-05,
      "loss": 0.6162,
      "step": 152150
    },
    {
      "epoch": 2.167165029189805,
      "grad_norm": 0.530760645866394,
      "learning_rate": 5.560283014388148e-05,
      "loss": 0.6004,
      "step": 152200
    },
    {
      "epoch": 2.167876975651431,
      "grad_norm": 0.8128235936164856,
      "learning_rate": 5.5555344508286244e-05,
      "loss": 0.6197,
      "step": 152250
    },
    {
      "epoch": 2.168588922113057,
      "grad_norm": 0.60881507396698,
      "learning_rate": 5.550785887269101e-05,
      "loss": 0.5918,
      "step": 152300
    },
    {
      "epoch": 2.169300868574683,
      "grad_norm": 0.6255040168762207,
      "learning_rate": 5.5460373237095785e-05,
      "loss": 0.6543,
      "step": 152350
    },
    {
      "epoch": 2.1700128150363094,
      "grad_norm": 0.6986767053604126,
      "learning_rate": 5.541288760150055e-05,
      "loss": 0.6549,
      "step": 152400
    },
    {
      "epoch": 2.1707247614979353,
      "grad_norm": 0.8168831467628479,
      "learning_rate": 5.536540196590532e-05,
      "loss": 0.6827,
      "step": 152450
    },
    {
      "epoch": 2.1714367079595616,
      "grad_norm": 0.5743753910064697,
      "learning_rate": 5.5317916330310084e-05,
      "loss": 0.614,
      "step": 152500
    },
    {
      "epoch": 2.1721486544211874,
      "grad_norm": 0.5786786675453186,
      "learning_rate": 5.527043069471485e-05,
      "loss": 0.5711,
      "step": 152550
    },
    {
      "epoch": 2.1728606008828137,
      "grad_norm": 0.748773992061615,
      "learning_rate": 5.5222945059119626e-05,
      "loss": 0.6369,
      "step": 152600
    },
    {
      "epoch": 2.1735725473444396,
      "grad_norm": 0.6299444437026978,
      "learning_rate": 5.517545942352439e-05,
      "loss": 0.588,
      "step": 152650
    },
    {
      "epoch": 2.174284493806066,
      "grad_norm": 0.33865776658058167,
      "learning_rate": 5.5127973787929154e-05,
      "loss": 0.6898,
      "step": 152700
    },
    {
      "epoch": 2.174996440267692,
      "grad_norm": 0.4192039966583252,
      "learning_rate": 5.508048815233392e-05,
      "loss": 0.6052,
      "step": 152750
    },
    {
      "epoch": 2.175708386729318,
      "grad_norm": 0.530876100063324,
      "learning_rate": 5.503300251673868e-05,
      "loss": 0.5785,
      "step": 152800
    },
    {
      "epoch": 2.176420333190944,
      "grad_norm": 0.4525507688522339,
      "learning_rate": 5.498551688114346e-05,
      "loss": 0.6439,
      "step": 152850
    },
    {
      "epoch": 2.1771322796525703,
      "grad_norm": 0.3856874406337738,
      "learning_rate": 5.4938031245548224e-05,
      "loss": 0.6066,
      "step": 152900
    },
    {
      "epoch": 2.177844226114196,
      "grad_norm": 0.5541327595710754,
      "learning_rate": 5.4890545609952995e-05,
      "loss": 0.6802,
      "step": 152950
    },
    {
      "epoch": 2.1785561725758225,
      "grad_norm": 0.464844286441803,
      "learning_rate": 5.484305997435776e-05,
      "loss": 0.5817,
      "step": 153000
    },
    {
      "epoch": 2.1792681190374483,
      "grad_norm": 0.4403549134731293,
      "learning_rate": 5.479557433876252e-05,
      "loss": 0.6229,
      "step": 153050
    },
    {
      "epoch": 2.1799800654990746,
      "grad_norm": 0.6895878314971924,
      "learning_rate": 5.47480887031673e-05,
      "loss": 0.7211,
      "step": 153100
    },
    {
      "epoch": 2.1806920119607005,
      "grad_norm": 0.4010559916496277,
      "learning_rate": 5.4700603067572064e-05,
      "loss": 0.6011,
      "step": 153150
    },
    {
      "epoch": 2.181403958422327,
      "grad_norm": 0.4901905953884125,
      "learning_rate": 5.465311743197683e-05,
      "loss": 0.6397,
      "step": 153200
    },
    {
      "epoch": 2.1821159048839527,
      "grad_norm": 0.7541553974151611,
      "learning_rate": 5.460563179638159e-05,
      "loss": 0.6385,
      "step": 153250
    },
    {
      "epoch": 2.182827851345579,
      "grad_norm": 0.5806929469108582,
      "learning_rate": 5.4558146160786363e-05,
      "loss": 0.6374,
      "step": 153300
    },
    {
      "epoch": 2.183539797807205,
      "grad_norm": 0.6464025974273682,
      "learning_rate": 5.4510660525191134e-05,
      "loss": 0.6118,
      "step": 153350
    },
    {
      "epoch": 2.184251744268831,
      "grad_norm": 0.7631059288978577,
      "learning_rate": 5.4463174889595905e-05,
      "loss": 0.6169,
      "step": 153400
    },
    {
      "epoch": 2.184963690730457,
      "grad_norm": 0.6005672216415405,
      "learning_rate": 5.441568925400067e-05,
      "loss": 0.6795,
      "step": 153450
    },
    {
      "epoch": 2.1856756371920834,
      "grad_norm": 0.5234745740890503,
      "learning_rate": 5.436820361840543e-05,
      "loss": 0.5767,
      "step": 153500
    },
    {
      "epoch": 2.186387583653709,
      "grad_norm": 0.4522565007209778,
      "learning_rate": 5.43207179828102e-05,
      "loss": 0.5932,
      "step": 153550
    },
    {
      "epoch": 2.1870995301153355,
      "grad_norm": 0.5015822649002075,
      "learning_rate": 5.4273232347214975e-05,
      "loss": 0.6326,
      "step": 153600
    },
    {
      "epoch": 2.1878114765769614,
      "grad_norm": 0.4941173493862152,
      "learning_rate": 5.422574671161974e-05,
      "loss": 0.6038,
      "step": 153650
    },
    {
      "epoch": 2.1885234230385873,
      "grad_norm": 0.7059151530265808,
      "learning_rate": 5.41782610760245e-05,
      "loss": 0.6387,
      "step": 153700
    },
    {
      "epoch": 2.1892353695002136,
      "grad_norm": 0.4840930104255676,
      "learning_rate": 5.4130775440429274e-05,
      "loss": 0.621,
      "step": 153750
    },
    {
      "epoch": 2.18994731596184,
      "grad_norm": 0.6344325542449951,
      "learning_rate": 5.408328980483404e-05,
      "loss": 0.5754,
      "step": 153800
    },
    {
      "epoch": 2.1906592624234658,
      "grad_norm": 0.6264549493789673,
      "learning_rate": 5.4035804169238815e-05,
      "loss": 0.6275,
      "step": 153850
    },
    {
      "epoch": 2.1913712088850916,
      "grad_norm": 0.5564232468605042,
      "learning_rate": 5.398831853364358e-05,
      "loss": 0.6491,
      "step": 153900
    },
    {
      "epoch": 2.192083155346718,
      "grad_norm": 0.6880562901496887,
      "learning_rate": 5.3940832898048344e-05,
      "loss": 0.6373,
      "step": 153950
    },
    {
      "epoch": 2.192795101808344,
      "grad_norm": 0.803313136100769,
      "learning_rate": 5.389334726245311e-05,
      "loss": 0.6683,
      "step": 154000
    },
    {
      "epoch": 2.19350704826997,
      "grad_norm": 0.4915863871574402,
      "learning_rate": 5.384586162685787e-05,
      "loss": 0.6119,
      "step": 154050
    },
    {
      "epoch": 2.194218994731596,
      "grad_norm": 0.5883871912956238,
      "learning_rate": 5.379837599126265e-05,
      "loss": 0.6035,
      "step": 154100
    },
    {
      "epoch": 2.1949309411932223,
      "grad_norm": 0.4296310245990753,
      "learning_rate": 5.375089035566741e-05,
      "loss": 0.5974,
      "step": 154150
    },
    {
      "epoch": 2.195642887654848,
      "grad_norm": 0.618454098701477,
      "learning_rate": 5.3703404720072184e-05,
      "loss": 0.6288,
      "step": 154200
    },
    {
      "epoch": 2.1963548341164745,
      "grad_norm": 1.543941617012024,
      "learning_rate": 5.365591908447695e-05,
      "loss": 0.6387,
      "step": 154250
    },
    {
      "epoch": 2.1970667805781003,
      "grad_norm": 0.44445979595184326,
      "learning_rate": 5.360843344888171e-05,
      "loss": 0.625,
      "step": 154300
    },
    {
      "epoch": 2.1977787270397267,
      "grad_norm": 0.5077559351921082,
      "learning_rate": 5.356094781328649e-05,
      "loss": 0.6075,
      "step": 154350
    },
    {
      "epoch": 2.1984906735013525,
      "grad_norm": 0.5418809652328491,
      "learning_rate": 5.3513462177691254e-05,
      "loss": 0.652,
      "step": 154400
    },
    {
      "epoch": 2.199202619962979,
      "grad_norm": 0.5969086289405823,
      "learning_rate": 5.346597654209602e-05,
      "loss": 0.6119,
      "step": 154450
    },
    {
      "epoch": 2.1999145664246047,
      "grad_norm": 1.0342189073562622,
      "learning_rate": 5.341849090650078e-05,
      "loss": 0.6461,
      "step": 154500
    },
    {
      "epoch": 2.200626512886231,
      "grad_norm": 0.36119723320007324,
      "learning_rate": 5.337100527090555e-05,
      "loss": 0.6444,
      "step": 154550
    },
    {
      "epoch": 2.201338459347857,
      "grad_norm": 0.46942129731178284,
      "learning_rate": 5.3323519635310324e-05,
      "loss": 0.6033,
      "step": 154600
    },
    {
      "epoch": 2.202050405809483,
      "grad_norm": 0.47679656744003296,
      "learning_rate": 5.3276033999715095e-05,
      "loss": 0.6556,
      "step": 154650
    },
    {
      "epoch": 2.202762352271109,
      "grad_norm": 0.6122197508811951,
      "learning_rate": 5.322854836411986e-05,
      "loss": 0.6148,
      "step": 154700
    },
    {
      "epoch": 2.2034742987327354,
      "grad_norm": 0.47971928119659424,
      "learning_rate": 5.318106272852462e-05,
      "loss": 0.6072,
      "step": 154750
    },
    {
      "epoch": 2.2041862451943612,
      "grad_norm": 0.5142849087715149,
      "learning_rate": 5.313357709292939e-05,
      "loss": 0.6807,
      "step": 154800
    },
    {
      "epoch": 2.2048981916559875,
      "grad_norm": 0.5516679883003235,
      "learning_rate": 5.3086091457334164e-05,
      "loss": 0.6252,
      "step": 154850
    },
    {
      "epoch": 2.2056101381176134,
      "grad_norm": 0.40718773007392883,
      "learning_rate": 5.303860582173893e-05,
      "loss": 0.5765,
      "step": 154900
    },
    {
      "epoch": 2.2063220845792397,
      "grad_norm": 0.5596166849136353,
      "learning_rate": 5.299112018614369e-05,
      "loss": 0.6775,
      "step": 154950
    },
    {
      "epoch": 2.2070340310408656,
      "grad_norm": 0.4975365102291107,
      "learning_rate": 5.294363455054846e-05,
      "loss": 0.6251,
      "step": 155000
    },
    {
      "epoch": 2.207745977502492,
      "grad_norm": 0.6659561395645142,
      "learning_rate": 5.289614891495323e-05,
      "loss": 0.6488,
      "step": 155050
    },
    {
      "epoch": 2.2084579239641178,
      "grad_norm": 0.4907912611961365,
      "learning_rate": 5.2848663279358005e-05,
      "loss": 0.6235,
      "step": 155100
    },
    {
      "epoch": 2.209169870425744,
      "grad_norm": 0.5463747978210449,
      "learning_rate": 5.280117764376277e-05,
      "loss": 0.5895,
      "step": 155150
    },
    {
      "epoch": 2.20988181688737,
      "grad_norm": 0.5384212136268616,
      "learning_rate": 5.275369200816753e-05,
      "loss": 0.6267,
      "step": 155200
    },
    {
      "epoch": 2.2105937633489963,
      "grad_norm": 0.7800803780555725,
      "learning_rate": 5.27062063725723e-05,
      "loss": 0.6718,
      "step": 155250
    },
    {
      "epoch": 2.211305709810622,
      "grad_norm": 0.5132015347480774,
      "learning_rate": 5.265872073697706e-05,
      "loss": 0.5992,
      "step": 155300
    },
    {
      "epoch": 2.2120176562722484,
      "grad_norm": 0.5659624934196472,
      "learning_rate": 5.261123510138184e-05,
      "loss": 0.6495,
      "step": 155350
    },
    {
      "epoch": 2.2127296027338743,
      "grad_norm": 0.8773508071899414,
      "learning_rate": 5.25637494657866e-05,
      "loss": 0.6135,
      "step": 155400
    },
    {
      "epoch": 2.2134415491955006,
      "grad_norm": 0.4542734622955322,
      "learning_rate": 5.251626383019137e-05,
      "loss": 0.64,
      "step": 155450
    },
    {
      "epoch": 2.2141534956571265,
      "grad_norm": 1.0202800035476685,
      "learning_rate": 5.246877819459614e-05,
      "loss": 0.6398,
      "step": 155500
    },
    {
      "epoch": 2.214865442118753,
      "grad_norm": 0.8046635985374451,
      "learning_rate": 5.24212925590009e-05,
      "loss": 0.6187,
      "step": 155550
    },
    {
      "epoch": 2.2155773885803787,
      "grad_norm": 0.34174567461013794,
      "learning_rate": 5.237380692340568e-05,
      "loss": 0.6448,
      "step": 155600
    },
    {
      "epoch": 2.216289335042005,
      "grad_norm": 0.497984379529953,
      "learning_rate": 5.2326321287810443e-05,
      "loss": 0.6599,
      "step": 155650
    },
    {
      "epoch": 2.217001281503631,
      "grad_norm": 0.5567129254341125,
      "learning_rate": 5.227883565221521e-05,
      "loss": 0.6564,
      "step": 155700
    },
    {
      "epoch": 2.217713227965257,
      "grad_norm": 0.342763215303421,
      "learning_rate": 5.223229972933188e-05,
      "loss": 0.6306,
      "step": 155750
    },
    {
      "epoch": 2.218425174426883,
      "grad_norm": 0.8549761176109314,
      "learning_rate": 5.2184814093736644e-05,
      "loss": 0.6577,
      "step": 155800
    },
    {
      "epoch": 2.2191371208885093,
      "grad_norm": 0.5174515247344971,
      "learning_rate": 5.213732845814141e-05,
      "loss": 0.6602,
      "step": 155850
    },
    {
      "epoch": 2.219849067350135,
      "grad_norm": 0.7809990048408508,
      "learning_rate": 5.2089842822546186e-05,
      "loss": 0.6293,
      "step": 155900
    },
    {
      "epoch": 2.2205610138117615,
      "grad_norm": 0.37659522891044617,
      "learning_rate": 5.204235718695095e-05,
      "loss": 0.6247,
      "step": 155950
    },
    {
      "epoch": 2.2212729602733874,
      "grad_norm": 0.40586057305336,
      "learning_rate": 5.1994871551355714e-05,
      "loss": 0.6033,
      "step": 156000
    },
    {
      "epoch": 2.2219849067350137,
      "grad_norm": 0.8912773132324219,
      "learning_rate": 5.1947385915760485e-05,
      "loss": 0.619,
      "step": 156050
    },
    {
      "epoch": 2.2226968531966396,
      "grad_norm": 0.40857967734336853,
      "learning_rate": 5.189990028016525e-05,
      "loss": 0.6357,
      "step": 156100
    },
    {
      "epoch": 2.223408799658266,
      "grad_norm": 0.5823964476585388,
      "learning_rate": 5.1852414644570026e-05,
      "loss": 0.6397,
      "step": 156150
    },
    {
      "epoch": 2.2241207461198917,
      "grad_norm": 0.8868613243103027,
      "learning_rate": 5.180492900897479e-05,
      "loss": 0.5976,
      "step": 156200
    },
    {
      "epoch": 2.224832692581518,
      "grad_norm": 0.40852442383766174,
      "learning_rate": 5.1757443373379554e-05,
      "loss": 0.5883,
      "step": 156250
    },
    {
      "epoch": 2.225544639043144,
      "grad_norm": 0.7560901641845703,
      "learning_rate": 5.170995773778432e-05,
      "loss": 0.6445,
      "step": 156300
    },
    {
      "epoch": 2.2262565855047702,
      "grad_norm": 0.6916713714599609,
      "learning_rate": 5.166247210218908e-05,
      "loss": 0.622,
      "step": 156350
    },
    {
      "epoch": 2.226968531966396,
      "grad_norm": 0.39313480257987976,
      "learning_rate": 5.1615936179305755e-05,
      "loss": 0.5941,
      "step": 156400
    },
    {
      "epoch": 2.2276804784280224,
      "grad_norm": 0.8393328785896301,
      "learning_rate": 5.156845054371053e-05,
      "loss": 0.6561,
      "step": 156450
    },
    {
      "epoch": 2.2283924248896483,
      "grad_norm": 0.45139774680137634,
      "learning_rate": 5.15209649081153e-05,
      "loss": 0.6345,
      "step": 156500
    },
    {
      "epoch": 2.2291043713512746,
      "grad_norm": 0.4540969729423523,
      "learning_rate": 5.147347927252007e-05,
      "loss": 0.6579,
      "step": 156550
    },
    {
      "epoch": 2.2298163178129005,
      "grad_norm": 0.4725242853164673,
      "learning_rate": 5.142599363692483e-05,
      "loss": 0.6298,
      "step": 156600
    },
    {
      "epoch": 2.2305282642745263,
      "grad_norm": 0.7201564908027649,
      "learning_rate": 5.1378508001329596e-05,
      "loss": 0.5952,
      "step": 156650
    },
    {
      "epoch": 2.2312402107361526,
      "grad_norm": 0.5102342963218689,
      "learning_rate": 5.133102236573437e-05,
      "loss": 0.6157,
      "step": 156700
    },
    {
      "epoch": 2.231952157197779,
      "grad_norm": 0.6748486161231995,
      "learning_rate": 5.128353673013914e-05,
      "loss": 0.6511,
      "step": 156750
    },
    {
      "epoch": 2.232664103659405,
      "grad_norm": 0.8214449882507324,
      "learning_rate": 5.12360510945439e-05,
      "loss": 0.6654,
      "step": 156800
    },
    {
      "epoch": 2.2333760501210307,
      "grad_norm": 0.39221540093421936,
      "learning_rate": 5.1188565458948666e-05,
      "loss": 0.6235,
      "step": 156850
    },
    {
      "epoch": 2.234087996582657,
      "grad_norm": 0.48305755853652954,
      "learning_rate": 5.114107982335343e-05,
      "loss": 0.5899,
      "step": 156900
    },
    {
      "epoch": 2.234799943044283,
      "grad_norm": 0.29536259174346924,
      "learning_rate": 5.109359418775821e-05,
      "loss": 0.6228,
      "step": 156950
    },
    {
      "epoch": 2.235511889505909,
      "grad_norm": 0.30684205889701843,
      "learning_rate": 5.104610855216297e-05,
      "loss": 0.6379,
      "step": 157000
    },
    {
      "epoch": 2.236223835967535,
      "grad_norm": 0.44789549708366394,
      "learning_rate": 5.099862291656774e-05,
      "loss": 0.6265,
      "step": 157050
    },
    {
      "epoch": 2.2369357824291614,
      "grad_norm": 0.5417699813842773,
      "learning_rate": 5.0951137280972506e-05,
      "loss": 0.6316,
      "step": 157100
    },
    {
      "epoch": 2.2376477288907872,
      "grad_norm": 0.4683241546154022,
      "learning_rate": 5.090365164537727e-05,
      "loss": 0.6349,
      "step": 157150
    },
    {
      "epoch": 2.2383596753524135,
      "grad_norm": 0.4831677973270416,
      "learning_rate": 5.085616600978205e-05,
      "loss": 0.6013,
      "step": 157200
    },
    {
      "epoch": 2.2390716218140394,
      "grad_norm": 0.5098283886909485,
      "learning_rate": 5.080868037418681e-05,
      "loss": 0.6114,
      "step": 157250
    },
    {
      "epoch": 2.2397835682756657,
      "grad_norm": 0.6012431979179382,
      "learning_rate": 5.0761194738591576e-05,
      "loss": 0.6632,
      "step": 157300
    },
    {
      "epoch": 2.2404955147372916,
      "grad_norm": 0.5180309414863586,
      "learning_rate": 5.071370910299634e-05,
      "loss": 0.6904,
      "step": 157350
    },
    {
      "epoch": 2.241207461198918,
      "grad_norm": 0.7469173073768616,
      "learning_rate": 5.066622346740111e-05,
      "loss": 0.6134,
      "step": 157400
    },
    {
      "epoch": 2.2419194076605438,
      "grad_norm": 0.3847638964653015,
      "learning_rate": 5.061873783180588e-05,
      "loss": 0.5998,
      "step": 157450
    },
    {
      "epoch": 2.24263135412217,
      "grad_norm": 0.5250817537307739,
      "learning_rate": 5.057125219621065e-05,
      "loss": 0.6699,
      "step": 157500
    },
    {
      "epoch": 2.243343300583796,
      "grad_norm": 0.7828034162521362,
      "learning_rate": 5.0523766560615417e-05,
      "loss": 0.5986,
      "step": 157550
    },
    {
      "epoch": 2.2440552470454223,
      "grad_norm": 0.5466170907020569,
      "learning_rate": 5.047628092502018e-05,
      "loss": 0.5422,
      "step": 157600
    },
    {
      "epoch": 2.244767193507048,
      "grad_norm": 0.38294607400894165,
      "learning_rate": 5.0428795289424945e-05,
      "loss": 0.5844,
      "step": 157650
    },
    {
      "epoch": 2.2454791399686744,
      "grad_norm": 0.35347363352775574,
      "learning_rate": 5.038130965382972e-05,
      "loss": 0.6124,
      "step": 157700
    },
    {
      "epoch": 2.2461910864303003,
      "grad_norm": 0.5375327467918396,
      "learning_rate": 5.0333824018234486e-05,
      "loss": 0.6385,
      "step": 157750
    },
    {
      "epoch": 2.2469030328919266,
      "grad_norm": 0.3558448553085327,
      "learning_rate": 5.028633838263925e-05,
      "loss": 0.6286,
      "step": 157800
    },
    {
      "epoch": 2.2476149793535525,
      "grad_norm": 0.6769005060195923,
      "learning_rate": 5.023885274704402e-05,
      "loss": 0.6762,
      "step": 157850
    },
    {
      "epoch": 2.248326925815179,
      "grad_norm": 0.5078948140144348,
      "learning_rate": 5.0191367111448785e-05,
      "loss": 0.6561,
      "step": 157900
    },
    {
      "epoch": 2.2490388722768047,
      "grad_norm": 0.6965242028236389,
      "learning_rate": 5.014388147585356e-05,
      "loss": 0.6329,
      "step": 157950
    },
    {
      "epoch": 2.249750818738431,
      "grad_norm": 0.539640486240387,
      "learning_rate": 5.009639584025833e-05,
      "loss": 0.6204,
      "step": 158000
    },
    {
      "epoch": 2.250462765200057,
      "grad_norm": 0.5697188377380371,
      "learning_rate": 5.004891020466309e-05,
      "loss": 0.6066,
      "step": 158050
    },
    {
      "epoch": 2.251174711661683,
      "grad_norm": 0.7372308969497681,
      "learning_rate": 5.0001424569067855e-05,
      "loss": 0.6019,
      "step": 158100
    },
    {
      "epoch": 2.251886658123309,
      "grad_norm": 0.48032498359680176,
      "learning_rate": 4.9953938933472626e-05,
      "loss": 0.5685,
      "step": 158150
    },
    {
      "epoch": 2.2525986045849353,
      "grad_norm": 0.6161688566207886,
      "learning_rate": 4.990645329787739e-05,
      "loss": 0.6017,
      "step": 158200
    },
    {
      "epoch": 2.253310551046561,
      "grad_norm": 0.35844317078590393,
      "learning_rate": 4.985896766228216e-05,
      "loss": 0.6056,
      "step": 158250
    },
    {
      "epoch": 2.2540224975081875,
      "grad_norm": 0.6281041502952576,
      "learning_rate": 4.981148202668693e-05,
      "loss": 0.6271,
      "step": 158300
    },
    {
      "epoch": 2.2547344439698134,
      "grad_norm": 0.7712239623069763,
      "learning_rate": 4.97639963910917e-05,
      "loss": 0.6247,
      "step": 158350
    },
    {
      "epoch": 2.2554463904314397,
      "grad_norm": 0.7496048212051392,
      "learning_rate": 4.9716510755496466e-05,
      "loss": 0.6542,
      "step": 158400
    },
    {
      "epoch": 2.2561583368930656,
      "grad_norm": 0.5296820998191833,
      "learning_rate": 4.966902511990123e-05,
      "loss": 0.6428,
      "step": 158450
    },
    {
      "epoch": 2.256870283354692,
      "grad_norm": 0.4743293225765228,
      "learning_rate": 4.9621539484306e-05,
      "loss": 0.6036,
      "step": 158500
    },
    {
      "epoch": 2.2575822298163177,
      "grad_norm": 0.4972899258136749,
      "learning_rate": 4.9574053848710765e-05,
      "loss": 0.597,
      "step": 158550
    },
    {
      "epoch": 2.258294176277944,
      "grad_norm": 0.5032157897949219,
      "learning_rate": 4.9526568213115536e-05,
      "loss": 0.5619,
      "step": 158600
    },
    {
      "epoch": 2.25900612273957,
      "grad_norm": 0.5583911538124084,
      "learning_rate": 4.94790825775203e-05,
      "loss": 0.5945,
      "step": 158650
    },
    {
      "epoch": 2.2597180692011962,
      "grad_norm": 0.5284196138381958,
      "learning_rate": 4.943159694192507e-05,
      "loss": 0.6537,
      "step": 158700
    },
    {
      "epoch": 2.260430015662822,
      "grad_norm": 0.47016045451164246,
      "learning_rate": 4.938411130632984e-05,
      "loss": 0.6027,
      "step": 158750
    },
    {
      "epoch": 2.2611419621244484,
      "grad_norm": 0.5841062068939209,
      "learning_rate": 4.9336625670734606e-05,
      "loss": 0.6704,
      "step": 158800
    },
    {
      "epoch": 2.2618539085860743,
      "grad_norm": 0.6989777088165283,
      "learning_rate": 4.928914003513938e-05,
      "loss": 0.6237,
      "step": 158850
    },
    {
      "epoch": 2.2625658550477006,
      "grad_norm": 0.87724369764328,
      "learning_rate": 4.924165439954414e-05,
      "loss": 0.6237,
      "step": 158900
    },
    {
      "epoch": 2.2632778015093264,
      "grad_norm": 0.5365281701087952,
      "learning_rate": 4.9194168763948905e-05,
      "loss": 0.6371,
      "step": 158950
    },
    {
      "epoch": 2.2639897479709528,
      "grad_norm": 0.7455468773841858,
      "learning_rate": 4.9146683128353676e-05,
      "loss": 0.6848,
      "step": 159000
    },
    {
      "epoch": 2.2647016944325786,
      "grad_norm": 0.6580207943916321,
      "learning_rate": 4.909919749275844e-05,
      "loss": 0.6681,
      "step": 159050
    },
    {
      "epoch": 2.265413640894205,
      "grad_norm": 0.5938348770141602,
      "learning_rate": 4.905171185716321e-05,
      "loss": 0.6242,
      "step": 159100
    },
    {
      "epoch": 2.266125587355831,
      "grad_norm": 0.9163922071456909,
      "learning_rate": 4.900422622156798e-05,
      "loss": 0.6531,
      "step": 159150
    },
    {
      "epoch": 2.266837533817457,
      "grad_norm": 0.6608870625495911,
      "learning_rate": 4.8956740585972746e-05,
      "loss": 0.6339,
      "step": 159200
    },
    {
      "epoch": 2.267549480279083,
      "grad_norm": 0.8683748245239258,
      "learning_rate": 4.8909254950377516e-05,
      "loss": 0.623,
      "step": 159250
    },
    {
      "epoch": 2.2682614267407093,
      "grad_norm": 0.45245012640953064,
      "learning_rate": 4.886176931478228e-05,
      "loss": 0.6969,
      "step": 159300
    },
    {
      "epoch": 2.268973373202335,
      "grad_norm": 0.6916724443435669,
      "learning_rate": 4.881428367918705e-05,
      "loss": 0.6822,
      "step": 159350
    },
    {
      "epoch": 2.269685319663961,
      "grad_norm": 0.44173547625541687,
      "learning_rate": 4.8766798043591815e-05,
      "loss": 0.6628,
      "step": 159400
    },
    {
      "epoch": 2.2703972661255873,
      "grad_norm": 0.6023834347724915,
      "learning_rate": 4.871931240799658e-05,
      "loss": 0.6182,
      "step": 159450
    },
    {
      "epoch": 2.2711092125872137,
      "grad_norm": 0.5240619778633118,
      "learning_rate": 4.867182677240135e-05,
      "loss": 0.6398,
      "step": 159500
    },
    {
      "epoch": 2.2718211590488395,
      "grad_norm": 0.5933771729469299,
      "learning_rate": 4.8624341136806114e-05,
      "loss": 0.644,
      "step": 159550
    },
    {
      "epoch": 2.2725331055104654,
      "grad_norm": 0.8693912625312805,
      "learning_rate": 4.8576855501210885e-05,
      "loss": 0.6401,
      "step": 159600
    },
    {
      "epoch": 2.2732450519720917,
      "grad_norm": 0.5248859524726868,
      "learning_rate": 4.8529369865615656e-05,
      "loss": 0.6143,
      "step": 159650
    },
    {
      "epoch": 2.273956998433718,
      "grad_norm": 0.3827531337738037,
      "learning_rate": 4.848188423002042e-05,
      "loss": 0.6479,
      "step": 159700
    },
    {
      "epoch": 2.274668944895344,
      "grad_norm": 0.6869708895683289,
      "learning_rate": 4.843439859442519e-05,
      "loss": 0.6709,
      "step": 159750
    },
    {
      "epoch": 2.2753808913569697,
      "grad_norm": 0.9395023584365845,
      "learning_rate": 4.8386912958829955e-05,
      "loss": 0.6218,
      "step": 159800
    },
    {
      "epoch": 2.276092837818596,
      "grad_norm": 0.5628349781036377,
      "learning_rate": 4.8339427323234726e-05,
      "loss": 0.66,
      "step": 159850
    },
    {
      "epoch": 2.2768047842802224,
      "grad_norm": 0.8149871230125427,
      "learning_rate": 4.829194168763949e-05,
      "loss": 0.6193,
      "step": 159900
    },
    {
      "epoch": 2.2775167307418482,
      "grad_norm": 0.7016221284866333,
      "learning_rate": 4.8244456052044254e-05,
      "loss": 0.6765,
      "step": 159950
    },
    {
      "epoch": 2.278228677203474,
      "grad_norm": 0.9439777731895447,
      "learning_rate": 4.8196970416449025e-05,
      "loss": 0.6375,
      "step": 160000
    },
    {
      "epoch": 2.2789406236651004,
      "grad_norm": 0.48294365406036377,
      "learning_rate": 4.8149484780853795e-05,
      "loss": 0.6454,
      "step": 160050
    },
    {
      "epoch": 2.2796525701267263,
      "grad_norm": 0.5756824016571045,
      "learning_rate": 4.8101999145258566e-05,
      "loss": 0.6758,
      "step": 160100
    },
    {
      "epoch": 2.2803645165883526,
      "grad_norm": 0.49081236124038696,
      "learning_rate": 4.805451350966333e-05,
      "loss": 0.6408,
      "step": 160150
    },
    {
      "epoch": 2.2810764630499785,
      "grad_norm": 0.4427952468395233,
      "learning_rate": 4.8007027874068094e-05,
      "loss": 0.6214,
      "step": 160200
    },
    {
      "epoch": 2.2817884095116048,
      "grad_norm": 0.7895782589912415,
      "learning_rate": 4.7959542238472865e-05,
      "loss": 0.6777,
      "step": 160250
    },
    {
      "epoch": 2.2825003559732306,
      "grad_norm": 0.7966433167457581,
      "learning_rate": 4.791205660287763e-05,
      "loss": 0.6755,
      "step": 160300
    },
    {
      "epoch": 2.283212302434857,
      "grad_norm": 0.5076044201850891,
      "learning_rate": 4.78645709672824e-05,
      "loss": 0.6714,
      "step": 160350
    },
    {
      "epoch": 2.283924248896483,
      "grad_norm": 0.740380585193634,
      "learning_rate": 4.7817085331687164e-05,
      "loss": 0.6103,
      "step": 160400
    },
    {
      "epoch": 2.284636195358109,
      "grad_norm": 0.5598039627075195,
      "learning_rate": 4.7769599696091935e-05,
      "loss": 0.6432,
      "step": 160450
    },
    {
      "epoch": 2.285348141819735,
      "grad_norm": 0.35814955830574036,
      "learning_rate": 4.7722114060496706e-05,
      "loss": 0.6166,
      "step": 160500
    },
    {
      "epoch": 2.2860600882813613,
      "grad_norm": 0.7567604780197144,
      "learning_rate": 4.767462842490147e-05,
      "loss": 0.6141,
      "step": 160550
    },
    {
      "epoch": 2.286772034742987,
      "grad_norm": 0.48079293966293335,
      "learning_rate": 4.762714278930624e-05,
      "loss": 0.6112,
      "step": 160600
    },
    {
      "epoch": 2.2874839812046135,
      "grad_norm": 0.6009765863418579,
      "learning_rate": 4.7579657153711005e-05,
      "loss": 0.6042,
      "step": 160650
    },
    {
      "epoch": 2.2881959276662394,
      "grad_norm": 0.5867553949356079,
      "learning_rate": 4.753217151811577e-05,
      "loss": 0.6582,
      "step": 160700
    },
    {
      "epoch": 2.2889078741278657,
      "grad_norm": 0.46501424908638,
      "learning_rate": 4.748468588252054e-05,
      "loss": 0.6337,
      "step": 160750
    },
    {
      "epoch": 2.2896198205894915,
      "grad_norm": 0.5562171339988708,
      "learning_rate": 4.7437200246925304e-05,
      "loss": 0.5753,
      "step": 160800
    },
    {
      "epoch": 2.290331767051118,
      "grad_norm": 0.7949395775794983,
      "learning_rate": 4.7389714611330075e-05,
      "loss": 0.6528,
      "step": 160850
    },
    {
      "epoch": 2.2910437135127437,
      "grad_norm": 0.5484274625778198,
      "learning_rate": 4.7342228975734845e-05,
      "loss": 0.659,
      "step": 160900
    },
    {
      "epoch": 2.29175565997437,
      "grad_norm": 0.6621754169464111,
      "learning_rate": 4.729569305285151e-05,
      "loss": 0.6175,
      "step": 160950
    },
    {
      "epoch": 2.292467606435996,
      "grad_norm": 0.49315911531448364,
      "learning_rate": 4.724820741725628e-05,
      "loss": 0.6023,
      "step": 161000
    },
    {
      "epoch": 2.293179552897622,
      "grad_norm": 0.5698114037513733,
      "learning_rate": 4.720072178166105e-05,
      "loss": 0.6637,
      "step": 161050
    },
    {
      "epoch": 2.293891499359248,
      "grad_norm": 0.2866593897342682,
      "learning_rate": 4.715323614606582e-05,
      "loss": 0.6127,
      "step": 161100
    },
    {
      "epoch": 2.2946034458208744,
      "grad_norm": 0.48311424255371094,
      "learning_rate": 4.710575051047059e-05,
      "loss": 0.6764,
      "step": 161150
    },
    {
      "epoch": 2.2953153922825003,
      "grad_norm": 0.6989254951477051,
      "learning_rate": 4.705826487487535e-05,
      "loss": 0.6089,
      "step": 161200
    },
    {
      "epoch": 2.2960273387441266,
      "grad_norm": 0.4718880355358124,
      "learning_rate": 4.701077923928012e-05,
      "loss": 0.6761,
      "step": 161250
    },
    {
      "epoch": 2.2967392852057524,
      "grad_norm": 0.5913665890693665,
      "learning_rate": 4.696329360368489e-05,
      "loss": 0.6601,
      "step": 161300
    },
    {
      "epoch": 2.2974512316673787,
      "grad_norm": 0.5901994109153748,
      "learning_rate": 4.691580796808965e-05,
      "loss": 0.6335,
      "step": 161350
    },
    {
      "epoch": 2.2981631781290046,
      "grad_norm": 0.606740415096283,
      "learning_rate": 4.686832233249442e-05,
      "loss": 0.6185,
      "step": 161400
    },
    {
      "epoch": 2.298875124590631,
      "grad_norm": 0.4874947667121887,
      "learning_rate": 4.682083669689919e-05,
      "loss": 0.5688,
      "step": 161450
    },
    {
      "epoch": 2.299587071052257,
      "grad_norm": 0.5244929194450378,
      "learning_rate": 4.677335106130396e-05,
      "loss": 0.5848,
      "step": 161500
    },
    {
      "epoch": 2.300299017513883,
      "grad_norm": 0.6963481307029724,
      "learning_rate": 4.672586542570873e-05,
      "loss": 0.5876,
      "step": 161550
    },
    {
      "epoch": 2.301010963975509,
      "grad_norm": 0.44163209199905396,
      "learning_rate": 4.667837979011349e-05,
      "loss": 0.5918,
      "step": 161600
    },
    {
      "epoch": 2.3017229104371353,
      "grad_norm": 0.40430769324302673,
      "learning_rate": 4.663089415451826e-05,
      "loss": 0.6823,
      "step": 161650
    },
    {
      "epoch": 2.302434856898761,
      "grad_norm": 0.4459913969039917,
      "learning_rate": 4.6583408518923026e-05,
      "loss": 0.62,
      "step": 161700
    },
    {
      "epoch": 2.3031468033603875,
      "grad_norm": 0.7415963411331177,
      "learning_rate": 4.65359228833278e-05,
      "loss": 0.5922,
      "step": 161750
    },
    {
      "epoch": 2.3038587498220133,
      "grad_norm": 0.43345195055007935,
      "learning_rate": 4.648843724773256e-05,
      "loss": 0.6193,
      "step": 161800
    },
    {
      "epoch": 2.3045706962836396,
      "grad_norm": 0.7643994092941284,
      "learning_rate": 4.644095161213733e-05,
      "loss": 0.6511,
      "step": 161850
    },
    {
      "epoch": 2.3052826427452655,
      "grad_norm": 0.5951457619667053,
      "learning_rate": 4.63934659765421e-05,
      "loss": 0.6246,
      "step": 161900
    },
    {
      "epoch": 2.305994589206892,
      "grad_norm": 0.4685530364513397,
      "learning_rate": 4.634598034094687e-05,
      "loss": 0.6377,
      "step": 161950
    },
    {
      "epoch": 2.3067065356685177,
      "grad_norm": 0.6647816300392151,
      "learning_rate": 4.629849470535164e-05,
      "loss": 0.6187,
      "step": 162000
    },
    {
      "epoch": 2.307418482130144,
      "grad_norm": 0.5457863807678223,
      "learning_rate": 4.62510090697564e-05,
      "loss": 0.6462,
      "step": 162050
    },
    {
      "epoch": 2.30813042859177,
      "grad_norm": 0.445776104927063,
      "learning_rate": 4.6203523434161166e-05,
      "loss": 0.5783,
      "step": 162100
    },
    {
      "epoch": 2.308842375053396,
      "grad_norm": 0.7605949640274048,
      "learning_rate": 4.6156037798565937e-05,
      "loss": 0.6595,
      "step": 162150
    },
    {
      "epoch": 2.309554321515022,
      "grad_norm": 0.7860704660415649,
      "learning_rate": 4.61085521629707e-05,
      "loss": 0.62,
      "step": 162200
    },
    {
      "epoch": 2.3102662679766484,
      "grad_norm": 0.5173550844192505,
      "learning_rate": 4.606106652737547e-05,
      "loss": 0.6735,
      "step": 162250
    },
    {
      "epoch": 2.3109782144382742,
      "grad_norm": 1.0104228258132935,
      "learning_rate": 4.601358089178024e-05,
      "loss": 0.6317,
      "step": 162300
    },
    {
      "epoch": 2.3116901608999,
      "grad_norm": 0.7649103999137878,
      "learning_rate": 4.5966095256185006e-05,
      "loss": 0.6501,
      "step": 162350
    },
    {
      "epoch": 2.3124021073615264,
      "grad_norm": 0.767203152179718,
      "learning_rate": 4.591860962058978e-05,
      "loss": 0.6076,
      "step": 162400
    },
    {
      "epoch": 2.3131140538231527,
      "grad_norm": 0.6016256213188171,
      "learning_rate": 4.587112398499454e-05,
      "loss": 0.6265,
      "step": 162450
    },
    {
      "epoch": 2.3138260002847786,
      "grad_norm": 0.40303128957748413,
      "learning_rate": 4.582363834939931e-05,
      "loss": 0.6243,
      "step": 162500
    },
    {
      "epoch": 2.3145379467464045,
      "grad_norm": 0.563260018825531,
      "learning_rate": 4.5776152713804076e-05,
      "loss": 0.6653,
      "step": 162550
    },
    {
      "epoch": 2.3152498932080308,
      "grad_norm": 0.797411322593689,
      "learning_rate": 4.572866707820884e-05,
      "loss": 0.6409,
      "step": 162600
    },
    {
      "epoch": 2.315961839669657,
      "grad_norm": 0.5795272588729858,
      "learning_rate": 4.568118144261361e-05,
      "loss": 0.5427,
      "step": 162650
    },
    {
      "epoch": 2.316673786131283,
      "grad_norm": 0.6271867752075195,
      "learning_rate": 4.5633695807018375e-05,
      "loss": 0.6161,
      "step": 162700
    },
    {
      "epoch": 2.317385732592909,
      "grad_norm": 0.5697126388549805,
      "learning_rate": 4.558621017142315e-05,
      "loss": 0.637,
      "step": 162750
    },
    {
      "epoch": 2.318097679054535,
      "grad_norm": 0.5416192412376404,
      "learning_rate": 4.553872453582792e-05,
      "loss": 0.6295,
      "step": 162800
    },
    {
      "epoch": 2.3188096255161614,
      "grad_norm": 0.7985019087791443,
      "learning_rate": 4.549123890023268e-05,
      "loss": 0.6291,
      "step": 162850
    },
    {
      "epoch": 2.3195215719777873,
      "grad_norm": 0.5397459268569946,
      "learning_rate": 4.544375326463745e-05,
      "loss": 0.6331,
      "step": 162900
    },
    {
      "epoch": 2.320233518439413,
      "grad_norm": 0.4763925075531006,
      "learning_rate": 4.5396267629042216e-05,
      "loss": 0.6547,
      "step": 162950
    },
    {
      "epoch": 2.3209454649010395,
      "grad_norm": 0.45229265093803406,
      "learning_rate": 4.5348781993446987e-05,
      "loss": 0.5969,
      "step": 163000
    },
    {
      "epoch": 2.3216574113626653,
      "grad_norm": 0.5357984900474548,
      "learning_rate": 4.530129635785175e-05,
      "loss": 0.5846,
      "step": 163050
    },
    {
      "epoch": 2.3223693578242917,
      "grad_norm": 0.7204368710517883,
      "learning_rate": 4.5253810722256515e-05,
      "loss": 0.6158,
      "step": 163100
    },
    {
      "epoch": 2.3230813042859175,
      "grad_norm": 0.6582292318344116,
      "learning_rate": 4.5206325086661285e-05,
      "loss": 0.622,
      "step": 163150
    },
    {
      "epoch": 2.323793250747544,
      "grad_norm": 0.45066022872924805,
      "learning_rate": 4.5158839451066056e-05,
      "loss": 0.6382,
      "step": 163200
    },
    {
      "epoch": 2.3245051972091697,
      "grad_norm": 0.4671263098716736,
      "learning_rate": 4.511135381547083e-05,
      "loss": 0.6004,
      "step": 163250
    },
    {
      "epoch": 2.325217143670796,
      "grad_norm": 0.6836893558502197,
      "learning_rate": 4.506386817987559e-05,
      "loss": 0.6184,
      "step": 163300
    },
    {
      "epoch": 2.325929090132422,
      "grad_norm": 0.6740344166755676,
      "learning_rate": 4.5017332256992264e-05,
      "loss": 0.639,
      "step": 163350
    },
    {
      "epoch": 2.326641036594048,
      "grad_norm": 0.5667642951011658,
      "learning_rate": 4.496984662139703e-05,
      "loss": 0.6401,
      "step": 163400
    },
    {
      "epoch": 2.327352983055674,
      "grad_norm": 0.6400852799415588,
      "learning_rate": 4.49223609858018e-05,
      "loss": 0.6022,
      "step": 163450
    },
    {
      "epoch": 2.3280649295173004,
      "grad_norm": 0.5510307550430298,
      "learning_rate": 4.487487535020656e-05,
      "loss": 0.6407,
      "step": 163500
    },
    {
      "epoch": 2.3287768759789262,
      "grad_norm": 0.4558106064796448,
      "learning_rate": 4.4827389714611334e-05,
      "loss": 0.5945,
      "step": 163550
    },
    {
      "epoch": 2.3294888224405526,
      "grad_norm": 0.5919645428657532,
      "learning_rate": 4.47799040790161e-05,
      "loss": 0.5724,
      "step": 163600
    },
    {
      "epoch": 2.3302007689021784,
      "grad_norm": 0.7518628239631653,
      "learning_rate": 4.473241844342086e-05,
      "loss": 0.6301,
      "step": 163650
    },
    {
      "epoch": 2.3309127153638047,
      "grad_norm": 0.5675749778747559,
      "learning_rate": 4.468493280782563e-05,
      "loss": 0.6066,
      "step": 163700
    },
    {
      "epoch": 2.3316246618254306,
      "grad_norm": 0.526981770992279,
      "learning_rate": 4.46374471722304e-05,
      "loss": 0.6237,
      "step": 163750
    },
    {
      "epoch": 2.332336608287057,
      "grad_norm": 0.8517922759056091,
      "learning_rate": 4.4589961536635174e-05,
      "loss": 0.6261,
      "step": 163800
    },
    {
      "epoch": 2.333048554748683,
      "grad_norm": 0.589832603931427,
      "learning_rate": 4.454247590103994e-05,
      "loss": 0.6916,
      "step": 163850
    },
    {
      "epoch": 2.333760501210309,
      "grad_norm": 0.77918940782547,
      "learning_rate": 4.44949902654447e-05,
      "loss": 0.645,
      "step": 163900
    },
    {
      "epoch": 2.334472447671935,
      "grad_norm": 0.6913883686065674,
      "learning_rate": 4.444750462984947e-05,
      "loss": 0.6142,
      "step": 163950
    },
    {
      "epoch": 2.3351843941335613,
      "grad_norm": 0.6419036984443665,
      "learning_rate": 4.440001899425424e-05,
      "loss": 0.5885,
      "step": 164000
    },
    {
      "epoch": 2.335896340595187,
      "grad_norm": 0.6848738789558411,
      "learning_rate": 4.435253335865901e-05,
      "loss": 0.6519,
      "step": 164050
    },
    {
      "epoch": 2.3366082870568134,
      "grad_norm": 0.627777099609375,
      "learning_rate": 4.430504772306377e-05,
      "loss": 0.6047,
      "step": 164100
    },
    {
      "epoch": 2.3373202335184393,
      "grad_norm": 0.6142570972442627,
      "learning_rate": 4.425756208746854e-05,
      "loss": 0.6232,
      "step": 164150
    },
    {
      "epoch": 2.3380321799800656,
      "grad_norm": 0.6622012853622437,
      "learning_rate": 4.4210076451873314e-05,
      "loss": 0.5812,
      "step": 164200
    },
    {
      "epoch": 2.3387441264416915,
      "grad_norm": 0.661210834980011,
      "learning_rate": 4.416259081627808e-05,
      "loss": 0.6185,
      "step": 164250
    },
    {
      "epoch": 2.339456072903318,
      "grad_norm": 0.6195626854896545,
      "learning_rate": 4.411510518068285e-05,
      "loss": 0.6179,
      "step": 164300
    },
    {
      "epoch": 2.3401680193649437,
      "grad_norm": 0.7829001545906067,
      "learning_rate": 4.406761954508761e-05,
      "loss": 0.6867,
      "step": 164350
    },
    {
      "epoch": 2.34087996582657,
      "grad_norm": 0.5483605265617371,
      "learning_rate": 4.402013390949238e-05,
      "loss": 0.6029,
      "step": 164400
    },
    {
      "epoch": 2.341591912288196,
      "grad_norm": 0.619853138923645,
      "learning_rate": 4.397264827389715e-05,
      "loss": 0.6223,
      "step": 164450
    },
    {
      "epoch": 2.342303858749822,
      "grad_norm": 0.593503475189209,
      "learning_rate": 4.392516263830191e-05,
      "loss": 0.5927,
      "step": 164500
    },
    {
      "epoch": 2.343015805211448,
      "grad_norm": 0.3388447165489197,
      "learning_rate": 4.387767700270668e-05,
      "loss": 0.6202,
      "step": 164550
    },
    {
      "epoch": 2.3437277516730743,
      "grad_norm": 0.5384566187858582,
      "learning_rate": 4.383019136711145e-05,
      "loss": 0.6482,
      "step": 164600
    },
    {
      "epoch": 2.3444396981347,
      "grad_norm": 0.523072361946106,
      "learning_rate": 4.378270573151622e-05,
      "loss": 0.6944,
      "step": 164650
    },
    {
      "epoch": 2.3451516445963265,
      "grad_norm": 0.3939298391342163,
      "learning_rate": 4.373522009592099e-05,
      "loss": 0.6088,
      "step": 164700
    },
    {
      "epoch": 2.3458635910579524,
      "grad_norm": 0.4791984558105469,
      "learning_rate": 4.368773446032575e-05,
      "loss": 0.656,
      "step": 164750
    },
    {
      "epoch": 2.3465755375195787,
      "grad_norm": 0.6676874756813049,
      "learning_rate": 4.364024882473052e-05,
      "loss": 0.6229,
      "step": 164800
    },
    {
      "epoch": 2.3472874839812046,
      "grad_norm": 0.5701244473457336,
      "learning_rate": 4.359276318913529e-05,
      "loss": 0.5818,
      "step": 164850
    },
    {
      "epoch": 2.347999430442831,
      "grad_norm": 0.8404282927513123,
      "learning_rate": 4.354527755354005e-05,
      "loss": 0.6532,
      "step": 164900
    },
    {
      "epoch": 2.3487113769044567,
      "grad_norm": 0.544599175453186,
      "learning_rate": 4.349779191794482e-05,
      "loss": 0.6466,
      "step": 164950
    },
    {
      "epoch": 2.349423323366083,
      "grad_norm": 0.8170461654663086,
      "learning_rate": 4.345030628234959e-05,
      "loss": 0.6558,
      "step": 165000
    },
    {
      "epoch": 2.350135269827709,
      "grad_norm": 0.606722891330719,
      "learning_rate": 4.3402820646754364e-05,
      "loss": 0.6675,
      "step": 165050
    },
    {
      "epoch": 2.350847216289335,
      "grad_norm": 0.4875310957431793,
      "learning_rate": 4.335533501115913e-05,
      "loss": 0.6142,
      "step": 165100
    },
    {
      "epoch": 2.351559162750961,
      "grad_norm": 0.522186279296875,
      "learning_rate": 4.330784937556389e-05,
      "loss": 0.6856,
      "step": 165150
    },
    {
      "epoch": 2.3522711092125874,
      "grad_norm": 0.6430922150611877,
      "learning_rate": 4.326036373996866e-05,
      "loss": 0.5932,
      "step": 165200
    },
    {
      "epoch": 2.3529830556742133,
      "grad_norm": 0.5089699625968933,
      "learning_rate": 4.3212878104373427e-05,
      "loss": 0.6085,
      "step": 165250
    },
    {
      "epoch": 2.353695002135839,
      "grad_norm": 0.48739248514175415,
      "learning_rate": 4.31653924687782e-05,
      "loss": 0.6399,
      "step": 165300
    },
    {
      "epoch": 2.3544069485974655,
      "grad_norm": 0.8960893154144287,
      "learning_rate": 4.311790683318296e-05,
      "loss": 0.6519,
      "step": 165350
    },
    {
      "epoch": 2.3551188950590918,
      "grad_norm": 0.783653199672699,
      "learning_rate": 4.307042119758773e-05,
      "loss": 0.5959,
      "step": 165400
    },
    {
      "epoch": 2.3558308415207176,
      "grad_norm": 0.6383863091468811,
      "learning_rate": 4.30229355619925e-05,
      "loss": 0.6457,
      "step": 165450
    },
    {
      "epoch": 2.3565427879823435,
      "grad_norm": 0.3825996220111847,
      "learning_rate": 4.297544992639727e-05,
      "loss": 0.5945,
      "step": 165500
    },
    {
      "epoch": 2.35725473444397,
      "grad_norm": 0.642620861530304,
      "learning_rate": 4.292796429080204e-05,
      "loss": 0.6482,
      "step": 165550
    },
    {
      "epoch": 2.357966680905596,
      "grad_norm": 0.5302549600601196,
      "learning_rate": 4.28804786552068e-05,
      "loss": 0.6584,
      "step": 165600
    },
    {
      "epoch": 2.358678627367222,
      "grad_norm": 0.4712216556072235,
      "learning_rate": 4.2832993019611566e-05,
      "loss": 0.6339,
      "step": 165650
    },
    {
      "epoch": 2.359390573828848,
      "grad_norm": 0.7924913763999939,
      "learning_rate": 4.278550738401634e-05,
      "loss": 0.6634,
      "step": 165700
    },
    {
      "epoch": 2.360102520290474,
      "grad_norm": 0.5203506946563721,
      "learning_rate": 4.27380217484211e-05,
      "loss": 0.6205,
      "step": 165750
    },
    {
      "epoch": 2.3608144667521,
      "grad_norm": 0.7226788401603699,
      "learning_rate": 4.269053611282587e-05,
      "loss": 0.6477,
      "step": 165800
    },
    {
      "epoch": 2.3615264132137264,
      "grad_norm": 0.6209343075752258,
      "learning_rate": 4.264305047723064e-05,
      "loss": 0.7005,
      "step": 165850
    },
    {
      "epoch": 2.3622383596753522,
      "grad_norm": 0.4826259911060333,
      "learning_rate": 4.259556484163541e-05,
      "loss": 0.6397,
      "step": 165900
    },
    {
      "epoch": 2.3629503061369785,
      "grad_norm": 0.8693824410438538,
      "learning_rate": 4.254807920604018e-05,
      "loss": 0.6426,
      "step": 165950
    },
    {
      "epoch": 2.3636622525986044,
      "grad_norm": 0.5377859473228455,
      "learning_rate": 4.250059357044494e-05,
      "loss": 0.6414,
      "step": 166000
    },
    {
      "epoch": 2.3643741990602307,
      "grad_norm": 0.5714370012283325,
      "learning_rate": 4.245310793484971e-05,
      "loss": 0.6771,
      "step": 166050
    },
    {
      "epoch": 2.3650861455218566,
      "grad_norm": 0.560341477394104,
      "learning_rate": 4.2405622299254477e-05,
      "loss": 0.6246,
      "step": 166100
    },
    {
      "epoch": 2.365798091983483,
      "grad_norm": 0.5017930269241333,
      "learning_rate": 4.235813666365924e-05,
      "loss": 0.623,
      "step": 166150
    },
    {
      "epoch": 2.3665100384451088,
      "grad_norm": 0.4796738624572754,
      "learning_rate": 4.231065102806401e-05,
      "loss": 0.6634,
      "step": 166200
    },
    {
      "epoch": 2.367221984906735,
      "grad_norm": 0.4448297321796417,
      "learning_rate": 4.2263165392468775e-05,
      "loss": 0.6275,
      "step": 166250
    },
    {
      "epoch": 2.367933931368361,
      "grad_norm": 0.5040013194084167,
      "learning_rate": 4.221662946958545e-05,
      "loss": 0.6003,
      "step": 166300
    },
    {
      "epoch": 2.3686458778299873,
      "grad_norm": 0.7272627353668213,
      "learning_rate": 4.216914383399022e-05,
      "loss": 0.6727,
      "step": 166350
    },
    {
      "epoch": 2.369357824291613,
      "grad_norm": 0.7135728001594543,
      "learning_rate": 4.212165819839499e-05,
      "loss": 0.6251,
      "step": 166400
    },
    {
      "epoch": 2.3700697707532394,
      "grad_norm": 0.6774812936782837,
      "learning_rate": 4.207417256279976e-05,
      "loss": 0.5634,
      "step": 166450
    },
    {
      "epoch": 2.3707817172148653,
      "grad_norm": 0.5915438532829285,
      "learning_rate": 4.2026686927204525e-05,
      "loss": 0.6603,
      "step": 166500
    },
    {
      "epoch": 2.3714936636764916,
      "grad_norm": 0.5856550931930542,
      "learning_rate": 4.197920129160929e-05,
      "loss": 0.6228,
      "step": 166550
    },
    {
      "epoch": 2.3722056101381175,
      "grad_norm": 0.6417903900146484,
      "learning_rate": 4.193171565601406e-05,
      "loss": 0.6213,
      "step": 166600
    },
    {
      "epoch": 2.372917556599744,
      "grad_norm": 0.43680575489997864,
      "learning_rate": 4.1884230020418824e-05,
      "loss": 0.614,
      "step": 166650
    },
    {
      "epoch": 2.3736295030613697,
      "grad_norm": 0.636881947517395,
      "learning_rate": 4.1836744384823594e-05,
      "loss": 0.6169,
      "step": 166700
    },
    {
      "epoch": 2.374341449522996,
      "grad_norm": 0.5162856578826904,
      "learning_rate": 4.178925874922836e-05,
      "loss": 0.6234,
      "step": 166750
    },
    {
      "epoch": 2.375053395984622,
      "grad_norm": 0.5740103125572205,
      "learning_rate": 4.174177311363312e-05,
      "loss": 0.6341,
      "step": 166800
    },
    {
      "epoch": 2.375765342446248,
      "grad_norm": 0.3898829221725464,
      "learning_rate": 4.16942874780379e-05,
      "loss": 0.6489,
      "step": 166850
    },
    {
      "epoch": 2.376477288907874,
      "grad_norm": 0.6939020156860352,
      "learning_rate": 4.1646801842442664e-05,
      "loss": 0.5926,
      "step": 166900
    },
    {
      "epoch": 2.3771892353695003,
      "grad_norm": 0.41392403841018677,
      "learning_rate": 4.1599316206847435e-05,
      "loss": 0.656,
      "step": 166950
    },
    {
      "epoch": 2.377901181831126,
      "grad_norm": 0.4387723505496979,
      "learning_rate": 4.15518305712522e-05,
      "loss": 0.623,
      "step": 167000
    },
    {
      "epoch": 2.3786131282927525,
      "grad_norm": 0.5558621883392334,
      "learning_rate": 4.150434493565696e-05,
      "loss": 0.6451,
      "step": 167050
    },
    {
      "epoch": 2.3793250747543784,
      "grad_norm": 0.5738113522529602,
      "learning_rate": 4.1456859300061734e-05,
      "loss": 0.6833,
      "step": 167100
    },
    {
      "epoch": 2.3800370212160047,
      "grad_norm": 0.46461740136146545,
      "learning_rate": 4.14093736644665e-05,
      "loss": 0.6106,
      "step": 167150
    },
    {
      "epoch": 2.3807489676776306,
      "grad_norm": 0.8256558179855347,
      "learning_rate": 4.136188802887127e-05,
      "loss": 0.6018,
      "step": 167200
    },
    {
      "epoch": 2.381460914139257,
      "grad_norm": 0.6902067065238953,
      "learning_rate": 4.131440239327603e-05,
      "loss": 0.6562,
      "step": 167250
    },
    {
      "epoch": 2.3821728606008827,
      "grad_norm": 0.38790595531463623,
      "learning_rate": 4.1266916757680804e-05,
      "loss": 0.6064,
      "step": 167300
    },
    {
      "epoch": 2.382884807062509,
      "grad_norm": 0.3135185241699219,
      "learning_rate": 4.1219431122085575e-05,
      "loss": 0.5995,
      "step": 167350
    },
    {
      "epoch": 2.383596753524135,
      "grad_norm": 0.6770346760749817,
      "learning_rate": 4.117194548649034e-05,
      "loss": 0.5847,
      "step": 167400
    },
    {
      "epoch": 2.3843086999857612,
      "grad_norm": 0.35160350799560547,
      "learning_rate": 4.112445985089511e-05,
      "loss": 0.6274,
      "step": 167450
    },
    {
      "epoch": 2.385020646447387,
      "grad_norm": 0.7854997515678406,
      "learning_rate": 4.1076974215299873e-05,
      "loss": 0.6412,
      "step": 167500
    },
    {
      "epoch": 2.3857325929090134,
      "grad_norm": 0.6590723395347595,
      "learning_rate": 4.102948857970464e-05,
      "loss": 0.6294,
      "step": 167550
    },
    {
      "epoch": 2.3864445393706393,
      "grad_norm": 0.5179527997970581,
      "learning_rate": 4.098200294410941e-05,
      "loss": 0.5898,
      "step": 167600
    },
    {
      "epoch": 2.3871564858322656,
      "grad_norm": 0.5032553672790527,
      "learning_rate": 4.093451730851417e-05,
      "loss": 0.6612,
      "step": 167650
    },
    {
      "epoch": 2.3878684322938915,
      "grad_norm": 0.5612747073173523,
      "learning_rate": 4.088703167291894e-05,
      "loss": 0.5474,
      "step": 167700
    },
    {
      "epoch": 2.3885803787555178,
      "grad_norm": 0.5602944493293762,
      "learning_rate": 4.0839546037323714e-05,
      "loss": 0.6617,
      "step": 167750
    },
    {
      "epoch": 2.3892923252171436,
      "grad_norm": 1.0778717994689941,
      "learning_rate": 4.079206040172848e-05,
      "loss": 0.5905,
      "step": 167800
    },
    {
      "epoch": 2.39000427167877,
      "grad_norm": 0.3790963888168335,
      "learning_rate": 4.074457476613325e-05,
      "loss": 0.6139,
      "step": 167850
    },
    {
      "epoch": 2.390716218140396,
      "grad_norm": 0.5548891425132751,
      "learning_rate": 4.069708913053801e-05,
      "loss": 0.6572,
      "step": 167900
    },
    {
      "epoch": 2.391428164602022,
      "grad_norm": 0.5819084048271179,
      "learning_rate": 4.0649603494942784e-05,
      "loss": 0.6487,
      "step": 167950
    },
    {
      "epoch": 2.392140111063648,
      "grad_norm": 0.44536781311035156,
      "learning_rate": 4.060211785934755e-05,
      "loss": 0.6883,
      "step": 168000
    },
    {
      "epoch": 2.392852057525274,
      "grad_norm": 1.0270931720733643,
      "learning_rate": 4.055463222375231e-05,
      "loss": 0.6257,
      "step": 168050
    },
    {
      "epoch": 2.3935640039869,
      "grad_norm": 0.5546972155570984,
      "learning_rate": 4.050714658815708e-05,
      "loss": 0.625,
      "step": 168100
    },
    {
      "epoch": 2.3942759504485265,
      "grad_norm": 0.536172091960907,
      "learning_rate": 4.0459660952561854e-05,
      "loss": 0.6805,
      "step": 168150
    },
    {
      "epoch": 2.3949878969101523,
      "grad_norm": 0.5473588705062866,
      "learning_rate": 4.0412175316966624e-05,
      "loss": 0.6804,
      "step": 168200
    },
    {
      "epoch": 2.395699843371778,
      "grad_norm": 0.2989763021469116,
      "learning_rate": 4.036468968137139e-05,
      "loss": 0.5618,
      "step": 168250
    },
    {
      "epoch": 2.3964117898334045,
      "grad_norm": 0.49669718742370605,
      "learning_rate": 4.031720404577615e-05,
      "loss": 0.6533,
      "step": 168300
    },
    {
      "epoch": 2.397123736295031,
      "grad_norm": 0.7266027927398682,
      "learning_rate": 4.027066812289283e-05,
      "loss": 0.7101,
      "step": 168350
    },
    {
      "epoch": 2.3978356827566567,
      "grad_norm": 0.6265985369682312,
      "learning_rate": 4.0223182487297596e-05,
      "loss": 0.6176,
      "step": 168400
    },
    {
      "epoch": 2.3985476292182826,
      "grad_norm": 0.6192251443862915,
      "learning_rate": 4.017569685170236e-05,
      "loss": 0.6592,
      "step": 168450
    },
    {
      "epoch": 2.399259575679909,
      "grad_norm": 0.6512449383735657,
      "learning_rate": 4.012916092881903e-05,
      "loss": 0.6453,
      "step": 168500
    },
    {
      "epoch": 2.399971522141535,
      "grad_norm": 0.5780828595161438,
      "learning_rate": 4.0081675293223803e-05,
      "loss": 0.5938,
      "step": 168550
    },
    {
      "epoch": 2.400683468603161,
      "grad_norm": 0.5385697484016418,
      "learning_rate": 4.003418965762857e-05,
      "loss": 0.6589,
      "step": 168600
    },
    {
      "epoch": 2.401395415064787,
      "grad_norm": 0.44038665294647217,
      "learning_rate": 3.998670402203334e-05,
      "loss": 0.6525,
      "step": 168650
    },
    {
      "epoch": 2.4021073615264132,
      "grad_norm": 0.9466754794120789,
      "learning_rate": 3.99392183864381e-05,
      "loss": 0.6276,
      "step": 168700
    },
    {
      "epoch": 2.402819307988039,
      "grad_norm": 0.5150135159492493,
      "learning_rate": 3.9891732750842866e-05,
      "loss": 0.6347,
      "step": 168750
    },
    {
      "epoch": 2.4035312544496654,
      "grad_norm": 0.5899202227592468,
      "learning_rate": 3.984424711524764e-05,
      "loss": 0.6159,
      "step": 168800
    },
    {
      "epoch": 2.4042432009112913,
      "grad_norm": 0.6017938256263733,
      "learning_rate": 3.979676147965241e-05,
      "loss": 0.5887,
      "step": 168850
    },
    {
      "epoch": 2.4049551473729176,
      "grad_norm": 0.4808051288127899,
      "learning_rate": 3.974927584405718e-05,
      "loss": 0.6364,
      "step": 168900
    },
    {
      "epoch": 2.4056670938345435,
      "grad_norm": 0.6648914813995361,
      "learning_rate": 3.970179020846194e-05,
      "loss": 0.6168,
      "step": 168950
    },
    {
      "epoch": 2.40637904029617,
      "grad_norm": 0.5646156072616577,
      "learning_rate": 3.965430457286671e-05,
      "loss": 0.6602,
      "step": 169000
    },
    {
      "epoch": 2.4070909867577956,
      "grad_norm": 0.8863014578819275,
      "learning_rate": 3.960681893727148e-05,
      "loss": 0.6153,
      "step": 169050
    },
    {
      "epoch": 2.407802933219422,
      "grad_norm": 0.6893537640571594,
      "learning_rate": 3.955933330167624e-05,
      "loss": 0.6098,
      "step": 169100
    },
    {
      "epoch": 2.408514879681048,
      "grad_norm": 0.5109591484069824,
      "learning_rate": 3.951184766608101e-05,
      "loss": 0.6628,
      "step": 169150
    },
    {
      "epoch": 2.409226826142674,
      "grad_norm": 0.5208398699760437,
      "learning_rate": 3.946436203048578e-05,
      "loss": 0.6353,
      "step": 169200
    },
    {
      "epoch": 2.4099387726043,
      "grad_norm": 0.6897340416908264,
      "learning_rate": 3.941687639489055e-05,
      "loss": 0.6251,
      "step": 169250
    },
    {
      "epoch": 2.4106507190659263,
      "grad_norm": 0.46859171986579895,
      "learning_rate": 3.936939075929532e-05,
      "loss": 0.5232,
      "step": 169300
    },
    {
      "epoch": 2.411362665527552,
      "grad_norm": 0.4245349168777466,
      "learning_rate": 3.932190512370008e-05,
      "loss": 0.5995,
      "step": 169350
    },
    {
      "epoch": 2.4120746119891785,
      "grad_norm": 0.8851580023765564,
      "learning_rate": 3.927441948810485e-05,
      "loss": 0.6151,
      "step": 169400
    },
    {
      "epoch": 2.4127865584508044,
      "grad_norm": 0.4269707500934601,
      "learning_rate": 3.922693385250962e-05,
      "loss": 0.6224,
      "step": 169450
    },
    {
      "epoch": 2.4134985049124307,
      "grad_norm": 0.3768964409828186,
      "learning_rate": 3.917944821691438e-05,
      "loss": 0.6132,
      "step": 169500
    },
    {
      "epoch": 2.4142104513740565,
      "grad_norm": 0.8751674890518188,
      "learning_rate": 3.913196258131915e-05,
      "loss": 0.6321,
      "step": 169550
    },
    {
      "epoch": 2.414922397835683,
      "grad_norm": 0.7424835562705994,
      "learning_rate": 3.9084476945723916e-05,
      "loss": 0.6371,
      "step": 169600
    },
    {
      "epoch": 2.4156343442973087,
      "grad_norm": 0.7331714630126953,
      "learning_rate": 3.903699131012869e-05,
      "loss": 0.6294,
      "step": 169650
    },
    {
      "epoch": 2.416346290758935,
      "grad_norm": 0.5237945318222046,
      "learning_rate": 3.898950567453346e-05,
      "loss": 0.6126,
      "step": 169700
    },
    {
      "epoch": 2.417058237220561,
      "grad_norm": 0.8541487455368042,
      "learning_rate": 3.894202003893822e-05,
      "loss": 0.63,
      "step": 169750
    },
    {
      "epoch": 2.417770183682187,
      "grad_norm": 0.7015787363052368,
      "learning_rate": 3.889453440334299e-05,
      "loss": 0.6618,
      "step": 169800
    },
    {
      "epoch": 2.418482130143813,
      "grad_norm": 0.48458603024482727,
      "learning_rate": 3.884704876774776e-05,
      "loss": 0.6268,
      "step": 169850
    },
    {
      "epoch": 2.4191940766054394,
      "grad_norm": 0.5254530310630798,
      "learning_rate": 3.879956313215253e-05,
      "loss": 0.6812,
      "step": 169900
    },
    {
      "epoch": 2.4199060230670653,
      "grad_norm": 0.903440535068512,
      "learning_rate": 3.875207749655729e-05,
      "loss": 0.5991,
      "step": 169950
    },
    {
      "epoch": 2.4206179695286916,
      "grad_norm": 0.6519542336463928,
      "learning_rate": 3.8704591860962056e-05,
      "loss": 0.5857,
      "step": 170000
    },
    {
      "epoch": 2.4213299159903174,
      "grad_norm": 0.7692539691925049,
      "learning_rate": 3.865710622536683e-05,
      "loss": 0.6234,
      "step": 170050
    },
    {
      "epoch": 2.4220418624519438,
      "grad_norm": 0.5720834732055664,
      "learning_rate": 3.86096205897716e-05,
      "loss": 0.5672,
      "step": 170100
    },
    {
      "epoch": 2.4227538089135696,
      "grad_norm": 0.6575677990913391,
      "learning_rate": 3.856213495417637e-05,
      "loss": 0.5972,
      "step": 170150
    },
    {
      "epoch": 2.423465755375196,
      "grad_norm": 0.7692657113075256,
      "learning_rate": 3.851464931858113e-05,
      "loss": 0.6682,
      "step": 170200
    },
    {
      "epoch": 2.424177701836822,
      "grad_norm": 0.6673375964164734,
      "learning_rate": 3.8467163682985896e-05,
      "loss": 0.5576,
      "step": 170250
    },
    {
      "epoch": 2.424889648298448,
      "grad_norm": 0.47799691557884216,
      "learning_rate": 3.841967804739067e-05,
      "loss": 0.6157,
      "step": 170300
    },
    {
      "epoch": 2.425601594760074,
      "grad_norm": 0.6943153142929077,
      "learning_rate": 3.837219241179543e-05,
      "loss": 0.6169,
      "step": 170350
    },
    {
      "epoch": 2.4263135412217003,
      "grad_norm": 0.7131806015968323,
      "learning_rate": 3.83247067762002e-05,
      "loss": 0.6258,
      "step": 170400
    },
    {
      "epoch": 2.427025487683326,
      "grad_norm": 0.6754544377326965,
      "learning_rate": 3.8277221140604966e-05,
      "loss": 0.6168,
      "step": 170450
    },
    {
      "epoch": 2.4277374341449525,
      "grad_norm": 0.6975967288017273,
      "learning_rate": 3.822973550500974e-05,
      "loss": 0.6343,
      "step": 170500
    },
    {
      "epoch": 2.4284493806065783,
      "grad_norm": 0.5930221676826477,
      "learning_rate": 3.818224986941451e-05,
      "loss": 0.6049,
      "step": 170550
    },
    {
      "epoch": 2.4291613270682046,
      "grad_norm": 1.060437560081482,
      "learning_rate": 3.813476423381927e-05,
      "loss": 0.6094,
      "step": 170600
    },
    {
      "epoch": 2.4298732735298305,
      "grad_norm": 0.5453473925590515,
      "learning_rate": 3.808727859822404e-05,
      "loss": 0.6222,
      "step": 170650
    },
    {
      "epoch": 2.430585219991457,
      "grad_norm": 0.5481536388397217,
      "learning_rate": 3.803979296262881e-05,
      "loss": 0.6132,
      "step": 170700
    },
    {
      "epoch": 2.4312971664530827,
      "grad_norm": 0.6760702729225159,
      "learning_rate": 3.799230732703357e-05,
      "loss": 0.5853,
      "step": 170750
    },
    {
      "epoch": 2.432009112914709,
      "grad_norm": 0.527374804019928,
      "learning_rate": 3.794482169143834e-05,
      "loss": 0.572,
      "step": 170800
    },
    {
      "epoch": 2.432721059376335,
      "grad_norm": 0.5629390478134155,
      "learning_rate": 3.7897336055843106e-05,
      "loss": 0.6593,
      "step": 170850
    },
    {
      "epoch": 2.433433005837961,
      "grad_norm": 0.3940265476703644,
      "learning_rate": 3.784985042024788e-05,
      "loss": 0.5767,
      "step": 170900
    },
    {
      "epoch": 2.434144952299587,
      "grad_norm": 0.5056038498878479,
      "learning_rate": 3.780236478465265e-05,
      "loss": 0.6292,
      "step": 170950
    },
    {
      "epoch": 2.434856898761213,
      "grad_norm": 0.31063830852508545,
      "learning_rate": 3.775487914905741e-05,
      "loss": 0.6259,
      "step": 171000
    },
    {
      "epoch": 2.4355688452228392,
      "grad_norm": 0.5852217674255371,
      "learning_rate": 3.770739351346218e-05,
      "loss": 0.5865,
      "step": 171050
    },
    {
      "epoch": 2.4362807916844655,
      "grad_norm": 0.8381847143173218,
      "learning_rate": 3.7659907877866946e-05,
      "loss": 0.6417,
      "step": 171100
    },
    {
      "epoch": 2.4369927381460914,
      "grad_norm": 0.7194189429283142,
      "learning_rate": 3.761242224227172e-05,
      "loss": 0.6165,
      "step": 171150
    },
    {
      "epoch": 2.4377046846077173,
      "grad_norm": 0.5427435040473938,
      "learning_rate": 3.756493660667648e-05,
      "loss": 0.6089,
      "step": 171200
    },
    {
      "epoch": 2.4384166310693436,
      "grad_norm": 0.44675999879837036,
      "learning_rate": 3.7517450971081245e-05,
      "loss": 0.6174,
      "step": 171250
    },
    {
      "epoch": 2.43912857753097,
      "grad_norm": 0.49594810605049133,
      "learning_rate": 3.7469965335486016e-05,
      "loss": 0.6166,
      "step": 171300
    },
    {
      "epoch": 2.4398405239925958,
      "grad_norm": 0.5942407846450806,
      "learning_rate": 3.742247969989078e-05,
      "loss": 0.6118,
      "step": 171350
    },
    {
      "epoch": 2.4405524704542216,
      "grad_norm": 0.5419918894767761,
      "learning_rate": 3.737499406429555e-05,
      "loss": 0.5688,
      "step": 171400
    },
    {
      "epoch": 2.441264416915848,
      "grad_norm": 0.5090631246566772,
      "learning_rate": 3.732750842870032e-05,
      "loss": 0.6335,
      "step": 171450
    },
    {
      "epoch": 2.4419763633774743,
      "grad_norm": 0.5049208998680115,
      "learning_rate": 3.7280022793105086e-05,
      "loss": 0.6228,
      "step": 171500
    },
    {
      "epoch": 2.4426883098391,
      "grad_norm": 0.6948191523551941,
      "learning_rate": 3.723253715750986e-05,
      "loss": 0.6343,
      "step": 171550
    },
    {
      "epoch": 2.443400256300726,
      "grad_norm": 0.5644991993904114,
      "learning_rate": 3.718505152191462e-05,
      "loss": 0.6232,
      "step": 171600
    },
    {
      "epoch": 2.4441122027623523,
      "grad_norm": 0.47996941208839417,
      "learning_rate": 3.713756588631939e-05,
      "loss": 0.661,
      "step": 171650
    },
    {
      "epoch": 2.444824149223978,
      "grad_norm": 0.587693989276886,
      "learning_rate": 3.7090080250724156e-05,
      "loss": 0.6459,
      "step": 171700
    },
    {
      "epoch": 2.4455360956856045,
      "grad_norm": 0.5718822479248047,
      "learning_rate": 3.704259461512892e-05,
      "loss": 0.6081,
      "step": 171750
    },
    {
      "epoch": 2.4462480421472304,
      "grad_norm": 0.4768083989620209,
      "learning_rate": 3.699510897953369e-05,
      "loss": 0.62,
      "step": 171800
    },
    {
      "epoch": 2.4469599886088567,
      "grad_norm": 0.5892204642295837,
      "learning_rate": 3.694762334393846e-05,
      "loss": 0.6178,
      "step": 171850
    },
    {
      "epoch": 2.4476719350704825,
      "grad_norm": 0.7306469678878784,
      "learning_rate": 3.690013770834323e-05,
      "loss": 0.6444,
      "step": 171900
    },
    {
      "epoch": 2.448383881532109,
      "grad_norm": 0.44075676798820496,
      "learning_rate": 3.6852652072747996e-05,
      "loss": 0.553,
      "step": 171950
    },
    {
      "epoch": 2.4490958279937347,
      "grad_norm": 0.7610949873924255,
      "learning_rate": 3.680516643715276e-05,
      "loss": 0.6301,
      "step": 172000
    },
    {
      "epoch": 2.449807774455361,
      "grad_norm": 0.7724499106407166,
      "learning_rate": 3.675768080155753e-05,
      "loss": 0.6466,
      "step": 172050
    },
    {
      "epoch": 2.450519720916987,
      "grad_norm": 0.5691103339195251,
      "learning_rate": 3.6710195165962295e-05,
      "loss": 0.6309,
      "step": 172100
    },
    {
      "epoch": 2.451231667378613,
      "grad_norm": 0.6033141613006592,
      "learning_rate": 3.6662709530367066e-05,
      "loss": 0.6613,
      "step": 172150
    },
    {
      "epoch": 2.451943613840239,
      "grad_norm": 0.59036785364151,
      "learning_rate": 3.661522389477183e-05,
      "loss": 0.5766,
      "step": 172200
    },
    {
      "epoch": 2.4526555603018654,
      "grad_norm": 0.49146944284439087,
      "learning_rate": 3.65677382591766e-05,
      "loss": 0.6792,
      "step": 172250
    },
    {
      "epoch": 2.4533675067634912,
      "grad_norm": 0.33935174345970154,
      "learning_rate": 3.652025262358137e-05,
      "loss": 0.6265,
      "step": 172300
    },
    {
      "epoch": 2.4540794532251176,
      "grad_norm": 0.5377582907676697,
      "learning_rate": 3.6472766987986136e-05,
      "loss": 0.6447,
      "step": 172350
    },
    {
      "epoch": 2.4547913996867434,
      "grad_norm": 0.5674497485160828,
      "learning_rate": 3.642528135239091e-05,
      "loss": 0.6192,
      "step": 172400
    },
    {
      "epoch": 2.4555033461483697,
      "grad_norm": 0.7330300807952881,
      "learning_rate": 3.637779571679567e-05,
      "loss": 0.6109,
      "step": 172450
    },
    {
      "epoch": 2.4562152926099956,
      "grad_norm": 0.6087262034416199,
      "learning_rate": 3.6330310081200435e-05,
      "loss": 0.597,
      "step": 172500
    },
    {
      "epoch": 2.456927239071622,
      "grad_norm": 0.6017341613769531,
      "learning_rate": 3.6282824445605206e-05,
      "loss": 0.6136,
      "step": 172550
    },
    {
      "epoch": 2.457639185533248,
      "grad_norm": 0.5490163564682007,
      "learning_rate": 3.623533881000997e-05,
      "loss": 0.6117,
      "step": 172600
    },
    {
      "epoch": 2.458351131994874,
      "grad_norm": 0.4675651788711548,
      "learning_rate": 3.618785317441474e-05,
      "loss": 0.6246,
      "step": 172650
    },
    {
      "epoch": 2.4590630784565,
      "grad_norm": 0.4899110496044159,
      "learning_rate": 3.614036753881951e-05,
      "loss": 0.5853,
      "step": 172700
    },
    {
      "epoch": 2.4597750249181263,
      "grad_norm": 0.8116058707237244,
      "learning_rate": 3.6092881903224275e-05,
      "loss": 0.6258,
      "step": 172750
    },
    {
      "epoch": 2.460486971379752,
      "grad_norm": 0.8420628309249878,
      "learning_rate": 3.6045396267629046e-05,
      "loss": 0.6666,
      "step": 172800
    },
    {
      "epoch": 2.4611989178413785,
      "grad_norm": 0.5977303385734558,
      "learning_rate": 3.599886034474572e-05,
      "loss": 0.6338,
      "step": 172850
    },
    {
      "epoch": 2.4619108643030043,
      "grad_norm": 0.5185881853103638,
      "learning_rate": 3.595137470915048e-05,
      "loss": 0.5829,
      "step": 172900
    },
    {
      "epoch": 2.4626228107646306,
      "grad_norm": 0.5477516055107117,
      "learning_rate": 3.5903889073555254e-05,
      "loss": 0.6467,
      "step": 172950
    },
    {
      "epoch": 2.4633347572262565,
      "grad_norm": 0.4138810336589813,
      "learning_rate": 3.585640343796002e-05,
      "loss": 0.6048,
      "step": 173000
    },
    {
      "epoch": 2.464046703687883,
      "grad_norm": 0.5212551951408386,
      "learning_rate": 3.580891780236479e-05,
      "loss": 0.6099,
      "step": 173050
    },
    {
      "epoch": 2.4647586501495087,
      "grad_norm": 0.5524730086326599,
      "learning_rate": 3.576143216676955e-05,
      "loss": 0.5614,
      "step": 173100
    },
    {
      "epoch": 2.465470596611135,
      "grad_norm": 0.8175185322761536,
      "learning_rate": 3.571394653117432e-05,
      "loss": 0.656,
      "step": 173150
    },
    {
      "epoch": 2.466182543072761,
      "grad_norm": 0.7266630530357361,
      "learning_rate": 3.566646089557909e-05,
      "loss": 0.6384,
      "step": 173200
    },
    {
      "epoch": 2.466894489534387,
      "grad_norm": 0.7024163007736206,
      "learning_rate": 3.561897525998386e-05,
      "loss": 0.6425,
      "step": 173250
    },
    {
      "epoch": 2.467606435996013,
      "grad_norm": 0.7264994978904724,
      "learning_rate": 3.557148962438863e-05,
      "loss": 0.6534,
      "step": 173300
    },
    {
      "epoch": 2.4683183824576393,
      "grad_norm": 0.571928083896637,
      "learning_rate": 3.552400398879339e-05,
      "loss": 0.6157,
      "step": 173350
    },
    {
      "epoch": 2.469030328919265,
      "grad_norm": 0.327603280544281,
      "learning_rate": 3.547651835319816e-05,
      "loss": 0.5621,
      "step": 173400
    },
    {
      "epoch": 2.4697422753808915,
      "grad_norm": 0.6926063895225525,
      "learning_rate": 3.542903271760293e-05,
      "loss": 0.6273,
      "step": 173450
    },
    {
      "epoch": 2.4704542218425174,
      "grad_norm": 0.8261429667472839,
      "learning_rate": 3.538154708200769e-05,
      "loss": 0.7093,
      "step": 173500
    },
    {
      "epoch": 2.4711661683041437,
      "grad_norm": 0.4954303503036499,
      "learning_rate": 3.533406144641246e-05,
      "loss": 0.6574,
      "step": 173550
    },
    {
      "epoch": 2.4718781147657696,
      "grad_norm": 0.7727460265159607,
      "learning_rate": 3.528657581081723e-05,
      "loss": 0.65,
      "step": 173600
    },
    {
      "epoch": 2.472590061227396,
      "grad_norm": 0.5307489633560181,
      "learning_rate": 3.5239090175222e-05,
      "loss": 0.6506,
      "step": 173650
    },
    {
      "epoch": 2.4733020076890218,
      "grad_norm": 0.599036455154419,
      "learning_rate": 3.519160453962677e-05,
      "loss": 0.6294,
      "step": 173700
    },
    {
      "epoch": 2.474013954150648,
      "grad_norm": 0.5468067526817322,
      "learning_rate": 3.514411890403153e-05,
      "loss": 0.5522,
      "step": 173750
    },
    {
      "epoch": 2.474725900612274,
      "grad_norm": 0.854727566242218,
      "learning_rate": 3.5096633268436304e-05,
      "loss": 0.6094,
      "step": 173800
    },
    {
      "epoch": 2.4754378470739002,
      "grad_norm": 0.5911221504211426,
      "learning_rate": 3.504914763284107e-05,
      "loss": 0.6412,
      "step": 173850
    },
    {
      "epoch": 2.476149793535526,
      "grad_norm": 0.6305606365203857,
      "learning_rate": 3.500166199724583e-05,
      "loss": 0.5709,
      "step": 173900
    },
    {
      "epoch": 2.476861739997152,
      "grad_norm": 1.3287423849105835,
      "learning_rate": 3.49541763616506e-05,
      "loss": 0.671,
      "step": 173950
    },
    {
      "epoch": 2.4775736864587783,
      "grad_norm": 0.7456496357917786,
      "learning_rate": 3.490669072605537e-05,
      "loss": 0.526,
      "step": 174000
    },
    {
      "epoch": 2.4782856329204046,
      "grad_norm": 0.4429238438606262,
      "learning_rate": 3.485920509046014e-05,
      "loss": 0.6126,
      "step": 174050
    },
    {
      "epoch": 2.4789975793820305,
      "grad_norm": 0.4569355845451355,
      "learning_rate": 3.481171945486491e-05,
      "loss": 0.6309,
      "step": 174100
    },
    {
      "epoch": 2.4797095258436563,
      "grad_norm": 0.5236028432846069,
      "learning_rate": 3.476423381926967e-05,
      "loss": 0.6495,
      "step": 174150
    },
    {
      "epoch": 2.4804214723052826,
      "grad_norm": 0.522986114025116,
      "learning_rate": 3.471674818367444e-05,
      "loss": 0.6374,
      "step": 174200
    },
    {
      "epoch": 2.481133418766909,
      "grad_norm": 0.49101272225379944,
      "learning_rate": 3.466926254807921e-05,
      "loss": 0.5911,
      "step": 174250
    },
    {
      "epoch": 2.481845365228535,
      "grad_norm": 0.6521424055099487,
      "learning_rate": 3.462177691248398e-05,
      "loss": 0.5969,
      "step": 174300
    },
    {
      "epoch": 2.4825573116901607,
      "grad_norm": 0.7432597279548645,
      "learning_rate": 3.457429127688874e-05,
      "loss": 0.6523,
      "step": 174350
    },
    {
      "epoch": 2.483269258151787,
      "grad_norm": 0.8022754192352295,
      "learning_rate": 3.4526805641293506e-05,
      "loss": 0.5998,
      "step": 174400
    },
    {
      "epoch": 2.4839812046134133,
      "grad_norm": 0.9729825854301453,
      "learning_rate": 3.447932000569828e-05,
      "loss": 0.6269,
      "step": 174450
    },
    {
      "epoch": 2.484693151075039,
      "grad_norm": 0.5890662670135498,
      "learning_rate": 3.443183437010304e-05,
      "loss": 0.6587,
      "step": 174500
    },
    {
      "epoch": 2.485405097536665,
      "grad_norm": 0.7832979559898376,
      "learning_rate": 3.438434873450781e-05,
      "loss": 0.6289,
      "step": 174550
    },
    {
      "epoch": 2.4861170439982914,
      "grad_norm": 0.7109131813049316,
      "learning_rate": 3.433686309891258e-05,
      "loss": 0.6298,
      "step": 174600
    },
    {
      "epoch": 2.4868289904599172,
      "grad_norm": 0.5019574165344238,
      "learning_rate": 3.428937746331735e-05,
      "loss": 0.6499,
      "step": 174650
    },
    {
      "epoch": 2.4875409369215435,
      "grad_norm": 0.6405216455459595,
      "learning_rate": 3.424189182772212e-05,
      "loss": 0.6373,
      "step": 174700
    },
    {
      "epoch": 2.4882528833831694,
      "grad_norm": 0.8498146533966064,
      "learning_rate": 3.419440619212688e-05,
      "loss": 0.6029,
      "step": 174750
    },
    {
      "epoch": 2.4889648298447957,
      "grad_norm": 0.7986143827438354,
      "learning_rate": 3.414692055653165e-05,
      "loss": 0.6662,
      "step": 174800
    },
    {
      "epoch": 2.4896767763064216,
      "grad_norm": 0.38682493567466736,
      "learning_rate": 3.4099434920936417e-05,
      "loss": 0.5844,
      "step": 174850
    },
    {
      "epoch": 2.490388722768048,
      "grad_norm": 0.7493003010749817,
      "learning_rate": 3.405194928534118e-05,
      "loss": 0.6231,
      "step": 174900
    },
    {
      "epoch": 2.4911006692296738,
      "grad_norm": 0.723987340927124,
      "learning_rate": 3.400446364974595e-05,
      "loss": 0.6198,
      "step": 174950
    },
    {
      "epoch": 2.4918126156913,
      "grad_norm": 0.51481032371521,
      "learning_rate": 3.3957927726862624e-05,
      "loss": 0.6296,
      "step": 175000
    },
    {
      "epoch": 2.492524562152926,
      "grad_norm": 1.0092594623565674,
      "learning_rate": 3.3910442091267395e-05,
      "loss": 0.6518,
      "step": 175050
    },
    {
      "epoch": 2.4932365086145523,
      "grad_norm": 0.9838377833366394,
      "learning_rate": 3.3862956455672166e-05,
      "loss": 0.6363,
      "step": 175100
    },
    {
      "epoch": 2.493948455076178,
      "grad_norm": 0.6164916753768921,
      "learning_rate": 3.381547082007693e-05,
      "loss": 0.6219,
      "step": 175150
    },
    {
      "epoch": 2.4946604015378044,
      "grad_norm": 0.626569926738739,
      "learning_rate": 3.37679851844817e-05,
      "loss": 0.6206,
      "step": 175200
    },
    {
      "epoch": 2.4953723479994303,
      "grad_norm": 0.8964367508888245,
      "learning_rate": 3.3720499548886465e-05,
      "loss": 0.6243,
      "step": 175250
    },
    {
      "epoch": 2.4960842944610566,
      "grad_norm": 0.6825113296508789,
      "learning_rate": 3.367301391329123e-05,
      "loss": 0.6659,
      "step": 175300
    },
    {
      "epoch": 2.4967962409226825,
      "grad_norm": 0.8908312320709229,
      "learning_rate": 3.3625528277696e-05,
      "loss": 0.6646,
      "step": 175350
    },
    {
      "epoch": 2.497508187384309,
      "grad_norm": 0.6088834404945374,
      "learning_rate": 3.3578042642100764e-05,
      "loss": 0.6029,
      "step": 175400
    },
    {
      "epoch": 2.4982201338459347,
      "grad_norm": 0.43151476979255676,
      "learning_rate": 3.3530557006505534e-05,
      "loss": 0.5498,
      "step": 175450
    },
    {
      "epoch": 2.498932080307561,
      "grad_norm": 0.6388131976127625,
      "learning_rate": 3.34830713709103e-05,
      "loss": 0.6586,
      "step": 175500
    },
    {
      "epoch": 2.499644026769187,
      "grad_norm": 0.33188024163246155,
      "learning_rate": 3.343558573531507e-05,
      "loss": 0.6456,
      "step": 175550
    },
    {
      "epoch": 2.500355973230813,
      "grad_norm": 0.49841731786727905,
      "learning_rate": 3.338810009971984e-05,
      "loss": 0.6336,
      "step": 175600
    },
    {
      "epoch": 2.501067919692439,
      "grad_norm": 0.49974527955055237,
      "learning_rate": 3.3340614464124604e-05,
      "loss": 0.5973,
      "step": 175650
    },
    {
      "epoch": 2.5017798661540653,
      "grad_norm": 0.7377253174781799,
      "learning_rate": 3.3293128828529375e-05,
      "loss": 0.5901,
      "step": 175700
    },
    {
      "epoch": 2.502491812615691,
      "grad_norm": 0.583877444267273,
      "learning_rate": 3.324564319293414e-05,
      "loss": 0.6455,
      "step": 175750
    },
    {
      "epoch": 2.5032037590773175,
      "grad_norm": 0.5129926800727844,
      "learning_rate": 3.31981575573389e-05,
      "loss": 0.6018,
      "step": 175800
    },
    {
      "epoch": 2.5039157055389434,
      "grad_norm": 0.7812789678573608,
      "learning_rate": 3.3150671921743674e-05,
      "loss": 0.6279,
      "step": 175850
    },
    {
      "epoch": 2.5046276520005697,
      "grad_norm": 0.6066351532936096,
      "learning_rate": 3.310318628614844e-05,
      "loss": 0.5894,
      "step": 175900
    },
    {
      "epoch": 2.5053395984621956,
      "grad_norm": 0.6235257387161255,
      "learning_rate": 3.305570065055321e-05,
      "loss": 0.6023,
      "step": 175950
    },
    {
      "epoch": 2.506051544923822,
      "grad_norm": 0.7168755531311035,
      "learning_rate": 3.300821501495798e-05,
      "loss": 0.6399,
      "step": 176000
    },
    {
      "epoch": 2.5067634913854477,
      "grad_norm": 0.7212738394737244,
      "learning_rate": 3.2960729379362744e-05,
      "loss": 0.6081,
      "step": 176050
    },
    {
      "epoch": 2.507475437847074,
      "grad_norm": 0.5677173137664795,
      "learning_rate": 3.2914193456479416e-05,
      "loss": 0.6465,
      "step": 176100
    },
    {
      "epoch": 2.5081873843087,
      "grad_norm": 0.44155189394950867,
      "learning_rate": 3.286670782088419e-05,
      "loss": 0.6337,
      "step": 176150
    },
    {
      "epoch": 2.5088993307703262,
      "grad_norm": 0.8415613770484924,
      "learning_rate": 3.281922218528895e-05,
      "loss": 0.6469,
      "step": 176200
    },
    {
      "epoch": 2.509611277231952,
      "grad_norm": 0.5274673700332642,
      "learning_rate": 3.277173654969372e-05,
      "loss": 0.6213,
      "step": 176250
    },
    {
      "epoch": 2.5103232236935784,
      "grad_norm": 0.6277701258659363,
      "learning_rate": 3.2724250914098486e-05,
      "loss": 0.5915,
      "step": 176300
    },
    {
      "epoch": 2.5110351701552043,
      "grad_norm": 0.5106731653213501,
      "learning_rate": 3.267676527850325e-05,
      "loss": 0.6207,
      "step": 176350
    },
    {
      "epoch": 2.5117471166168306,
      "grad_norm": 0.48940902948379517,
      "learning_rate": 3.262927964290802e-05,
      "loss": 0.5743,
      "step": 176400
    },
    {
      "epoch": 2.5124590630784565,
      "grad_norm": 0.4882277846336365,
      "learning_rate": 3.2581794007312785e-05,
      "loss": 0.6719,
      "step": 176450
    },
    {
      "epoch": 2.5131710095400823,
      "grad_norm": 0.7241308689117432,
      "learning_rate": 3.2534308371717556e-05,
      "loss": 0.667,
      "step": 176500
    },
    {
      "epoch": 2.5138829560017086,
      "grad_norm": 0.7412237524986267,
      "learning_rate": 3.248682273612233e-05,
      "loss": 0.628,
      "step": 176550
    },
    {
      "epoch": 2.514594902463335,
      "grad_norm": 0.4714491367340088,
      "learning_rate": 3.243933710052709e-05,
      "loss": 0.5722,
      "step": 176600
    },
    {
      "epoch": 2.515306848924961,
      "grad_norm": 0.6129506826400757,
      "learning_rate": 3.239185146493186e-05,
      "loss": 0.666,
      "step": 176650
    },
    {
      "epoch": 2.5160187953865867,
      "grad_norm": 0.5313162207603455,
      "learning_rate": 3.2344365829336626e-05,
      "loss": 0.6208,
      "step": 176700
    },
    {
      "epoch": 2.516730741848213,
      "grad_norm": 0.6144934892654419,
      "learning_rate": 3.2296880193741396e-05,
      "loss": 0.5963,
      "step": 176750
    },
    {
      "epoch": 2.5174426883098393,
      "grad_norm": 0.9061194062232971,
      "learning_rate": 3.224939455814616e-05,
      "loss": 0.6441,
      "step": 176800
    },
    {
      "epoch": 2.518154634771465,
      "grad_norm": 0.7587077021598816,
      "learning_rate": 3.2201908922550925e-05,
      "loss": 0.6303,
      "step": 176850
    },
    {
      "epoch": 2.518866581233091,
      "grad_norm": 0.8339639902114868,
      "learning_rate": 3.2154423286955695e-05,
      "loss": 0.5892,
      "step": 176900
    },
    {
      "epoch": 2.5195785276947174,
      "grad_norm": 0.767310380935669,
      "learning_rate": 3.2106937651360466e-05,
      "loss": 0.6014,
      "step": 176950
    },
    {
      "epoch": 2.5202904741563437,
      "grad_norm": 0.4917021691799164,
      "learning_rate": 3.205945201576524e-05,
      "loss": 0.6382,
      "step": 177000
    },
    {
      "epoch": 2.5210024206179695,
      "grad_norm": 0.7912982106208801,
      "learning_rate": 3.201196638017e-05,
      "loss": 0.5945,
      "step": 177050
    },
    {
      "epoch": 2.5217143670795954,
      "grad_norm": 0.7817532420158386,
      "learning_rate": 3.1964480744574765e-05,
      "loss": 0.5916,
      "step": 177100
    },
    {
      "epoch": 2.5224263135412217,
      "grad_norm": 0.5560590028762817,
      "learning_rate": 3.1916995108979536e-05,
      "loss": 0.6233,
      "step": 177150
    },
    {
      "epoch": 2.523138260002848,
      "grad_norm": 0.7098690271377563,
      "learning_rate": 3.18695094733843e-05,
      "loss": 0.6289,
      "step": 177200
    },
    {
      "epoch": 2.523850206464474,
      "grad_norm": 0.4595727324485779,
      "learning_rate": 3.182202383778907e-05,
      "loss": 0.6579,
      "step": 177250
    },
    {
      "epoch": 2.5245621529260998,
      "grad_norm": 0.7489604353904724,
      "learning_rate": 3.1774538202193835e-05,
      "loss": 0.5747,
      "step": 177300
    },
    {
      "epoch": 2.525274099387726,
      "grad_norm": 0.6710104942321777,
      "learning_rate": 3.1727052566598606e-05,
      "loss": 0.599,
      "step": 177350
    },
    {
      "epoch": 2.5259860458493524,
      "grad_norm": 0.45916643738746643,
      "learning_rate": 3.1679566931003377e-05,
      "loss": 0.6356,
      "step": 177400
    },
    {
      "epoch": 2.5266979923109782,
      "grad_norm": 0.6061031222343445,
      "learning_rate": 3.163208129540814e-05,
      "loss": 0.6373,
      "step": 177450
    },
    {
      "epoch": 2.527409938772604,
      "grad_norm": 0.7289179563522339,
      "learning_rate": 3.158459565981291e-05,
      "loss": 0.6654,
      "step": 177500
    },
    {
      "epoch": 2.5281218852342304,
      "grad_norm": 0.5066401362419128,
      "learning_rate": 3.1537110024217676e-05,
      "loss": 0.6667,
      "step": 177550
    },
    {
      "epoch": 2.5288338316958567,
      "grad_norm": 0.46615368127822876,
      "learning_rate": 3.148962438862244e-05,
      "loss": 0.6742,
      "step": 177600
    },
    {
      "epoch": 2.5295457781574826,
      "grad_norm": 0.3721095323562622,
      "learning_rate": 3.144213875302721e-05,
      "loss": 0.6326,
      "step": 177650
    },
    {
      "epoch": 2.5302577246191085,
      "grad_norm": 0.43655845522880554,
      "learning_rate": 3.1394653117431974e-05,
      "loss": 0.6104,
      "step": 177700
    },
    {
      "epoch": 2.530969671080735,
      "grad_norm": 0.890724241733551,
      "learning_rate": 3.1347167481836745e-05,
      "loss": 0.6512,
      "step": 177750
    },
    {
      "epoch": 2.5316816175423607,
      "grad_norm": 0.32426461577415466,
      "learning_rate": 3.1299681846241516e-05,
      "loss": 0.5985,
      "step": 177800
    },
    {
      "epoch": 2.532393564003987,
      "grad_norm": 0.6266244053840637,
      "learning_rate": 3.125219621064628e-05,
      "loss": 0.6455,
      "step": 177850
    },
    {
      "epoch": 2.533105510465613,
      "grad_norm": 0.6142084002494812,
      "learning_rate": 3.120471057505105e-05,
      "loss": 0.6835,
      "step": 177900
    },
    {
      "epoch": 2.533817456927239,
      "grad_norm": 0.5490369200706482,
      "learning_rate": 3.1157224939455815e-05,
      "loss": 0.6324,
      "step": 177950
    },
    {
      "epoch": 2.534529403388865,
      "grad_norm": 0.5163689255714417,
      "learning_rate": 3.1109739303860586e-05,
      "loss": 0.6209,
      "step": 178000
    },
    {
      "epoch": 2.5352413498504913,
      "grad_norm": 0.5321555137634277,
      "learning_rate": 3.106225366826535e-05,
      "loss": 0.6354,
      "step": 178050
    },
    {
      "epoch": 2.535953296312117,
      "grad_norm": 0.5438058972358704,
      "learning_rate": 3.1014768032670114e-05,
      "loss": 0.5979,
      "step": 178100
    },
    {
      "epoch": 2.5366652427737435,
      "grad_norm": 0.48697784543037415,
      "learning_rate": 3.0967282397074885e-05,
      "loss": 0.6578,
      "step": 178150
    },
    {
      "epoch": 2.5373771892353694,
      "grad_norm": 0.8634123802185059,
      "learning_rate": 3.0919796761479656e-05,
      "loss": 0.6161,
      "step": 178200
    },
    {
      "epoch": 2.5380891356969957,
      "grad_norm": 0.371403306722641,
      "learning_rate": 3.0872311125884427e-05,
      "loss": 0.6003,
      "step": 178250
    },
    {
      "epoch": 2.5388010821586215,
      "grad_norm": 0.34892329573631287,
      "learning_rate": 3.082482549028919e-05,
      "loss": 0.5916,
      "step": 178300
    },
    {
      "epoch": 2.539513028620248,
      "grad_norm": 0.5577355027198792,
      "learning_rate": 3.0777339854693955e-05,
      "loss": 0.6228,
      "step": 178350
    },
    {
      "epoch": 2.5402249750818737,
      "grad_norm": 0.5168969035148621,
      "learning_rate": 3.0729854219098725e-05,
      "loss": 0.638,
      "step": 178400
    },
    {
      "epoch": 2.5409369215435,
      "grad_norm": 0.7731125354766846,
      "learning_rate": 3.068236858350349e-05,
      "loss": 0.6463,
      "step": 178450
    },
    {
      "epoch": 2.541648868005126,
      "grad_norm": 0.5029135942459106,
      "learning_rate": 3.063488294790826e-05,
      "loss": 0.6023,
      "step": 178500
    },
    {
      "epoch": 2.542360814466752,
      "grad_norm": 0.6188061237335205,
      "learning_rate": 3.0587397312313024e-05,
      "loss": 0.6071,
      "step": 178550
    },
    {
      "epoch": 2.543072760928378,
      "grad_norm": 0.6928516626358032,
      "learning_rate": 3.053991167671779e-05,
      "loss": 0.6469,
      "step": 178600
    },
    {
      "epoch": 2.5437847073900044,
      "grad_norm": 0.5252838730812073,
      "learning_rate": 3.0492426041122563e-05,
      "loss": 0.6288,
      "step": 178650
    },
    {
      "epoch": 2.5444966538516303,
      "grad_norm": 0.5928943157196045,
      "learning_rate": 3.0444940405527327e-05,
      "loss": 0.6187,
      "step": 178700
    },
    {
      "epoch": 2.5452086003132566,
      "grad_norm": 0.8439455628395081,
      "learning_rate": 3.0397454769932098e-05,
      "loss": 0.6136,
      "step": 178750
    },
    {
      "epoch": 2.5459205467748824,
      "grad_norm": 0.7877865433692932,
      "learning_rate": 3.0349969134336865e-05,
      "loss": 0.6686,
      "step": 178800
    },
    {
      "epoch": 2.5466324932365088,
      "grad_norm": 0.6817658543586731,
      "learning_rate": 3.030248349874163e-05,
      "loss": 0.5596,
      "step": 178850
    },
    {
      "epoch": 2.5473444396981346,
      "grad_norm": 0.8862041234970093,
      "learning_rate": 3.02549978631464e-05,
      "loss": 0.7085,
      "step": 178900
    },
    {
      "epoch": 2.548056386159761,
      "grad_norm": 0.5119351148605347,
      "learning_rate": 3.0207512227551167e-05,
      "loss": 0.6231,
      "step": 178950
    },
    {
      "epoch": 2.548768332621387,
      "grad_norm": 1.0230278968811035,
      "learning_rate": 3.0160026591955938e-05,
      "loss": 0.6185,
      "step": 179000
    },
    {
      "epoch": 2.549480279083013,
      "grad_norm": 0.5683369636535645,
      "learning_rate": 3.0112540956360702e-05,
      "loss": 0.6284,
      "step": 179050
    },
    {
      "epoch": 2.550192225544639,
      "grad_norm": 0.8211681246757507,
      "learning_rate": 3.0065055320765466e-05,
      "loss": 0.6373,
      "step": 179100
    },
    {
      "epoch": 2.5509041720062653,
      "grad_norm": 0.6600590348243713,
      "learning_rate": 3.0017569685170237e-05,
      "loss": 0.619,
      "step": 179150
    },
    {
      "epoch": 2.551616118467891,
      "grad_norm": 0.5217639803886414,
      "learning_rate": 2.9970084049575005e-05,
      "loss": 0.6143,
      "step": 179200
    },
    {
      "epoch": 2.552328064929517,
      "grad_norm": 0.4356484115123749,
      "learning_rate": 2.9922598413979775e-05,
      "loss": 0.6249,
      "step": 179250
    },
    {
      "epoch": 2.5530400113911433,
      "grad_norm": 0.7608672976493835,
      "learning_rate": 2.9876062491096445e-05,
      "loss": 0.6277,
      "step": 179300
    },
    {
      "epoch": 2.5537519578527696,
      "grad_norm": 0.7596253156661987,
      "learning_rate": 2.9828576855501212e-05,
      "loss": 0.6592,
      "step": 179350
    },
    {
      "epoch": 2.5544639043143955,
      "grad_norm": 0.5185864567756653,
      "learning_rate": 2.9781091219905983e-05,
      "loss": 0.6024,
      "step": 179400
    },
    {
      "epoch": 2.5551758507760214,
      "grad_norm": 0.666309654712677,
      "learning_rate": 2.9733605584310747e-05,
      "loss": 0.61,
      "step": 179450
    },
    {
      "epoch": 2.5558877972376477,
      "grad_norm": 0.6719746589660645,
      "learning_rate": 2.9686119948715514e-05,
      "loss": 0.6359,
      "step": 179500
    },
    {
      "epoch": 2.556599743699274,
      "grad_norm": 0.86326003074646,
      "learning_rate": 2.9638634313120285e-05,
      "loss": 0.6829,
      "step": 179550
    },
    {
      "epoch": 2.5573116901609,
      "grad_norm": 0.6598193049430847,
      "learning_rate": 2.959114867752505e-05,
      "loss": 0.6023,
      "step": 179600
    },
    {
      "epoch": 2.5580236366225257,
      "grad_norm": 0.612997829914093,
      "learning_rate": 2.954366304192982e-05,
      "loss": 0.6276,
      "step": 179650
    },
    {
      "epoch": 2.558735583084152,
      "grad_norm": 0.581472635269165,
      "learning_rate": 2.9496177406334584e-05,
      "loss": 0.6342,
      "step": 179700
    },
    {
      "epoch": 2.5594475295457784,
      "grad_norm": 0.5244921445846558,
      "learning_rate": 2.944869177073935e-05,
      "loss": 0.6593,
      "step": 179750
    },
    {
      "epoch": 2.5601594760074042,
      "grad_norm": 0.772901177406311,
      "learning_rate": 2.9401206135144122e-05,
      "loss": 0.6424,
      "step": 179800
    },
    {
      "epoch": 2.56087142246903,
      "grad_norm": 0.8772259950637817,
      "learning_rate": 2.9353720499548886e-05,
      "loss": 0.6089,
      "step": 179850
    },
    {
      "epoch": 2.5615833689306564,
      "grad_norm": 0.9853230118751526,
      "learning_rate": 2.9306234863953657e-05,
      "loss": 0.615,
      "step": 179900
    },
    {
      "epoch": 2.5622953153922827,
      "grad_norm": 0.6189660429954529,
      "learning_rate": 2.9258749228358425e-05,
      "loss": 0.6539,
      "step": 179950
    },
    {
      "epoch": 2.5630072618539086,
      "grad_norm": 0.8612189292907715,
      "learning_rate": 2.921126359276319e-05,
      "loss": 0.6245,
      "step": 180000
    },
    {
      "epoch": 2.5637192083155345,
      "grad_norm": 0.3990000784397125,
      "learning_rate": 2.916377795716796e-05,
      "loss": 0.615,
      "step": 180050
    },
    {
      "epoch": 2.5644311547771608,
      "grad_norm": 0.38603082299232483,
      "learning_rate": 2.9116292321572724e-05,
      "loss": 0.6203,
      "step": 180100
    },
    {
      "epoch": 2.565143101238787,
      "grad_norm": 0.7144617438316345,
      "learning_rate": 2.9068806685977494e-05,
      "loss": 0.6681,
      "step": 180150
    },
    {
      "epoch": 2.565855047700413,
      "grad_norm": 0.553920567035675,
      "learning_rate": 2.9021321050382262e-05,
      "loss": 0.6488,
      "step": 180200
    },
    {
      "epoch": 2.566566994162039,
      "grad_norm": 1.0985004901885986,
      "learning_rate": 2.8973835414787026e-05,
      "loss": 0.6385,
      "step": 180250
    },
    {
      "epoch": 2.567278940623665,
      "grad_norm": 0.7189435958862305,
      "learning_rate": 2.8926349779191797e-05,
      "loss": 0.6286,
      "step": 180300
    },
    {
      "epoch": 2.5679908870852914,
      "grad_norm": 0.5533272624015808,
      "learning_rate": 2.887886414359656e-05,
      "loss": 0.5979,
      "step": 180350
    },
    {
      "epoch": 2.5687028335469173,
      "grad_norm": 0.5634889602661133,
      "learning_rate": 2.8831378508001332e-05,
      "loss": 0.6652,
      "step": 180400
    },
    {
      "epoch": 2.569414780008543,
      "grad_norm": 0.8154162168502808,
      "learning_rate": 2.87838928724061e-05,
      "loss": 0.635,
      "step": 180450
    },
    {
      "epoch": 2.5701267264701695,
      "grad_norm": 0.8215343356132507,
      "learning_rate": 2.8736407236810863e-05,
      "loss": 0.6108,
      "step": 180500
    },
    {
      "epoch": 2.570838672931796,
      "grad_norm": 0.4885670840740204,
      "learning_rate": 2.8688921601215634e-05,
      "loss": 0.6672,
      "step": 180550
    },
    {
      "epoch": 2.5715506193934217,
      "grad_norm": 0.8786566257476807,
      "learning_rate": 2.86414359656204e-05,
      "loss": 0.6252,
      "step": 180600
    },
    {
      "epoch": 2.5722625658550475,
      "grad_norm": 0.7593790292739868,
      "learning_rate": 2.8593950330025172e-05,
      "loss": 0.7123,
      "step": 180650
    },
    {
      "epoch": 2.572974512316674,
      "grad_norm": 0.5235706567764282,
      "learning_rate": 2.8546464694429936e-05,
      "loss": 0.6066,
      "step": 180700
    },
    {
      "epoch": 2.5736864587782997,
      "grad_norm": 0.8901592493057251,
      "learning_rate": 2.84989790588347e-05,
      "loss": 0.5953,
      "step": 180750
    },
    {
      "epoch": 2.574398405239926,
      "grad_norm": 0.5498079657554626,
      "learning_rate": 2.845149342323947e-05,
      "loss": 0.682,
      "step": 180800
    },
    {
      "epoch": 2.575110351701552,
      "grad_norm": 0.5051152110099792,
      "learning_rate": 2.840400778764424e-05,
      "loss": 0.6474,
      "step": 180850
    },
    {
      "epoch": 2.575822298163178,
      "grad_norm": 0.9255136847496033,
      "learning_rate": 2.835652215204901e-05,
      "loss": 0.6255,
      "step": 180900
    },
    {
      "epoch": 2.576534244624804,
      "grad_norm": 0.5249192714691162,
      "learning_rate": 2.8309036516453774e-05,
      "loss": 0.6409,
      "step": 180950
    },
    {
      "epoch": 2.5772461910864304,
      "grad_norm": 0.47042712569236755,
      "learning_rate": 2.826155088085854e-05,
      "loss": 0.6625,
      "step": 181000
    },
    {
      "epoch": 2.5779581375480562,
      "grad_norm": 0.6307588815689087,
      "learning_rate": 2.8214065245263312e-05,
      "loss": 0.6341,
      "step": 181050
    },
    {
      "epoch": 2.5786700840096826,
      "grad_norm": 0.5923211574554443,
      "learning_rate": 2.8166579609668076e-05,
      "loss": 0.6495,
      "step": 181100
    },
    {
      "epoch": 2.5793820304713084,
      "grad_norm": 0.8773027658462524,
      "learning_rate": 2.8119093974072847e-05,
      "loss": 0.6442,
      "step": 181150
    },
    {
      "epoch": 2.5800939769329347,
      "grad_norm": 0.35889363288879395,
      "learning_rate": 2.807160833847761e-05,
      "loss": 0.6609,
      "step": 181200
    },
    {
      "epoch": 2.5808059233945606,
      "grad_norm": 0.9036291241645813,
      "learning_rate": 2.8024122702882378e-05,
      "loss": 0.6161,
      "step": 181250
    },
    {
      "epoch": 2.581517869856187,
      "grad_norm": 0.9605249762535095,
      "learning_rate": 2.797663706728715e-05,
      "loss": 0.6594,
      "step": 181300
    },
    {
      "epoch": 2.582229816317813,
      "grad_norm": 0.4620293080806732,
      "learning_rate": 2.7929151431691913e-05,
      "loss": 0.6782,
      "step": 181350
    },
    {
      "epoch": 2.582941762779439,
      "grad_norm": 0.6995996236801147,
      "learning_rate": 2.7881665796096684e-05,
      "loss": 0.6171,
      "step": 181400
    },
    {
      "epoch": 2.583653709241065,
      "grad_norm": 0.6735500693321228,
      "learning_rate": 2.7834180160501448e-05,
      "loss": 0.6513,
      "step": 181450
    },
    {
      "epoch": 2.5843656557026913,
      "grad_norm": 0.6381338238716125,
      "learning_rate": 2.7786694524906215e-05,
      "loss": 0.6252,
      "step": 181500
    },
    {
      "epoch": 2.585077602164317,
      "grad_norm": 0.47969233989715576,
      "learning_rate": 2.7739208889310986e-05,
      "loss": 0.6244,
      "step": 181550
    },
    {
      "epoch": 2.5857895486259435,
      "grad_norm": 0.5017187595367432,
      "learning_rate": 2.769172325371575e-05,
      "loss": 0.6187,
      "step": 181600
    },
    {
      "epoch": 2.5865014950875693,
      "grad_norm": 0.4472413957118988,
      "learning_rate": 2.764423761812052e-05,
      "loss": 0.5861,
      "step": 181650
    },
    {
      "epoch": 2.5872134415491956,
      "grad_norm": 0.4984409511089325,
      "learning_rate": 2.759675198252529e-05,
      "loss": 0.6008,
      "step": 181700
    },
    {
      "epoch": 2.5879253880108215,
      "grad_norm": 0.5605930685997009,
      "learning_rate": 2.7549266346930053e-05,
      "loss": 0.5867,
      "step": 181750
    },
    {
      "epoch": 2.588637334472448,
      "grad_norm": 0.7322903275489807,
      "learning_rate": 2.7501780711334824e-05,
      "loss": 0.6244,
      "step": 181800
    },
    {
      "epoch": 2.5893492809340737,
      "grad_norm": 0.5021806359291077,
      "learning_rate": 2.7454295075739588e-05,
      "loss": 0.6553,
      "step": 181850
    },
    {
      "epoch": 2.5900612273957,
      "grad_norm": 0.5803574323654175,
      "learning_rate": 2.740680944014436e-05,
      "loss": 0.6239,
      "step": 181900
    },
    {
      "epoch": 2.590773173857326,
      "grad_norm": 0.6222758293151855,
      "learning_rate": 2.7359323804549126e-05,
      "loss": 0.5976,
      "step": 181950
    },
    {
      "epoch": 2.591485120318952,
      "grad_norm": 0.8730967044830322,
      "learning_rate": 2.731183816895389e-05,
      "loss": 0.616,
      "step": 182000
    },
    {
      "epoch": 2.592197066780578,
      "grad_norm": 0.7051030993461609,
      "learning_rate": 2.726435253335866e-05,
      "loss": 0.6598,
      "step": 182050
    },
    {
      "epoch": 2.5929090132422044,
      "grad_norm": 0.7626584768295288,
      "learning_rate": 2.7216866897763428e-05,
      "loss": 0.5959,
      "step": 182100
    },
    {
      "epoch": 2.59362095970383,
      "grad_norm": 0.9356790781021118,
      "learning_rate": 2.71693812621682e-05,
      "loss": 0.6641,
      "step": 182150
    },
    {
      "epoch": 2.594332906165456,
      "grad_norm": 0.9582386612892151,
      "learning_rate": 2.7121895626572963e-05,
      "loss": 0.6553,
      "step": 182200
    },
    {
      "epoch": 2.5950448526270824,
      "grad_norm": 0.6011162400245667,
      "learning_rate": 2.7074409990977727e-05,
      "loss": 0.6295,
      "step": 182250
    },
    {
      "epoch": 2.5957567990887087,
      "grad_norm": 0.6073254346847534,
      "learning_rate": 2.7026924355382498e-05,
      "loss": 0.5947,
      "step": 182300
    },
    {
      "epoch": 2.5964687455503346,
      "grad_norm": 0.7002020478248596,
      "learning_rate": 2.6979438719787265e-05,
      "loss": 0.635,
      "step": 182350
    },
    {
      "epoch": 2.5971806920119604,
      "grad_norm": 0.5804306864738464,
      "learning_rate": 2.6931953084192036e-05,
      "loss": 0.6807,
      "step": 182400
    },
    {
      "epoch": 2.5978926384735868,
      "grad_norm": 0.6510314345359802,
      "learning_rate": 2.68844674485968e-05,
      "loss": 0.6342,
      "step": 182450
    },
    {
      "epoch": 2.598604584935213,
      "grad_norm": 0.35120829939842224,
      "learning_rate": 2.6836981813001564e-05,
      "loss": 0.6621,
      "step": 182500
    },
    {
      "epoch": 2.599316531396839,
      "grad_norm": 0.6292037963867188,
      "learning_rate": 2.678949617740634e-05,
      "loss": 0.6711,
      "step": 182550
    },
    {
      "epoch": 2.600028477858465,
      "grad_norm": 0.4182930588722229,
      "learning_rate": 2.6742010541811103e-05,
      "loss": 0.6589,
      "step": 182600
    },
    {
      "epoch": 2.600740424320091,
      "grad_norm": 0.7567071914672852,
      "learning_rate": 2.6694524906215873e-05,
      "loss": 0.6301,
      "step": 182650
    },
    {
      "epoch": 2.6014523707817174,
      "grad_norm": 1.1175166368484497,
      "learning_rate": 2.6647039270620637e-05,
      "loss": 0.5703,
      "step": 182700
    },
    {
      "epoch": 2.6021643172433433,
      "grad_norm": 0.6291348338127136,
      "learning_rate": 2.6599553635025405e-05,
      "loss": 0.6276,
      "step": 182750
    },
    {
      "epoch": 2.602876263704969,
      "grad_norm": 0.5730504989624023,
      "learning_rate": 2.6552067999430176e-05,
      "loss": 0.6123,
      "step": 182800
    },
    {
      "epoch": 2.6035882101665955,
      "grad_norm": 0.5342438817024231,
      "learning_rate": 2.650458236383494e-05,
      "loss": 0.5974,
      "step": 182850
    },
    {
      "epoch": 2.604300156628222,
      "grad_norm": 0.5156773328781128,
      "learning_rate": 2.645709672823971e-05,
      "loss": 0.6432,
      "step": 182900
    },
    {
      "epoch": 2.6050121030898477,
      "grad_norm": 0.7118901014328003,
      "learning_rate": 2.6409611092644475e-05,
      "loss": 0.6227,
      "step": 182950
    },
    {
      "epoch": 2.6057240495514735,
      "grad_norm": 0.5165082812309265,
      "learning_rate": 2.6362125457049242e-05,
      "loss": 0.5483,
      "step": 183000
    },
    {
      "epoch": 2.6064359960131,
      "grad_norm": 0.46783846616744995,
      "learning_rate": 2.6314639821454013e-05,
      "loss": 0.6388,
      "step": 183050
    },
    {
      "epoch": 2.607147942474726,
      "grad_norm": 0.8266456723213196,
      "learning_rate": 2.6267154185858777e-05,
      "loss": 0.579,
      "step": 183100
    },
    {
      "epoch": 2.607859888936352,
      "grad_norm": 0.4224484860897064,
      "learning_rate": 2.6219668550263548e-05,
      "loss": 0.647,
      "step": 183150
    },
    {
      "epoch": 2.608571835397978,
      "grad_norm": 0.4088779389858246,
      "learning_rate": 2.6172182914668315e-05,
      "loss": 0.6134,
      "step": 183200
    },
    {
      "epoch": 2.609283781859604,
      "grad_norm": 0.9261763095855713,
      "learning_rate": 2.612469727907308e-05,
      "loss": 0.6529,
      "step": 183250
    },
    {
      "epoch": 2.6099957283212305,
      "grad_norm": 0.46277469396591187,
      "learning_rate": 2.607721164347785e-05,
      "loss": 0.683,
      "step": 183300
    },
    {
      "epoch": 2.6107076747828564,
      "grad_norm": 0.8468387126922607,
      "learning_rate": 2.6029726007882614e-05,
      "loss": 0.6278,
      "step": 183350
    },
    {
      "epoch": 2.6114196212444822,
      "grad_norm": 0.6297939419746399,
      "learning_rate": 2.5982240372287385e-05,
      "loss": 0.6003,
      "step": 183400
    },
    {
      "epoch": 2.6121315677061085,
      "grad_norm": 1.166418433189392,
      "learning_rate": 2.5934754736692153e-05,
      "loss": 0.6219,
      "step": 183450
    },
    {
      "epoch": 2.612843514167735,
      "grad_norm": 0.7298154830932617,
      "learning_rate": 2.5887269101096917e-05,
      "loss": 0.6514,
      "step": 183500
    },
    {
      "epoch": 2.6135554606293607,
      "grad_norm": 0.7360721826553345,
      "learning_rate": 2.5839783465501687e-05,
      "loss": 0.6168,
      "step": 183550
    },
    {
      "epoch": 2.6142674070909866,
      "grad_norm": 0.9431326389312744,
      "learning_rate": 2.5792297829906455e-05,
      "loss": 0.5852,
      "step": 183600
    },
    {
      "epoch": 2.614979353552613,
      "grad_norm": 0.5162522792816162,
      "learning_rate": 2.5744812194311226e-05,
      "loss": 0.5983,
      "step": 183650
    },
    {
      "epoch": 2.6156913000142388,
      "grad_norm": 0.5575938820838928,
      "learning_rate": 2.569732655871599e-05,
      "loss": 0.6312,
      "step": 183700
    },
    {
      "epoch": 2.616403246475865,
      "grad_norm": 0.41242337226867676,
      "learning_rate": 2.5649840923120754e-05,
      "loss": 0.6235,
      "step": 183750
    },
    {
      "epoch": 2.617115192937491,
      "grad_norm": 0.7622303366661072,
      "learning_rate": 2.5602355287525525e-05,
      "loss": 0.5949,
      "step": 183800
    },
    {
      "epoch": 2.6178271393991173,
      "grad_norm": 0.51328444480896,
      "learning_rate": 2.5554869651930292e-05,
      "loss": 0.5967,
      "step": 183850
    },
    {
      "epoch": 2.618539085860743,
      "grad_norm": 0.48505499958992004,
      "learning_rate": 2.5507384016335063e-05,
      "loss": 0.6036,
      "step": 183900
    },
    {
      "epoch": 2.6192510323223694,
      "grad_norm": 0.5735896825790405,
      "learning_rate": 2.5459898380739827e-05,
      "loss": 0.6246,
      "step": 183950
    },
    {
      "epoch": 2.6199629787839953,
      "grad_norm": 0.5206006169319153,
      "learning_rate": 2.541241274514459e-05,
      "loss": 0.5881,
      "step": 184000
    },
    {
      "epoch": 2.6206749252456216,
      "grad_norm": 0.5534780025482178,
      "learning_rate": 2.5364927109549362e-05,
      "loss": 0.6863,
      "step": 184050
    },
    {
      "epoch": 2.6213868717072475,
      "grad_norm": 0.5514718294143677,
      "learning_rate": 2.531744147395413e-05,
      "loss": 0.6001,
      "step": 184100
    },
    {
      "epoch": 2.622098818168874,
      "grad_norm": 0.9414767026901245,
      "learning_rate": 2.52699558383589e-05,
      "loss": 0.6148,
      "step": 184150
    },
    {
      "epoch": 2.6228107646304997,
      "grad_norm": 0.4599176347255707,
      "learning_rate": 2.5222470202763664e-05,
      "loss": 0.6342,
      "step": 184200
    },
    {
      "epoch": 2.623522711092126,
      "grad_norm": 0.3879556953907013,
      "learning_rate": 2.5175934279880337e-05,
      "loss": 0.6097,
      "step": 184250
    },
    {
      "epoch": 2.624234657553752,
      "grad_norm": 0.5242322683334351,
      "learning_rate": 2.5128448644285108e-05,
      "loss": 0.6304,
      "step": 184300
    },
    {
      "epoch": 2.624946604015378,
      "grad_norm": 0.9924328327178955,
      "learning_rate": 2.508096300868987e-05,
      "loss": 0.6687,
      "step": 184350
    },
    {
      "epoch": 2.625658550477004,
      "grad_norm": 0.41130390763282776,
      "learning_rate": 2.503347737309464e-05,
      "loss": 0.6551,
      "step": 184400
    },
    {
      "epoch": 2.6263704969386303,
      "grad_norm": 0.567279040813446,
      "learning_rate": 2.498694145021131e-05,
      "loss": 0.6598,
      "step": 184450
    },
    {
      "epoch": 2.627082443400256,
      "grad_norm": 0.3249640166759491,
      "learning_rate": 2.493945581461608e-05,
      "loss": 0.5549,
      "step": 184500
    },
    {
      "epoch": 2.6277943898618825,
      "grad_norm": 0.32609719038009644,
      "learning_rate": 2.489197017902085e-05,
      "loss": 0.6421,
      "step": 184550
    },
    {
      "epoch": 2.6285063363235084,
      "grad_norm": 0.6542086601257324,
      "learning_rate": 2.4844484543425614e-05,
      "loss": 0.6626,
      "step": 184600
    },
    {
      "epoch": 2.6292182827851347,
      "grad_norm": 1.0252313613891602,
      "learning_rate": 2.479699890783038e-05,
      "loss": 0.6259,
      "step": 184650
    },
    {
      "epoch": 2.6299302292467606,
      "grad_norm": 0.4616096019744873,
      "learning_rate": 2.474951327223515e-05,
      "loss": 0.5776,
      "step": 184700
    },
    {
      "epoch": 2.630642175708387,
      "grad_norm": 0.7453413605690002,
      "learning_rate": 2.470202763663992e-05,
      "loss": 0.6096,
      "step": 184750
    },
    {
      "epoch": 2.6313541221700127,
      "grad_norm": 0.5934725403785706,
      "learning_rate": 2.4654542001044687e-05,
      "loss": 0.606,
      "step": 184800
    },
    {
      "epoch": 2.632066068631639,
      "grad_norm": 0.47139546275138855,
      "learning_rate": 2.460705636544945e-05,
      "loss": 0.5819,
      "step": 184850
    },
    {
      "epoch": 2.632778015093265,
      "grad_norm": 0.49067994952201843,
      "learning_rate": 2.455957072985422e-05,
      "loss": 0.6335,
      "step": 184900
    },
    {
      "epoch": 2.6334899615548912,
      "grad_norm": 0.39187464118003845,
      "learning_rate": 2.451208509425899e-05,
      "loss": 0.6047,
      "step": 184950
    },
    {
      "epoch": 2.634201908016517,
      "grad_norm": 0.3481791615486145,
      "learning_rate": 2.4464599458663757e-05,
      "loss": 0.6867,
      "step": 185000
    },
    {
      "epoch": 2.6349138544781434,
      "grad_norm": 0.5369227528572083,
      "learning_rate": 2.4417113823068524e-05,
      "loss": 0.618,
      "step": 185050
    },
    {
      "epoch": 2.6356258009397693,
      "grad_norm": 0.4921988248825073,
      "learning_rate": 2.436962818747329e-05,
      "loss": 0.6127,
      "step": 185100
    },
    {
      "epoch": 2.636337747401395,
      "grad_norm": 0.6285223960876465,
      "learning_rate": 2.432214255187806e-05,
      "loss": 0.6417,
      "step": 185150
    },
    {
      "epoch": 2.6370496938630215,
      "grad_norm": 0.45941582322120667,
      "learning_rate": 2.4274656916282827e-05,
      "loss": 0.6244,
      "step": 185200
    },
    {
      "epoch": 2.6377616403246478,
      "grad_norm": 0.6825294494628906,
      "learning_rate": 2.4227171280687594e-05,
      "loss": 0.6389,
      "step": 185250
    },
    {
      "epoch": 2.6384735867862736,
      "grad_norm": 0.6328167915344238,
      "learning_rate": 2.417968564509236e-05,
      "loss": 0.6196,
      "step": 185300
    },
    {
      "epoch": 2.6391855332478995,
      "grad_norm": 0.6520638465881348,
      "learning_rate": 2.413220000949713e-05,
      "loss": 0.6352,
      "step": 185350
    },
    {
      "epoch": 2.639897479709526,
      "grad_norm": 0.6596881747245789,
      "learning_rate": 2.4084714373901896e-05,
      "loss": 0.6303,
      "step": 185400
    },
    {
      "epoch": 2.640609426171152,
      "grad_norm": 0.6590718030929565,
      "learning_rate": 2.4037228738306664e-05,
      "loss": 0.6684,
      "step": 185450
    },
    {
      "epoch": 2.641321372632778,
      "grad_norm": 0.43578600883483887,
      "learning_rate": 2.398974310271143e-05,
      "loss": 0.6159,
      "step": 185500
    },
    {
      "epoch": 2.642033319094404,
      "grad_norm": 0.38008445501327515,
      "learning_rate": 2.39422574671162e-05,
      "loss": 0.6754,
      "step": 185550
    },
    {
      "epoch": 2.64274526555603,
      "grad_norm": 0.6763860583305359,
      "learning_rate": 2.3894771831520966e-05,
      "loss": 0.6255,
      "step": 185600
    },
    {
      "epoch": 2.6434572120176565,
      "grad_norm": 0.6591178774833679,
      "learning_rate": 2.3847286195925734e-05,
      "loss": 0.557,
      "step": 185650
    },
    {
      "epoch": 2.6441691584792824,
      "grad_norm": 0.798931896686554,
      "learning_rate": 2.37998005603305e-05,
      "loss": 0.6631,
      "step": 185700
    },
    {
      "epoch": 2.6448811049409082,
      "grad_norm": 0.7077682018280029,
      "learning_rate": 2.375231492473527e-05,
      "loss": 0.6393,
      "step": 185750
    },
    {
      "epoch": 2.6455930514025345,
      "grad_norm": 0.35469865798950195,
      "learning_rate": 2.3704829289140036e-05,
      "loss": 0.6422,
      "step": 185800
    },
    {
      "epoch": 2.646304997864161,
      "grad_norm": 0.5022165179252625,
      "learning_rate": 2.3657343653544803e-05,
      "loss": 0.6522,
      "step": 185850
    },
    {
      "epoch": 2.6470169443257867,
      "grad_norm": 0.5159925818443298,
      "learning_rate": 2.360985801794957e-05,
      "loss": 0.6208,
      "step": 185900
    },
    {
      "epoch": 2.6477288907874126,
      "grad_norm": 0.5175469517707825,
      "learning_rate": 2.356237238235434e-05,
      "loss": 0.6308,
      "step": 185950
    },
    {
      "epoch": 2.648440837249039,
      "grad_norm": 0.7159419655799866,
      "learning_rate": 2.3514886746759106e-05,
      "loss": 0.6534,
      "step": 186000
    },
    {
      "epoch": 2.649152783710665,
      "grad_norm": 0.511883020401001,
      "learning_rate": 2.3467401111163877e-05,
      "loss": 0.6219,
      "step": 186050
    },
    {
      "epoch": 2.649864730172291,
      "grad_norm": 0.7640464901924133,
      "learning_rate": 2.341991547556864e-05,
      "loss": 0.6557,
      "step": 186100
    },
    {
      "epoch": 2.650576676633917,
      "grad_norm": 0.38526004552841187,
      "learning_rate": 2.3372429839973408e-05,
      "loss": 0.6393,
      "step": 186150
    },
    {
      "epoch": 2.6512886230955433,
      "grad_norm": 0.7569889426231384,
      "learning_rate": 2.3324944204378176e-05,
      "loss": 0.6617,
      "step": 186200
    },
    {
      "epoch": 2.6520005695571696,
      "grad_norm": 0.906502902507782,
      "learning_rate": 2.3277458568782946e-05,
      "loss": 0.6203,
      "step": 186250
    },
    {
      "epoch": 2.6527125160187954,
      "grad_norm": 0.4352591335773468,
      "learning_rate": 2.3229972933187714e-05,
      "loss": 0.5618,
      "step": 186300
    },
    {
      "epoch": 2.6534244624804213,
      "grad_norm": 0.519575834274292,
      "learning_rate": 2.3182487297592478e-05,
      "loss": 0.6172,
      "step": 186350
    },
    {
      "epoch": 2.6541364089420476,
      "grad_norm": 0.825657308101654,
      "learning_rate": 2.3135001661997245e-05,
      "loss": 0.6444,
      "step": 186400
    },
    {
      "epoch": 2.654848355403674,
      "grad_norm": 0.510104775428772,
      "learning_rate": 2.3087516026402016e-05,
      "loss": 0.5939,
      "step": 186450
    },
    {
      "epoch": 2.6555603018653,
      "grad_norm": 0.3008088171482086,
      "learning_rate": 2.3040030390806784e-05,
      "loss": 0.5892,
      "step": 186500
    },
    {
      "epoch": 2.6562722483269257,
      "grad_norm": 0.3538961410522461,
      "learning_rate": 2.299254475521155e-05,
      "loss": 0.5996,
      "step": 186550
    },
    {
      "epoch": 2.656984194788552,
      "grad_norm": 0.4891030788421631,
      "learning_rate": 2.2945059119616315e-05,
      "loss": 0.6607,
      "step": 186600
    },
    {
      "epoch": 2.657696141250178,
      "grad_norm": 0.9055416584014893,
      "learning_rate": 2.2897573484021086e-05,
      "loss": 0.6683,
      "step": 186650
    },
    {
      "epoch": 2.658408087711804,
      "grad_norm": 0.6309531927108765,
      "learning_rate": 2.2850087848425853e-05,
      "loss": 0.6225,
      "step": 186700
    },
    {
      "epoch": 2.65912003417343,
      "grad_norm": 1.1248207092285156,
      "learning_rate": 2.280260221283062e-05,
      "loss": 0.6461,
      "step": 186750
    },
    {
      "epoch": 2.6598319806350563,
      "grad_norm": 0.8485748171806335,
      "learning_rate": 2.2755116577235388e-05,
      "loss": 0.6004,
      "step": 186800
    },
    {
      "epoch": 2.660543927096682,
      "grad_norm": 0.8010073900222778,
      "learning_rate": 2.2707630941640152e-05,
      "loss": 0.6448,
      "step": 186850
    },
    {
      "epoch": 2.6612558735583085,
      "grad_norm": 0.7951970100402832,
      "learning_rate": 2.2660145306044923e-05,
      "loss": 0.6129,
      "step": 186900
    },
    {
      "epoch": 2.6619678200199344,
      "grad_norm": 0.40633663535118103,
      "learning_rate": 2.261265967044969e-05,
      "loss": 0.6105,
      "step": 186950
    },
    {
      "epoch": 2.6626797664815607,
      "grad_norm": 0.499585896730423,
      "learning_rate": 2.2565174034854458e-05,
      "loss": 0.6419,
      "step": 187000
    },
    {
      "epoch": 2.6633917129431866,
      "grad_norm": 1.0355993509292603,
      "learning_rate": 2.2517688399259225e-05,
      "loss": 0.6709,
      "step": 187050
    },
    {
      "epoch": 2.664103659404813,
      "grad_norm": 0.4470544755458832,
      "learning_rate": 2.2470202763663993e-05,
      "loss": 0.5963,
      "step": 187100
    },
    {
      "epoch": 2.6648156058664387,
      "grad_norm": 0.6620635986328125,
      "learning_rate": 2.242271712806876e-05,
      "loss": 0.6799,
      "step": 187150
    },
    {
      "epoch": 2.665527552328065,
      "grad_norm": 0.5238373875617981,
      "learning_rate": 2.2375231492473528e-05,
      "loss": 0.5778,
      "step": 187200
    },
    {
      "epoch": 2.666239498789691,
      "grad_norm": 0.5692763328552246,
      "learning_rate": 2.2327745856878295e-05,
      "loss": 0.6138,
      "step": 187250
    },
    {
      "epoch": 2.666951445251317,
      "grad_norm": 0.8049208521842957,
      "learning_rate": 2.2280260221283063e-05,
      "loss": 0.6114,
      "step": 187300
    },
    {
      "epoch": 2.667663391712943,
      "grad_norm": 0.5484206676483154,
      "learning_rate": 2.223277458568783e-05,
      "loss": 0.6196,
      "step": 187350
    },
    {
      "epoch": 2.6683753381745694,
      "grad_norm": 0.4841022193431854,
      "learning_rate": 2.2185288950092598e-05,
      "loss": 0.6124,
      "step": 187400
    },
    {
      "epoch": 2.6690872846361953,
      "grad_norm": 0.5702551603317261,
      "learning_rate": 2.2137803314497365e-05,
      "loss": 0.61,
      "step": 187450
    },
    {
      "epoch": 2.6697992310978216,
      "grad_norm": 0.47546401619911194,
      "learning_rate": 2.2090317678902132e-05,
      "loss": 0.6618,
      "step": 187500
    },
    {
      "epoch": 2.6705111775594474,
      "grad_norm": 0.5452341437339783,
      "learning_rate": 2.2042832043306903e-05,
      "loss": 0.658,
      "step": 187550
    },
    {
      "epoch": 2.6712231240210738,
      "grad_norm": 0.8270982503890991,
      "learning_rate": 2.1995346407711667e-05,
      "loss": 0.6205,
      "step": 187600
    },
    {
      "epoch": 2.6719350704826996,
      "grad_norm": 0.4452865719795227,
      "learning_rate": 2.1947860772116435e-05,
      "loss": 0.6211,
      "step": 187650
    },
    {
      "epoch": 2.672647016944326,
      "grad_norm": 0.614643394947052,
      "learning_rate": 2.1900375136521202e-05,
      "loss": 0.6358,
      "step": 187700
    },
    {
      "epoch": 2.673358963405952,
      "grad_norm": 0.3540366590023041,
      "learning_rate": 2.1852889500925973e-05,
      "loss": 0.5968,
      "step": 187750
    },
    {
      "epoch": 2.674070909867578,
      "grad_norm": 0.7720978260040283,
      "learning_rate": 2.180540386533074e-05,
      "loss": 0.5888,
      "step": 187800
    },
    {
      "epoch": 2.674782856329204,
      "grad_norm": 0.6901649236679077,
      "learning_rate": 2.1757918229735505e-05,
      "loss": 0.6728,
      "step": 187850
    },
    {
      "epoch": 2.6754948027908303,
      "grad_norm": 0.6314734220504761,
      "learning_rate": 2.171138230685218e-05,
      "loss": 0.6435,
      "step": 187900
    },
    {
      "epoch": 2.676206749252456,
      "grad_norm": 0.46359720826148987,
      "learning_rate": 2.1663896671256945e-05,
      "loss": 0.6059,
      "step": 187950
    },
    {
      "epoch": 2.6769186957140825,
      "grad_norm": 0.5726799368858337,
      "learning_rate": 2.1616411035661712e-05,
      "loss": 0.5806,
      "step": 188000
    },
    {
      "epoch": 2.6776306421757083,
      "grad_norm": 0.6277689933776855,
      "learning_rate": 2.156892540006648e-05,
      "loss": 0.6335,
      "step": 188050
    },
    {
      "epoch": 2.678342588637334,
      "grad_norm": 0.34240156412124634,
      "learning_rate": 2.152143976447125e-05,
      "loss": 0.597,
      "step": 188100
    },
    {
      "epoch": 2.6790545350989605,
      "grad_norm": 0.6970991492271423,
      "learning_rate": 2.1473954128876018e-05,
      "loss": 0.6302,
      "step": 188150
    },
    {
      "epoch": 2.679766481560587,
      "grad_norm": 0.684295117855072,
      "learning_rate": 2.1426468493280782e-05,
      "loss": 0.6356,
      "step": 188200
    },
    {
      "epoch": 2.6804784280222127,
      "grad_norm": 0.6634743213653564,
      "learning_rate": 2.137898285768555e-05,
      "loss": 0.5652,
      "step": 188250
    },
    {
      "epoch": 2.6811903744838386,
      "grad_norm": 0.48472341895103455,
      "learning_rate": 2.133149722209032e-05,
      "loss": 0.6663,
      "step": 188300
    },
    {
      "epoch": 2.681902320945465,
      "grad_norm": 0.6730612516403198,
      "learning_rate": 2.1284011586495088e-05,
      "loss": 0.635,
      "step": 188350
    },
    {
      "epoch": 2.682614267407091,
      "grad_norm": 0.2744407057762146,
      "learning_rate": 2.1236525950899855e-05,
      "loss": 0.6386,
      "step": 188400
    },
    {
      "epoch": 2.683326213868717,
      "grad_norm": 0.6890406012535095,
      "learning_rate": 2.118904031530462e-05,
      "loss": 0.5919,
      "step": 188450
    },
    {
      "epoch": 2.684038160330343,
      "grad_norm": 0.5952099561691284,
      "learning_rate": 2.114155467970939e-05,
      "loss": 0.6621,
      "step": 188500
    },
    {
      "epoch": 2.6847501067919692,
      "grad_norm": 0.5507481098175049,
      "learning_rate": 2.1094069044114157e-05,
      "loss": 0.6123,
      "step": 188550
    },
    {
      "epoch": 2.6854620532535955,
      "grad_norm": 0.6822786331176758,
      "learning_rate": 2.1046583408518925e-05,
      "loss": 0.6334,
      "step": 188600
    },
    {
      "epoch": 2.6861739997152214,
      "grad_norm": 0.7592896819114685,
      "learning_rate": 2.0999097772923692e-05,
      "loss": 0.6386,
      "step": 188650
    },
    {
      "epoch": 2.6868859461768473,
      "grad_norm": 0.4789387285709381,
      "learning_rate": 2.095161213732846e-05,
      "loss": 0.6561,
      "step": 188700
    },
    {
      "epoch": 2.6875978926384736,
      "grad_norm": 0.5381683707237244,
      "learning_rate": 2.0904126501733227e-05,
      "loss": 0.6246,
      "step": 188750
    },
    {
      "epoch": 2.6883098391001,
      "grad_norm": 0.5132061839103699,
      "learning_rate": 2.0856640866137995e-05,
      "loss": 0.6327,
      "step": 188800
    },
    {
      "epoch": 2.6890217855617258,
      "grad_norm": 0.4771726131439209,
      "learning_rate": 2.0809155230542762e-05,
      "loss": 0.608,
      "step": 188850
    },
    {
      "epoch": 2.6897337320233516,
      "grad_norm": 0.3650323152542114,
      "learning_rate": 2.076166959494753e-05,
      "loss": 0.6217,
      "step": 188900
    },
    {
      "epoch": 2.690445678484978,
      "grad_norm": 0.5753374099731445,
      "learning_rate": 2.0714183959352297e-05,
      "loss": 0.6012,
      "step": 188950
    },
    {
      "epoch": 2.6911576249466043,
      "grad_norm": 0.547548770904541,
      "learning_rate": 2.0666698323757064e-05,
      "loss": 0.6397,
      "step": 189000
    },
    {
      "epoch": 2.69186957140823,
      "grad_norm": 0.8193452954292297,
      "learning_rate": 2.0619212688161832e-05,
      "loss": 0.6159,
      "step": 189050
    },
    {
      "epoch": 2.692581517869856,
      "grad_norm": 0.7481796741485596,
      "learning_rate": 2.05717270525666e-05,
      "loss": 0.5979,
      "step": 189100
    },
    {
      "epoch": 2.6932934643314823,
      "grad_norm": 0.8414033055305481,
      "learning_rate": 2.0524241416971367e-05,
      "loss": 0.6635,
      "step": 189150
    },
    {
      "epoch": 2.6940054107931086,
      "grad_norm": 0.5551251173019409,
      "learning_rate": 2.0476755781376134e-05,
      "loss": 0.6507,
      "step": 189200
    },
    {
      "epoch": 2.6947173572547345,
      "grad_norm": 0.546055793762207,
      "learning_rate": 2.04292701457809e-05,
      "loss": 0.6184,
      "step": 189250
    },
    {
      "epoch": 2.6954293037163604,
      "grad_norm": 0.6456553339958191,
      "learning_rate": 2.038178451018567e-05,
      "loss": 0.6448,
      "step": 189300
    },
    {
      "epoch": 2.6961412501779867,
      "grad_norm": 0.7693870663642883,
      "learning_rate": 2.0334298874590436e-05,
      "loss": 0.6413,
      "step": 189350
    },
    {
      "epoch": 2.6968531966396125,
      "grad_norm": 0.9204987287521362,
      "learning_rate": 2.0286813238995207e-05,
      "loss": 0.6297,
      "step": 189400
    },
    {
      "epoch": 2.697565143101239,
      "grad_norm": 0.9455733895301819,
      "learning_rate": 2.023932760339997e-05,
      "loss": 0.6185,
      "step": 189450
    },
    {
      "epoch": 2.6982770895628647,
      "grad_norm": 0.6396558880805969,
      "learning_rate": 2.019184196780474e-05,
      "loss": 0.6323,
      "step": 189500
    },
    {
      "epoch": 2.698989036024491,
      "grad_norm": 0.7808989882469177,
      "learning_rate": 2.0144356332209506e-05,
      "loss": 0.5533,
      "step": 189550
    },
    {
      "epoch": 2.699700982486117,
      "grad_norm": 0.41978952288627625,
      "learning_rate": 2.0096870696614277e-05,
      "loss": 0.6869,
      "step": 189600
    },
    {
      "epoch": 2.700412928947743,
      "grad_norm": 0.8229519724845886,
      "learning_rate": 2.0049385061019044e-05,
      "loss": 0.5884,
      "step": 189650
    },
    {
      "epoch": 2.701124875409369,
      "grad_norm": 0.7186939716339111,
      "learning_rate": 2.000189942542381e-05,
      "loss": 0.5947,
      "step": 189700
    },
    {
      "epoch": 2.7018368218709954,
      "grad_norm": 0.49135634303092957,
      "learning_rate": 1.9954413789828576e-05,
      "loss": 0.6293,
      "step": 189750
    },
    {
      "epoch": 2.7025487683326213,
      "grad_norm": 1.0226342678070068,
      "learning_rate": 1.9906928154233347e-05,
      "loss": 0.6489,
      "step": 189800
    },
    {
      "epoch": 2.7032607147942476,
      "grad_norm": 0.8678361177444458,
      "learning_rate": 1.9859442518638114e-05,
      "loss": 0.6026,
      "step": 189850
    },
    {
      "epoch": 2.7039726612558734,
      "grad_norm": 0.7078526616096497,
      "learning_rate": 1.981195688304288e-05,
      "loss": 0.6363,
      "step": 189900
    },
    {
      "epoch": 2.7046846077174997,
      "grad_norm": 0.5654433369636536,
      "learning_rate": 1.9764471247447646e-05,
      "loss": 0.6401,
      "step": 189950
    },
    {
      "epoch": 2.7053965541791256,
      "grad_norm": 0.5470847487449646,
      "learning_rate": 1.9716985611852417e-05,
      "loss": 0.6547,
      "step": 190000
    },
    {
      "epoch": 2.706108500640752,
      "grad_norm": 0.735388457775116,
      "learning_rate": 1.9669499976257184e-05,
      "loss": 0.6252,
      "step": 190050
    },
    {
      "epoch": 2.706820447102378,
      "grad_norm": 0.8222448825836182,
      "learning_rate": 1.962201434066195e-05,
      "loss": 0.6081,
      "step": 190100
    },
    {
      "epoch": 2.707532393564004,
      "grad_norm": 0.7849565744400024,
      "learning_rate": 1.957452870506672e-05,
      "loss": 0.6221,
      "step": 190150
    },
    {
      "epoch": 2.70824434002563,
      "grad_norm": 0.684196949005127,
      "learning_rate": 1.9527043069471483e-05,
      "loss": 0.6536,
      "step": 190200
    },
    {
      "epoch": 2.7089562864872563,
      "grad_norm": 0.5081435441970825,
      "learning_rate": 1.9479557433876254e-05,
      "loss": 0.6024,
      "step": 190250
    },
    {
      "epoch": 2.709668232948882,
      "grad_norm": 0.6944224238395691,
      "learning_rate": 1.943207179828102e-05,
      "loss": 0.6177,
      "step": 190300
    },
    {
      "epoch": 2.7103801794105085,
      "grad_norm": 0.4477616250514984,
      "learning_rate": 1.938458616268579e-05,
      "loss": 0.6096,
      "step": 190350
    },
    {
      "epoch": 2.7110921258721343,
      "grad_norm": 0.4806790053844452,
      "learning_rate": 1.9337100527090556e-05,
      "loss": 0.662,
      "step": 190400
    },
    {
      "epoch": 2.7118040723337606,
      "grad_norm": 0.6442699432373047,
      "learning_rate": 1.9289614891495324e-05,
      "loss": 0.656,
      "step": 190450
    },
    {
      "epoch": 2.7125160187953865,
      "grad_norm": 0.4960704445838928,
      "learning_rate": 1.924212925590009e-05,
      "loss": 0.641,
      "step": 190500
    },
    {
      "epoch": 2.713227965257013,
      "grad_norm": 0.7726340889930725,
      "learning_rate": 1.919464362030486e-05,
      "loss": 0.6538,
      "step": 190550
    },
    {
      "epoch": 2.7139399117186387,
      "grad_norm": 0.4888817071914673,
      "learning_rate": 1.9147157984709626e-05,
      "loss": 0.7045,
      "step": 190600
    },
    {
      "epoch": 2.714651858180265,
      "grad_norm": 0.6600555777549744,
      "learning_rate": 1.9099672349114393e-05,
      "loss": 0.6634,
      "step": 190650
    },
    {
      "epoch": 2.715363804641891,
      "grad_norm": 0.7606469392776489,
      "learning_rate": 1.905218671351916e-05,
      "loss": 0.6222,
      "step": 190700
    },
    {
      "epoch": 2.716075751103517,
      "grad_norm": 0.8660136461257935,
      "learning_rate": 1.9004701077923928e-05,
      "loss": 0.6179,
      "step": 190750
    },
    {
      "epoch": 2.716787697565143,
      "grad_norm": 0.7090425491333008,
      "learning_rate": 1.8957215442328696e-05,
      "loss": 0.6326,
      "step": 190800
    },
    {
      "epoch": 2.717499644026769,
      "grad_norm": 0.4715679883956909,
      "learning_rate": 1.8909729806733463e-05,
      "loss": 0.6652,
      "step": 190850
    },
    {
      "epoch": 2.7182115904883952,
      "grad_norm": 0.7090082764625549,
      "learning_rate": 1.8862244171138234e-05,
      "loss": 0.6349,
      "step": 190900
    },
    {
      "epoch": 2.7189235369500215,
      "grad_norm": 0.5385703444480896,
      "learning_rate": 1.8814758535542998e-05,
      "loss": 0.6222,
      "step": 190950
    },
    {
      "epoch": 2.7196354834116474,
      "grad_norm": 0.6828751564025879,
      "learning_rate": 1.8767272899947765e-05,
      "loss": 0.6353,
      "step": 191000
    },
    {
      "epoch": 2.7203474298732733,
      "grad_norm": 0.49716582894325256,
      "learning_rate": 1.8719787264352533e-05,
      "loss": 0.6567,
      "step": 191050
    },
    {
      "epoch": 2.7210593763348996,
      "grad_norm": 0.6219061017036438,
      "learning_rate": 1.8672301628757304e-05,
      "loss": 0.6599,
      "step": 191100
    },
    {
      "epoch": 2.721771322796526,
      "grad_norm": 0.8006978034973145,
      "learning_rate": 1.862481599316207e-05,
      "loss": 0.6394,
      "step": 191150
    },
    {
      "epoch": 2.7224832692581518,
      "grad_norm": 0.6172988414764404,
      "learning_rate": 1.8577330357566835e-05,
      "loss": 0.6165,
      "step": 191200
    },
    {
      "epoch": 2.7231952157197776,
      "grad_norm": 0.7169840931892395,
      "learning_rate": 1.8529844721971603e-05,
      "loss": 0.6405,
      "step": 191250
    },
    {
      "epoch": 2.723907162181404,
      "grad_norm": 0.33631396293640137,
      "learning_rate": 1.8482359086376373e-05,
      "loss": 0.6303,
      "step": 191300
    },
    {
      "epoch": 2.7246191086430303,
      "grad_norm": 0.46271851658821106,
      "learning_rate": 1.843487345078114e-05,
      "loss": 0.6741,
      "step": 191350
    },
    {
      "epoch": 2.725331055104656,
      "grad_norm": 0.3326462209224701,
      "learning_rate": 1.838738781518591e-05,
      "loss": 0.6076,
      "step": 191400
    },
    {
      "epoch": 2.726043001566282,
      "grad_norm": 0.6553472876548767,
      "learning_rate": 1.8339902179590672e-05,
      "loss": 0.5809,
      "step": 191450
    },
    {
      "epoch": 2.7267549480279083,
      "grad_norm": 0.7435415983200073,
      "learning_rate": 1.829241654399544e-05,
      "loss": 0.6162,
      "step": 191500
    },
    {
      "epoch": 2.7274668944895346,
      "grad_norm": 0.6224614977836609,
      "learning_rate": 1.824493090840021e-05,
      "loss": 0.6234,
      "step": 191550
    },
    {
      "epoch": 2.7281788409511605,
      "grad_norm": 0.8127505779266357,
      "learning_rate": 1.8197445272804978e-05,
      "loss": 0.6444,
      "step": 191600
    },
    {
      "epoch": 2.7288907874127863,
      "grad_norm": 0.5416666269302368,
      "learning_rate": 1.8149959637209746e-05,
      "loss": 0.6174,
      "step": 191650
    },
    {
      "epoch": 2.7296027338744127,
      "grad_norm": 0.82188880443573,
      "learning_rate": 1.810247400161451e-05,
      "loss": 0.6449,
      "step": 191700
    },
    {
      "epoch": 2.730314680336039,
      "grad_norm": 0.7317487001419067,
      "learning_rate": 1.805498836601928e-05,
      "loss": 0.6115,
      "step": 191750
    },
    {
      "epoch": 2.731026626797665,
      "grad_norm": 0.6070451140403748,
      "learning_rate": 1.8007502730424048e-05,
      "loss": 0.6452,
      "step": 191800
    },
    {
      "epoch": 2.7317385732592907,
      "grad_norm": 0.3402576744556427,
      "learning_rate": 1.7960017094828815e-05,
      "loss": 0.6237,
      "step": 191850
    },
    {
      "epoch": 2.732450519720917,
      "grad_norm": 0.5564374327659607,
      "learning_rate": 1.7912531459233583e-05,
      "loss": 0.5897,
      "step": 191900
    },
    {
      "epoch": 2.7331624661825433,
      "grad_norm": 0.6841512322425842,
      "learning_rate": 1.786504582363835e-05,
      "loss": 0.6277,
      "step": 191950
    },
    {
      "epoch": 2.733874412644169,
      "grad_norm": 0.4276615381240845,
      "learning_rate": 1.7817560188043118e-05,
      "loss": 0.5973,
      "step": 192000
    },
    {
      "epoch": 2.734586359105795,
      "grad_norm": 0.43230536580085754,
      "learning_rate": 1.7770074552447885e-05,
      "loss": 0.6576,
      "step": 192050
    },
    {
      "epoch": 2.7352983055674214,
      "grad_norm": 0.5296700596809387,
      "learning_rate": 1.7722588916852653e-05,
      "loss": 0.6413,
      "step": 192100
    },
    {
      "epoch": 2.7360102520290477,
      "grad_norm": 0.6555434465408325,
      "learning_rate": 1.7676052993969325e-05,
      "loss": 0.627,
      "step": 192150
    },
    {
      "epoch": 2.7367221984906736,
      "grad_norm": 0.34122544527053833,
      "learning_rate": 1.7628567358374093e-05,
      "loss": 0.596,
      "step": 192200
    },
    {
      "epoch": 2.7374341449522994,
      "grad_norm": 0.5200930237770081,
      "learning_rate": 1.758108172277886e-05,
      "loss": 0.6122,
      "step": 192250
    },
    {
      "epoch": 2.7381460914139257,
      "grad_norm": 0.6307513117790222,
      "learning_rate": 1.7533596087183627e-05,
      "loss": 0.5838,
      "step": 192300
    },
    {
      "epoch": 2.7388580378755516,
      "grad_norm": 1.115981101989746,
      "learning_rate": 1.7486110451588395e-05,
      "loss": 0.6613,
      "step": 192350
    },
    {
      "epoch": 2.739569984337178,
      "grad_norm": 0.46229928731918335,
      "learning_rate": 1.7438624815993162e-05,
      "loss": 0.5702,
      "step": 192400
    },
    {
      "epoch": 2.7402819307988038,
      "grad_norm": 0.6884341239929199,
      "learning_rate": 1.739113918039793e-05,
      "loss": 0.6742,
      "step": 192450
    },
    {
      "epoch": 2.74099387726043,
      "grad_norm": 1.135391354560852,
      "learning_rate": 1.7343653544802697e-05,
      "loss": 0.6337,
      "step": 192500
    },
    {
      "epoch": 2.741705823722056,
      "grad_norm": 0.44638311862945557,
      "learning_rate": 1.7296167909207468e-05,
      "loss": 0.6216,
      "step": 192550
    },
    {
      "epoch": 2.7424177701836823,
      "grad_norm": 0.44604289531707764,
      "learning_rate": 1.7248682273612232e-05,
      "loss": 0.601,
      "step": 192600
    },
    {
      "epoch": 2.743129716645308,
      "grad_norm": 0.7500429153442383,
      "learning_rate": 1.7201196638017e-05,
      "loss": 0.6255,
      "step": 192650
    },
    {
      "epoch": 2.7438416631069344,
      "grad_norm": 0.4882408678531647,
      "learning_rate": 1.7153711002421767e-05,
      "loss": 0.6832,
      "step": 192700
    },
    {
      "epoch": 2.7445536095685603,
      "grad_norm": 0.6223248839378357,
      "learning_rate": 1.7106225366826538e-05,
      "loss": 0.6187,
      "step": 192750
    },
    {
      "epoch": 2.7452655560301866,
      "grad_norm": 0.39337387681007385,
      "learning_rate": 1.7058739731231305e-05,
      "loss": 0.6536,
      "step": 192800
    },
    {
      "epoch": 2.7459775024918125,
      "grad_norm": 0.5602307319641113,
      "learning_rate": 1.701125409563607e-05,
      "loss": 0.6441,
      "step": 192850
    },
    {
      "epoch": 2.746689448953439,
      "grad_norm": 0.4429384768009186,
      "learning_rate": 1.6963768460040837e-05,
      "loss": 0.6114,
      "step": 192900
    },
    {
      "epoch": 2.7474013954150647,
      "grad_norm": 0.39112603664398193,
      "learning_rate": 1.6916282824445608e-05,
      "loss": 0.5637,
      "step": 192950
    },
    {
      "epoch": 2.748113341876691,
      "grad_norm": 0.6367714405059814,
      "learning_rate": 1.6868797188850375e-05,
      "loss": 0.6216,
      "step": 193000
    },
    {
      "epoch": 2.748825288338317,
      "grad_norm": 0.7909930348396301,
      "learning_rate": 1.6821311553255142e-05,
      "loss": 0.5764,
      "step": 193050
    },
    {
      "epoch": 2.749537234799943,
      "grad_norm": 0.4828977584838867,
      "learning_rate": 1.6773825917659907e-05,
      "loss": 0.6326,
      "step": 193100
    },
    {
      "epoch": 2.750249181261569,
      "grad_norm": 0.32441502809524536,
      "learning_rate": 1.6726340282064677e-05,
      "loss": 0.6438,
      "step": 193150
    },
    {
      "epoch": 2.7509611277231953,
      "grad_norm": 0.6506915092468262,
      "learning_rate": 1.6678854646469445e-05,
      "loss": 0.6244,
      "step": 193200
    },
    {
      "epoch": 2.751673074184821,
      "grad_norm": 0.4683450758457184,
      "learning_rate": 1.6631369010874212e-05,
      "loss": 0.6495,
      "step": 193250
    },
    {
      "epoch": 2.7523850206464475,
      "grad_norm": 0.5062587857246399,
      "learning_rate": 1.658388337527898e-05,
      "loss": 0.5907,
      "step": 193300
    },
    {
      "epoch": 2.7530969671080734,
      "grad_norm": 0.542054295539856,
      "learning_rate": 1.6536397739683747e-05,
      "loss": 0.5868,
      "step": 193350
    },
    {
      "epoch": 2.7538089135696997,
      "grad_norm": 0.3639809191226959,
      "learning_rate": 1.6488912104088515e-05,
      "loss": 0.6291,
      "step": 193400
    },
    {
      "epoch": 2.7545208600313256,
      "grad_norm": 0.7123187780380249,
      "learning_rate": 1.6441426468493282e-05,
      "loss": 0.6069,
      "step": 193450
    },
    {
      "epoch": 2.755232806492952,
      "grad_norm": 0.7084641456604004,
      "learning_rate": 1.639394083289805e-05,
      "loss": 0.6516,
      "step": 193500
    },
    {
      "epoch": 2.7559447529545777,
      "grad_norm": 0.4495229125022888,
      "learning_rate": 1.6346455197302817e-05,
      "loss": 0.5851,
      "step": 193550
    },
    {
      "epoch": 2.756656699416204,
      "grad_norm": 0.8104872107505798,
      "learning_rate": 1.6298969561707584e-05,
      "loss": 0.5862,
      "step": 193600
    },
    {
      "epoch": 2.75736864587783,
      "grad_norm": 0.5428557991981506,
      "learning_rate": 1.6251483926112352e-05,
      "loss": 0.6127,
      "step": 193650
    },
    {
      "epoch": 2.7580805923394562,
      "grad_norm": 0.6646242737770081,
      "learning_rate": 1.620399829051712e-05,
      "loss": 0.6291,
      "step": 193700
    },
    {
      "epoch": 2.758792538801082,
      "grad_norm": 0.6602131128311157,
      "learning_rate": 1.6156512654921887e-05,
      "loss": 0.6241,
      "step": 193750
    },
    {
      "epoch": 2.759504485262708,
      "grad_norm": 0.4595329761505127,
      "learning_rate": 1.6109027019326654e-05,
      "loss": 0.5821,
      "step": 193800
    },
    {
      "epoch": 2.7602164317243343,
      "grad_norm": 0.7582204937934875,
      "learning_rate": 1.606154138373142e-05,
      "loss": 0.6609,
      "step": 193850
    },
    {
      "epoch": 2.7609283781859606,
      "grad_norm": 0.37869536876678467,
      "learning_rate": 1.601405574813619e-05,
      "loss": 0.6537,
      "step": 193900
    },
    {
      "epoch": 2.7616403246475865,
      "grad_norm": 0.728742778301239,
      "learning_rate": 1.5966570112540956e-05,
      "loss": 0.6486,
      "step": 193950
    },
    {
      "epoch": 2.7623522711092123,
      "grad_norm": 0.8120704293251038,
      "learning_rate": 1.5919084476945724e-05,
      "loss": 0.6167,
      "step": 194000
    },
    {
      "epoch": 2.7630642175708386,
      "grad_norm": 0.6532411575317383,
      "learning_rate": 1.5871598841350495e-05,
      "loss": 0.6682,
      "step": 194050
    },
    {
      "epoch": 2.763776164032465,
      "grad_norm": 0.5753340721130371,
      "learning_rate": 1.582411320575526e-05,
      "loss": 0.673,
      "step": 194100
    },
    {
      "epoch": 2.764488110494091,
      "grad_norm": 0.46839407086372375,
      "learning_rate": 1.5776627570160026e-05,
      "loss": 0.6199,
      "step": 194150
    },
    {
      "epoch": 2.7652000569557167,
      "grad_norm": 0.45047855377197266,
      "learning_rate": 1.5729141934564794e-05,
      "loss": 0.57,
      "step": 194200
    },
    {
      "epoch": 2.765912003417343,
      "grad_norm": 0.4918041527271271,
      "learning_rate": 1.5682606011681466e-05,
      "loss": 0.6301,
      "step": 194250
    },
    {
      "epoch": 2.7666239498789693,
      "grad_norm": 0.6270296573638916,
      "learning_rate": 1.5635120376086234e-05,
      "loss": 0.6104,
      "step": 194300
    },
    {
      "epoch": 2.767335896340595,
      "grad_norm": 0.5364261865615845,
      "learning_rate": 1.5587634740491e-05,
      "loss": 0.6344,
      "step": 194350
    },
    {
      "epoch": 2.768047842802221,
      "grad_norm": 0.6966946125030518,
      "learning_rate": 1.5540149104895772e-05,
      "loss": 0.6544,
      "step": 194400
    },
    {
      "epoch": 2.7687597892638474,
      "grad_norm": 0.6933049559593201,
      "learning_rate": 1.5492663469300536e-05,
      "loss": 0.6112,
      "step": 194450
    },
    {
      "epoch": 2.7694717357254737,
      "grad_norm": 0.6826298236846924,
      "learning_rate": 1.5445177833705303e-05,
      "loss": 0.6121,
      "step": 194500
    },
    {
      "epoch": 2.7701836821870995,
      "grad_norm": 0.6044347882270813,
      "learning_rate": 1.539769219811007e-05,
      "loss": 0.5528,
      "step": 194550
    },
    {
      "epoch": 2.7708956286487254,
      "grad_norm": 0.5204940438270569,
      "learning_rate": 1.5350206562514842e-05,
      "loss": 0.6631,
      "step": 194600
    },
    {
      "epoch": 2.7716075751103517,
      "grad_norm": 0.7220757007598877,
      "learning_rate": 1.530272092691961e-05,
      "loss": 0.6456,
      "step": 194650
    },
    {
      "epoch": 2.772319521571978,
      "grad_norm": 0.727501630783081,
      "learning_rate": 1.5255235291324373e-05,
      "loss": 0.6024,
      "step": 194700
    },
    {
      "epoch": 2.773031468033604,
      "grad_norm": 0.7256930470466614,
      "learning_rate": 1.5207749655729142e-05,
      "loss": 0.6017,
      "step": 194750
    },
    {
      "epoch": 2.7737434144952298,
      "grad_norm": 0.4410790205001831,
      "learning_rate": 1.516026402013391e-05,
      "loss": 0.6217,
      "step": 194800
    },
    {
      "epoch": 2.774455360956856,
      "grad_norm": 0.5687968134880066,
      "learning_rate": 1.5112778384538679e-05,
      "loss": 0.5918,
      "step": 194850
    },
    {
      "epoch": 2.7751673074184824,
      "grad_norm": 0.5815179347991943,
      "learning_rate": 1.5065292748943446e-05,
      "loss": 0.6104,
      "step": 194900
    },
    {
      "epoch": 2.7758792538801083,
      "grad_norm": 0.484509140253067,
      "learning_rate": 1.5017807113348212e-05,
      "loss": 0.5861,
      "step": 194950
    },
    {
      "epoch": 2.776591200341734,
      "grad_norm": 0.680898904800415,
      "learning_rate": 1.497032147775298e-05,
      "loss": 0.5892,
      "step": 195000
    },
    {
      "epoch": 2.7773031468033604,
      "grad_norm": 0.5094682574272156,
      "learning_rate": 1.4922835842157749e-05,
      "loss": 0.5878,
      "step": 195050
    },
    {
      "epoch": 2.7780150932649867,
      "grad_norm": 0.557694673538208,
      "learning_rate": 1.4875350206562516e-05,
      "loss": 0.6427,
      "step": 195100
    },
    {
      "epoch": 2.7787270397266126,
      "grad_norm": 0.9552493691444397,
      "learning_rate": 1.4827864570967284e-05,
      "loss": 0.6557,
      "step": 195150
    },
    {
      "epoch": 2.7794389861882385,
      "grad_norm": 0.555528461933136,
      "learning_rate": 1.478037893537205e-05,
      "loss": 0.6127,
      "step": 195200
    },
    {
      "epoch": 2.780150932649865,
      "grad_norm": 0.3855656385421753,
      "learning_rate": 1.4732893299776817e-05,
      "loss": 0.5682,
      "step": 195250
    },
    {
      "epoch": 2.7808628791114907,
      "grad_norm": 0.5806857943534851,
      "learning_rate": 1.4685407664181586e-05,
      "loss": 0.6339,
      "step": 195300
    },
    {
      "epoch": 2.781574825573117,
      "grad_norm": 0.6666297912597656,
      "learning_rate": 1.4637922028586353e-05,
      "loss": 0.6525,
      "step": 195350
    },
    {
      "epoch": 2.782286772034743,
      "grad_norm": 0.6783741116523743,
      "learning_rate": 1.4590436392991123e-05,
      "loss": 0.6049,
      "step": 195400
    },
    {
      "epoch": 2.782998718496369,
      "grad_norm": 0.6430146098136902,
      "learning_rate": 1.4542950757395887e-05,
      "loss": 0.594,
      "step": 195450
    },
    {
      "epoch": 2.783710664957995,
      "grad_norm": 0.9065188765525818,
      "learning_rate": 1.4495465121800656e-05,
      "loss": 0.631,
      "step": 195500
    },
    {
      "epoch": 2.7844226114196213,
      "grad_norm": 0.6864643096923828,
      "learning_rate": 1.4447979486205423e-05,
      "loss": 0.6695,
      "step": 195550
    },
    {
      "epoch": 2.785134557881247,
      "grad_norm": 0.5499810576438904,
      "learning_rate": 1.4400493850610192e-05,
      "loss": 0.5588,
      "step": 195600
    },
    {
      "epoch": 2.7858465043428735,
      "grad_norm": 0.6538709998130798,
      "learning_rate": 1.435300821501496e-05,
      "loss": 0.6308,
      "step": 195650
    },
    {
      "epoch": 2.7865584508044994,
      "grad_norm": 0.44770142436027527,
      "learning_rate": 1.4305522579419726e-05,
      "loss": 0.6718,
      "step": 195700
    },
    {
      "epoch": 2.7872703972661257,
      "grad_norm": 0.49839138984680176,
      "learning_rate": 1.4258036943824493e-05,
      "loss": 0.6279,
      "step": 195750
    },
    {
      "epoch": 2.7879823437277516,
      "grad_norm": 0.6562375426292419,
      "learning_rate": 1.4210551308229262e-05,
      "loss": 0.5627,
      "step": 195800
    },
    {
      "epoch": 2.788694290189378,
      "grad_norm": 0.6397470831871033,
      "learning_rate": 1.416306567263403e-05,
      "loss": 0.5948,
      "step": 195850
    },
    {
      "epoch": 2.7894062366510037,
      "grad_norm": 0.6734151840209961,
      "learning_rate": 1.4115580037038797e-05,
      "loss": 0.6164,
      "step": 195900
    },
    {
      "epoch": 2.79011818311263,
      "grad_norm": 0.630952775478363,
      "learning_rate": 1.4068094401443563e-05,
      "loss": 0.6139,
      "step": 195950
    },
    {
      "epoch": 2.790830129574256,
      "grad_norm": 0.42433762550354004,
      "learning_rate": 1.402060876584833e-05,
      "loss": 0.6275,
      "step": 196000
    },
    {
      "epoch": 2.7915420760358822,
      "grad_norm": 0.6407381296157837,
      "learning_rate": 1.3974072842965003e-05,
      "loss": 0.6308,
      "step": 196050
    },
    {
      "epoch": 2.792254022497508,
      "grad_norm": 0.5351278185844421,
      "learning_rate": 1.392658720736977e-05,
      "loss": 0.637,
      "step": 196100
    },
    {
      "epoch": 2.7929659689591344,
      "grad_norm": 0.6396952867507935,
      "learning_rate": 1.387910157177454e-05,
      "loss": 0.7088,
      "step": 196150
    },
    {
      "epoch": 2.7936779154207603,
      "grad_norm": 0.8222958445549011,
      "learning_rate": 1.3831615936179307e-05,
      "loss": 0.6134,
      "step": 196200
    },
    {
      "epoch": 2.7943898618823866,
      "grad_norm": 1.0742789506912231,
      "learning_rate": 1.3784130300584074e-05,
      "loss": 0.5927,
      "step": 196250
    },
    {
      "epoch": 2.7951018083440125,
      "grad_norm": 0.49890586733818054,
      "learning_rate": 1.373664466498884e-05,
      "loss": 0.5833,
      "step": 196300
    },
    {
      "epoch": 2.7958137548056388,
      "grad_norm": 0.5488620400428772,
      "learning_rate": 1.3689159029393609e-05,
      "loss": 0.6185,
      "step": 196350
    },
    {
      "epoch": 2.7965257012672646,
      "grad_norm": 0.5997825264930725,
      "learning_rate": 1.3641673393798377e-05,
      "loss": 0.5957,
      "step": 196400
    },
    {
      "epoch": 2.797237647728891,
      "grad_norm": 0.7517690658569336,
      "learning_rate": 1.3594187758203144e-05,
      "loss": 0.6311,
      "step": 196450
    },
    {
      "epoch": 2.797949594190517,
      "grad_norm": 0.8485053777694702,
      "learning_rate": 1.3546702122607913e-05,
      "loss": 0.616,
      "step": 196500
    },
    {
      "epoch": 2.798661540652143,
      "grad_norm": 0.5495760440826416,
      "learning_rate": 1.3499216487012679e-05,
      "loss": 0.5941,
      "step": 196550
    },
    {
      "epoch": 2.799373487113769,
      "grad_norm": 0.6644322872161865,
      "learning_rate": 1.3451730851417446e-05,
      "loss": 0.619,
      "step": 196600
    },
    {
      "epoch": 2.8000854335753953,
      "grad_norm": 0.5451083779335022,
      "learning_rate": 1.3404245215822214e-05,
      "loss": 0.6157,
      "step": 196650
    },
    {
      "epoch": 2.800797380037021,
      "grad_norm": 1.1992390155792236,
      "learning_rate": 1.3356759580226983e-05,
      "loss": 0.657,
      "step": 196700
    },
    {
      "epoch": 2.801509326498647,
      "grad_norm": 0.8970928192138672,
      "learning_rate": 1.330927394463175e-05,
      "loss": 0.634,
      "step": 196750
    },
    {
      "epoch": 2.8022212729602733,
      "grad_norm": 0.7090418934822083,
      "learning_rate": 1.3261788309036516e-05,
      "loss": 0.6125,
      "step": 196800
    },
    {
      "epoch": 2.8029332194218997,
      "grad_norm": 0.6017774939537048,
      "learning_rate": 1.3214302673441284e-05,
      "loss": 0.6043,
      "step": 196850
    },
    {
      "epoch": 2.8036451658835255,
      "grad_norm": 0.6420480012893677,
      "learning_rate": 1.3166817037846053e-05,
      "loss": 0.5975,
      "step": 196900
    },
    {
      "epoch": 2.8043571123451514,
      "grad_norm": 0.783672034740448,
      "learning_rate": 1.311933140225082e-05,
      "loss": 0.6641,
      "step": 196950
    },
    {
      "epoch": 2.8050690588067777,
      "grad_norm": 0.582582950592041,
      "learning_rate": 1.3071845766655588e-05,
      "loss": 0.6273,
      "step": 197000
    },
    {
      "epoch": 2.805781005268404,
      "grad_norm": 0.6522271037101746,
      "learning_rate": 1.3024360131060353e-05,
      "loss": 0.6663,
      "step": 197050
    },
    {
      "epoch": 2.80649295173003,
      "grad_norm": 0.6617291569709778,
      "learning_rate": 1.2976874495465122e-05,
      "loss": 0.6587,
      "step": 197100
    },
    {
      "epoch": 2.8072048981916558,
      "grad_norm": 0.6793417930603027,
      "learning_rate": 1.292938885986989e-05,
      "loss": 0.6019,
      "step": 197150
    },
    {
      "epoch": 2.807916844653282,
      "grad_norm": 0.7575954794883728,
      "learning_rate": 1.2881903224274657e-05,
      "loss": 0.6111,
      "step": 197200
    },
    {
      "epoch": 2.8086287911149084,
      "grad_norm": 0.4912566840648651,
      "learning_rate": 1.2834417588679426e-05,
      "loss": 0.5973,
      "step": 197250
    },
    {
      "epoch": 2.8093407375765342,
      "grad_norm": 0.5227546095848083,
      "learning_rate": 1.278693195308419e-05,
      "loss": 0.6765,
      "step": 197300
    },
    {
      "epoch": 2.81005268403816,
      "grad_norm": 0.6367138028144836,
      "learning_rate": 1.273944631748896e-05,
      "loss": 0.5895,
      "step": 197350
    },
    {
      "epoch": 2.8107646304997864,
      "grad_norm": 0.694107174873352,
      "learning_rate": 1.2691960681893727e-05,
      "loss": 0.5816,
      "step": 197400
    },
    {
      "epoch": 2.8114765769614127,
      "grad_norm": 0.4568180739879608,
      "learning_rate": 1.2644475046298496e-05,
      "loss": 0.6301,
      "step": 197450
    },
    {
      "epoch": 2.8121885234230386,
      "grad_norm": 0.6910839080810547,
      "learning_rate": 1.2596989410703264e-05,
      "loss": 0.6083,
      "step": 197500
    },
    {
      "epoch": 2.8129004698846645,
      "grad_norm": 0.7790769338607788,
      "learning_rate": 1.254950377510803e-05,
      "loss": 0.6345,
      "step": 197550
    },
    {
      "epoch": 2.813612416346291,
      "grad_norm": 0.4473228454589844,
      "learning_rate": 1.2502018139512797e-05,
      "loss": 0.5835,
      "step": 197600
    },
    {
      "epoch": 2.814324362807917,
      "grad_norm": 0.7825135588645935,
      "learning_rate": 1.2454532503917566e-05,
      "loss": 0.5907,
      "step": 197650
    },
    {
      "epoch": 2.815036309269543,
      "grad_norm": 0.4719949960708618,
      "learning_rate": 1.2407046868322333e-05,
      "loss": 0.646,
      "step": 197700
    },
    {
      "epoch": 2.815748255731169,
      "grad_norm": 0.45348361134529114,
      "learning_rate": 1.2359561232727101e-05,
      "loss": 0.6477,
      "step": 197750
    },
    {
      "epoch": 2.816460202192795,
      "grad_norm": 0.48079222440719604,
      "learning_rate": 1.2312075597131868e-05,
      "loss": 0.6199,
      "step": 197800
    },
    {
      "epoch": 2.8171721486544214,
      "grad_norm": 0.6886710524559021,
      "learning_rate": 1.2264589961536636e-05,
      "loss": 0.6321,
      "step": 197850
    },
    {
      "epoch": 2.8178840951160473,
      "grad_norm": 0.6475088596343994,
      "learning_rate": 1.2217104325941403e-05,
      "loss": 0.6148,
      "step": 197900
    },
    {
      "epoch": 2.818596041577673,
      "grad_norm": 0.7823886275291443,
      "learning_rate": 1.216961869034617e-05,
      "loss": 0.6238,
      "step": 197950
    },
    {
      "epoch": 2.8193079880392995,
      "grad_norm": 0.5106921195983887,
      "learning_rate": 1.2122133054750938e-05,
      "loss": 0.5881,
      "step": 198000
    },
    {
      "epoch": 2.820019934500926,
      "grad_norm": 0.5183159708976746,
      "learning_rate": 1.2074647419155706e-05,
      "loss": 0.6265,
      "step": 198050
    },
    {
      "epoch": 2.8207318809625517,
      "grad_norm": 0.5634740591049194,
      "learning_rate": 1.2027161783560475e-05,
      "loss": 0.6478,
      "step": 198100
    },
    {
      "epoch": 2.8214438274241775,
      "grad_norm": 0.7067920565605164,
      "learning_rate": 1.197967614796524e-05,
      "loss": 0.6283,
      "step": 198150
    },
    {
      "epoch": 2.822155773885804,
      "grad_norm": 0.5864097476005554,
      "learning_rate": 1.193219051237001e-05,
      "loss": 0.6554,
      "step": 198200
    },
    {
      "epoch": 2.8228677203474297,
      "grad_norm": 0.35929036140441895,
      "learning_rate": 1.1884704876774775e-05,
      "loss": 0.6536,
      "step": 198250
    },
    {
      "epoch": 2.823579666809056,
      "grad_norm": 0.5448621511459351,
      "learning_rate": 1.1838168953891448e-05,
      "loss": 0.6381,
      "step": 198300
    },
    {
      "epoch": 2.824291613270682,
      "grad_norm": 0.5568029880523682,
      "learning_rate": 1.1790683318296215e-05,
      "loss": 0.6056,
      "step": 198350
    },
    {
      "epoch": 2.825003559732308,
      "grad_norm": 0.6724300384521484,
      "learning_rate": 1.1743197682700983e-05,
      "loss": 0.6182,
      "step": 198400
    },
    {
      "epoch": 2.825715506193934,
      "grad_norm": 0.3797796070575714,
      "learning_rate": 1.1695712047105752e-05,
      "loss": 0.5804,
      "step": 198450
    },
    {
      "epoch": 2.8264274526555604,
      "grad_norm": 0.6098197102546692,
      "learning_rate": 1.1648226411510518e-05,
      "loss": 0.6051,
      "step": 198500
    },
    {
      "epoch": 2.8271393991171863,
      "grad_norm": 0.5414014458656311,
      "learning_rate": 1.1600740775915287e-05,
      "loss": 0.6526,
      "step": 198550
    },
    {
      "epoch": 2.8278513455788126,
      "grad_norm": 0.5039028525352478,
      "learning_rate": 1.1553255140320053e-05,
      "loss": 0.6241,
      "step": 198600
    },
    {
      "epoch": 2.8285632920404384,
      "grad_norm": 0.6031395196914673,
      "learning_rate": 1.1505769504724822e-05,
      "loss": 0.6038,
      "step": 198650
    },
    {
      "epoch": 2.8292752385020647,
      "grad_norm": 0.5190285444259644,
      "learning_rate": 1.1458283869129589e-05,
      "loss": 0.6774,
      "step": 198700
    },
    {
      "epoch": 2.8299871849636906,
      "grad_norm": 0.5206231474876404,
      "learning_rate": 1.1410798233534357e-05,
      "loss": 0.624,
      "step": 198750
    },
    {
      "epoch": 2.830699131425317,
      "grad_norm": 0.9117158055305481,
      "learning_rate": 1.1363312597939124e-05,
      "loss": 0.6328,
      "step": 198800
    },
    {
      "epoch": 2.831411077886943,
      "grad_norm": 0.5943897366523743,
      "learning_rate": 1.1315826962343891e-05,
      "loss": 0.5727,
      "step": 198850
    },
    {
      "epoch": 2.832123024348569,
      "grad_norm": 0.6165595054626465,
      "learning_rate": 1.1268341326748659e-05,
      "loss": 0.5634,
      "step": 198900
    },
    {
      "epoch": 2.832834970810195,
      "grad_norm": 0.5703089237213135,
      "learning_rate": 1.1220855691153426e-05,
      "loss": 0.6268,
      "step": 198950
    },
    {
      "epoch": 2.8335469172718213,
      "grad_norm": 0.6414206027984619,
      "learning_rate": 1.1173370055558194e-05,
      "loss": 0.6641,
      "step": 199000
    },
    {
      "epoch": 2.834258863733447,
      "grad_norm": 0.5794171094894409,
      "learning_rate": 1.1125884419962961e-05,
      "loss": 0.6306,
      "step": 199050
    },
    {
      "epoch": 2.8349708101950735,
      "grad_norm": 0.9493052363395691,
      "learning_rate": 1.1078398784367729e-05,
      "loss": 0.5907,
      "step": 199100
    },
    {
      "epoch": 2.8356827566566993,
      "grad_norm": 0.6039842963218689,
      "learning_rate": 1.1030913148772496e-05,
      "loss": 0.633,
      "step": 199150
    },
    {
      "epoch": 2.8363947031183256,
      "grad_norm": 0.6014659404754639,
      "learning_rate": 1.0983427513177265e-05,
      "loss": 0.6576,
      "step": 199200
    },
    {
      "epoch": 2.8371066495799515,
      "grad_norm": 0.5550588965415955,
      "learning_rate": 1.0935941877582031e-05,
      "loss": 0.5902,
      "step": 199250
    },
    {
      "epoch": 2.837818596041578,
      "grad_norm": 0.6443132162094116,
      "learning_rate": 1.08884562419868e-05,
      "loss": 0.647,
      "step": 199300
    },
    {
      "epoch": 2.8385305425032037,
      "grad_norm": 0.3399328589439392,
      "learning_rate": 1.0840970606391566e-05,
      "loss": 0.6284,
      "step": 199350
    },
    {
      "epoch": 2.83924248896483,
      "grad_norm": 0.5113833546638489,
      "learning_rate": 1.0793484970796335e-05,
      "loss": 0.6672,
      "step": 199400
    },
    {
      "epoch": 2.839954435426456,
      "grad_norm": 0.805874228477478,
      "learning_rate": 1.0745999335201102e-05,
      "loss": 0.6111,
      "step": 199450
    },
    {
      "epoch": 2.840666381888082,
      "grad_norm": 0.6070775389671326,
      "learning_rate": 1.069851369960587e-05,
      "loss": 0.5647,
      "step": 199500
    },
    {
      "epoch": 2.841378328349708,
      "grad_norm": 0.5975291132926941,
      "learning_rate": 1.0651028064010637e-05,
      "loss": 0.5869,
      "step": 199550
    },
    {
      "epoch": 2.8420902748113344,
      "grad_norm": 0.7040490508079529,
      "learning_rate": 1.0603542428415405e-05,
      "loss": 0.6017,
      "step": 199600
    },
    {
      "epoch": 2.8428022212729602,
      "grad_norm": 0.42107611894607544,
      "learning_rate": 1.0556056792820172e-05,
      "loss": 0.6094,
      "step": 199650
    },
    {
      "epoch": 2.843514167734586,
      "grad_norm": 0.4554671347141266,
      "learning_rate": 1.050857115722494e-05,
      "loss": 0.5885,
      "step": 199700
    },
    {
      "epoch": 2.8442261141962124,
      "grad_norm": 0.6220812797546387,
      "learning_rate": 1.0461085521629707e-05,
      "loss": 0.6528,
      "step": 199750
    },
    {
      "epoch": 2.8449380606578387,
      "grad_norm": 0.6197019219398499,
      "learning_rate": 1.0413599886034475e-05,
      "loss": 0.6335,
      "step": 199800
    },
    {
      "epoch": 2.8456500071194646,
      "grad_norm": 0.5926464796066284,
      "learning_rate": 1.0366114250439242e-05,
      "loss": 0.579,
      "step": 199850
    },
    {
      "epoch": 2.8463619535810905,
      "grad_norm": 0.3912477195262909,
      "learning_rate": 1.031862861484401e-05,
      "loss": 0.6059,
      "step": 199900
    },
    {
      "epoch": 2.8470739000427168,
      "grad_norm": 0.5908161997795105,
      "learning_rate": 1.0271142979248779e-05,
      "loss": 0.6271,
      "step": 199950
    },
    {
      "epoch": 2.847785846504343,
      "grad_norm": 0.7047178149223328,
      "learning_rate": 1.0223657343653544e-05,
      "loss": 0.6828,
      "step": 200000
    },
    {
      "epoch": 2.848497792965969,
      "grad_norm": 0.31160321831703186,
      "learning_rate": 1.0176171708058314e-05,
      "loss": 0.6374,
      "step": 200050
    },
    {
      "epoch": 2.849209739427595,
      "grad_norm": 0.4955957531929016,
      "learning_rate": 1.012868607246308e-05,
      "loss": 0.6256,
      "step": 200100
    },
    {
      "epoch": 2.849921685889221,
      "grad_norm": 0.7628871202468872,
      "learning_rate": 1.0081200436867848e-05,
      "loss": 0.586,
      "step": 200150
    },
    {
      "epoch": 2.8506336323508474,
      "grad_norm": 0.49797749519348145,
      "learning_rate": 1.0033714801272616e-05,
      "loss": 0.6126,
      "step": 200200
    },
    {
      "epoch": 2.8513455788124733,
      "grad_norm": 0.6167324185371399,
      "learning_rate": 9.986229165677383e-06,
      "loss": 0.6671,
      "step": 200250
    },
    {
      "epoch": 2.852057525274099,
      "grad_norm": 0.5613489747047424,
      "learning_rate": 9.93874353008215e-06,
      "loss": 0.5611,
      "step": 200300
    },
    {
      "epoch": 2.8527694717357255,
      "grad_norm": 0.350090891122818,
      "learning_rate": 9.891257894486918e-06,
      "loss": 0.5995,
      "step": 200350
    },
    {
      "epoch": 2.853481418197352,
      "grad_norm": 0.5038313269615173,
      "learning_rate": 9.843772258891686e-06,
      "loss": 0.5982,
      "step": 200400
    },
    {
      "epoch": 2.8541933646589777,
      "grad_norm": 0.5126240849494934,
      "learning_rate": 9.796286623296453e-06,
      "loss": 0.6103,
      "step": 200450
    },
    {
      "epoch": 2.8549053111206035,
      "grad_norm": 0.8701808452606201,
      "learning_rate": 9.74880098770122e-06,
      "loss": 0.6355,
      "step": 200500
    },
    {
      "epoch": 2.85561725758223,
      "grad_norm": 0.6414539217948914,
      "learning_rate": 9.701315352105988e-06,
      "loss": 0.5844,
      "step": 200550
    },
    {
      "epoch": 2.856329204043856,
      "grad_norm": 0.39822983741760254,
      "learning_rate": 9.653829716510757e-06,
      "loss": 0.5729,
      "step": 200600
    },
    {
      "epoch": 2.857041150505482,
      "grad_norm": 0.39804691076278687,
      "learning_rate": 9.606344080915523e-06,
      "loss": 0.5315,
      "step": 200650
    },
    {
      "epoch": 2.857753096967108,
      "grad_norm": 0.6027020215988159,
      "learning_rate": 9.558858445320292e-06,
      "loss": 0.5903,
      "step": 200700
    },
    {
      "epoch": 2.858465043428734,
      "grad_norm": 0.5646470189094543,
      "learning_rate": 9.511372809725058e-06,
      "loss": 0.6359,
      "step": 200750
    },
    {
      "epoch": 2.8591769898903605,
      "grad_norm": 0.6012035608291626,
      "learning_rate": 9.463887174129827e-06,
      "loss": 0.6049,
      "step": 200800
    },
    {
      "epoch": 2.8598889363519864,
      "grad_norm": 0.39544183015823364,
      "learning_rate": 9.416401538534594e-06,
      "loss": 0.6302,
      "step": 200850
    },
    {
      "epoch": 2.8606008828136122,
      "grad_norm": 0.7535346746444702,
      "learning_rate": 9.368915902939362e-06,
      "loss": 0.6449,
      "step": 200900
    },
    {
      "epoch": 2.8613128292752386,
      "grad_norm": 0.7132997512817383,
      "learning_rate": 9.32143026734413e-06,
      "loss": 0.5749,
      "step": 200950
    },
    {
      "epoch": 2.8620247757368644,
      "grad_norm": 0.6891600489616394,
      "learning_rate": 9.273944631748897e-06,
      "loss": 0.5906,
      "step": 201000
    },
    {
      "epoch": 2.8627367221984907,
      "grad_norm": 0.4104975759983063,
      "learning_rate": 9.226458996153664e-06,
      "loss": 0.623,
      "step": 201050
    },
    {
      "epoch": 2.8634486686601166,
      "grad_norm": 0.6966743469238281,
      "learning_rate": 9.178973360558432e-06,
      "loss": 0.6427,
      "step": 201100
    },
    {
      "epoch": 2.864160615121743,
      "grad_norm": 0.564293384552002,
      "learning_rate": 9.131487724963199e-06,
      "loss": 0.5764,
      "step": 201150
    },
    {
      "epoch": 2.864872561583369,
      "grad_norm": 1.0763524770736694,
      "learning_rate": 9.084002089367966e-06,
      "loss": 0.6676,
      "step": 201200
    },
    {
      "epoch": 2.865584508044995,
      "grad_norm": 0.6060575246810913,
      "learning_rate": 9.036516453772734e-06,
      "loss": 0.7094,
      "step": 201250
    },
    {
      "epoch": 2.866296454506621,
      "grad_norm": 0.5035820603370667,
      "learning_rate": 8.989030818177501e-06,
      "loss": 0.6313,
      "step": 201300
    },
    {
      "epoch": 2.8670084009682473,
      "grad_norm": 0.9603790044784546,
      "learning_rate": 8.94154518258227e-06,
      "loss": 0.6365,
      "step": 201350
    },
    {
      "epoch": 2.867720347429873,
      "grad_norm": 0.4922676682472229,
      "learning_rate": 8.894059546987036e-06,
      "loss": 0.6676,
      "step": 201400
    },
    {
      "epoch": 2.8684322938914995,
      "grad_norm": 0.501472532749176,
      "learning_rate": 8.846573911391805e-06,
      "loss": 0.6169,
      "step": 201450
    },
    {
      "epoch": 2.8691442403531253,
      "grad_norm": 0.6224469542503357,
      "learning_rate": 8.799088275796571e-06,
      "loss": 0.6371,
      "step": 201500
    },
    {
      "epoch": 2.8698561868147516,
      "grad_norm": 0.5215885043144226,
      "learning_rate": 8.75160264020134e-06,
      "loss": 0.605,
      "step": 201550
    },
    {
      "epoch": 2.8705681332763775,
      "grad_norm": 0.931250810623169,
      "learning_rate": 8.704117004606108e-06,
      "loss": 0.6199,
      "step": 201600
    },
    {
      "epoch": 2.871280079738004,
      "grad_norm": 0.5961074233055115,
      "learning_rate": 8.656631369010875e-06,
      "loss": 0.6289,
      "step": 201650
    },
    {
      "epoch": 2.8719920261996297,
      "grad_norm": 0.6228479743003845,
      "learning_rate": 8.609145733415643e-06,
      "loss": 0.5975,
      "step": 201700
    },
    {
      "epoch": 2.872703972661256,
      "grad_norm": 0.7360463738441467,
      "learning_rate": 8.56166009782041e-06,
      "loss": 0.6268,
      "step": 201750
    },
    {
      "epoch": 2.873415919122882,
      "grad_norm": 1.0730600357055664,
      "learning_rate": 8.514174462225177e-06,
      "loss": 0.5826,
      "step": 201800
    },
    {
      "epoch": 2.874127865584508,
      "grad_norm": 0.5396009683609009,
      "learning_rate": 8.466688826629945e-06,
      "loss": 0.6577,
      "step": 201850
    },
    {
      "epoch": 2.874839812046134,
      "grad_norm": 0.6262459754943848,
      "learning_rate": 8.419203191034712e-06,
      "loss": 0.645,
      "step": 201900
    },
    {
      "epoch": 2.8755517585077603,
      "grad_norm": 0.5649413466453552,
      "learning_rate": 8.37171755543948e-06,
      "loss": 0.6357,
      "step": 201950
    },
    {
      "epoch": 2.876263704969386,
      "grad_norm": 0.49261608719825745,
      "learning_rate": 8.324231919844247e-06,
      "loss": 0.6466,
      "step": 202000
    },
    {
      "epoch": 2.8769756514310125,
      "grad_norm": 0.46820610761642456,
      "learning_rate": 8.276746284249015e-06,
      "loss": 0.6038,
      "step": 202050
    },
    {
      "epoch": 2.8776875978926384,
      "grad_norm": 0.47456252574920654,
      "learning_rate": 8.229260648653784e-06,
      "loss": 0.6372,
      "step": 202100
    },
    {
      "epoch": 2.8783995443542647,
      "grad_norm": 0.9168520569801331,
      "learning_rate": 8.18177501305855e-06,
      "loss": 0.6435,
      "step": 202150
    },
    {
      "epoch": 2.8791114908158906,
      "grad_norm": 0.5701863169670105,
      "learning_rate": 8.134289377463319e-06,
      "loss": 0.625,
      "step": 202200
    },
    {
      "epoch": 2.879823437277517,
      "grad_norm": 0.7762960195541382,
      "learning_rate": 8.086803741868084e-06,
      "loss": 0.6489,
      "step": 202250
    },
    {
      "epoch": 2.8805353837391428,
      "grad_norm": 0.5290984511375427,
      "learning_rate": 8.039318106272854e-06,
      "loss": 0.579,
      "step": 202300
    },
    {
      "epoch": 2.881247330200769,
      "grad_norm": 0.7076377272605896,
      "learning_rate": 7.992782183389524e-06,
      "loss": 0.6289,
      "step": 202350
    },
    {
      "epoch": 2.881959276662395,
      "grad_norm": 0.7207008004188538,
      "learning_rate": 7.945296547794292e-06,
      "loss": 0.6074,
      "step": 202400
    },
    {
      "epoch": 2.882671223124021,
      "grad_norm": 0.5928089022636414,
      "learning_rate": 7.897810912199061e-06,
      "loss": 0.6568,
      "step": 202450
    },
    {
      "epoch": 2.883383169585647,
      "grad_norm": 0.7949203252792358,
      "learning_rate": 7.850325276603827e-06,
      "loss": 0.6228,
      "step": 202500
    },
    {
      "epoch": 2.8840951160472734,
      "grad_norm": 0.5874942541122437,
      "learning_rate": 7.802839641008596e-06,
      "loss": 0.6536,
      "step": 202550
    },
    {
      "epoch": 2.8848070625088993,
      "grad_norm": 0.5601254105567932,
      "learning_rate": 7.755354005413362e-06,
      "loss": 0.6216,
      "step": 202600
    },
    {
      "epoch": 2.885519008970525,
      "grad_norm": 0.43484270572662354,
      "learning_rate": 7.70786836981813e-06,
      "loss": 0.6422,
      "step": 202650
    },
    {
      "epoch": 2.8862309554321515,
      "grad_norm": 0.44023793935775757,
      "learning_rate": 7.660382734222898e-06,
      "loss": 0.6646,
      "step": 202700
    },
    {
      "epoch": 2.886942901893778,
      "grad_norm": 0.5740359425544739,
      "learning_rate": 7.612897098627665e-06,
      "loss": 0.682,
      "step": 202750
    },
    {
      "epoch": 2.8876548483554036,
      "grad_norm": 0.4734180271625519,
      "learning_rate": 7.565411463032433e-06,
      "loss": 0.6481,
      "step": 202800
    },
    {
      "epoch": 2.8883667948170295,
      "grad_norm": 0.5783068537712097,
      "learning_rate": 7.5179258274372e-06,
      "loss": 0.6712,
      "step": 202850
    },
    {
      "epoch": 2.889078741278656,
      "grad_norm": 0.61299729347229,
      "learning_rate": 7.470440191841968e-06,
      "loss": 0.6586,
      "step": 202900
    },
    {
      "epoch": 2.889790687740282,
      "grad_norm": 0.4475652277469635,
      "learning_rate": 7.422954556246736e-06,
      "loss": 0.6115,
      "step": 202950
    },
    {
      "epoch": 2.890502634201908,
      "grad_norm": 0.3466436266899109,
      "learning_rate": 7.375468920651503e-06,
      "loss": 0.6459,
      "step": 203000
    },
    {
      "epoch": 2.891214580663534,
      "grad_norm": 0.5386353135108948,
      "learning_rate": 7.327983285056271e-06,
      "loss": 0.6383,
      "step": 203050
    },
    {
      "epoch": 2.89192652712516,
      "grad_norm": 0.4254855215549469,
      "learning_rate": 7.280497649461038e-06,
      "loss": 0.5665,
      "step": 203100
    },
    {
      "epoch": 2.8926384735867865,
      "grad_norm": 0.642573893070221,
      "learning_rate": 7.233012013865806e-06,
      "loss": 0.599,
      "step": 203150
    },
    {
      "epoch": 2.8933504200484124,
      "grad_norm": 0.6371849775314331,
      "learning_rate": 7.185526378270574e-06,
      "loss": 0.6242,
      "step": 203200
    },
    {
      "epoch": 2.8940623665100382,
      "grad_norm": 0.6852909922599792,
      "learning_rate": 7.138040742675341e-06,
      "loss": 0.5942,
      "step": 203250
    },
    {
      "epoch": 2.8947743129716645,
      "grad_norm": 0.4341759979724884,
      "learning_rate": 7.090555107080108e-06,
      "loss": 0.6159,
      "step": 203300
    },
    {
      "epoch": 2.895486259433291,
      "grad_norm": 0.8091109991073608,
      "learning_rate": 7.043069471484876e-06,
      "loss": 0.6276,
      "step": 203350
    },
    {
      "epoch": 2.8961982058949167,
      "grad_norm": 0.627403199672699,
      "learning_rate": 6.995583835889643e-06,
      "loss": 0.624,
      "step": 203400
    },
    {
      "epoch": 2.8969101523565426,
      "grad_norm": 0.5222672820091248,
      "learning_rate": 6.9480982002944116e-06,
      "loss": 0.5942,
      "step": 203450
    },
    {
      "epoch": 2.897622098818169,
      "grad_norm": 0.6464759707450867,
      "learning_rate": 6.900612564699178e-06,
      "loss": 0.609,
      "step": 203500
    },
    {
      "epoch": 2.898334045279795,
      "grad_norm": 0.5504827499389648,
      "learning_rate": 6.8531269291039464e-06,
      "loss": 0.6171,
      "step": 203550
    },
    {
      "epoch": 2.899045991741421,
      "grad_norm": 0.696607768535614,
      "learning_rate": 6.805641293508713e-06,
      "loss": 0.6193,
      "step": 203600
    },
    {
      "epoch": 2.899757938203047,
      "grad_norm": 0.5769424438476562,
      "learning_rate": 6.758155657913481e-06,
      "loss": 0.6163,
      "step": 203650
    },
    {
      "epoch": 2.9004698846646733,
      "grad_norm": 0.6279970407485962,
      "learning_rate": 6.71067002231825e-06,
      "loss": 0.6437,
      "step": 203700
    },
    {
      "epoch": 2.9011818311262996,
      "grad_norm": 0.9338735938072205,
      "learning_rate": 6.663184386723016e-06,
      "loss": 0.6386,
      "step": 203750
    },
    {
      "epoch": 2.9018937775879254,
      "grad_norm": 0.6138551831245422,
      "learning_rate": 6.6156987511277845e-06,
      "loss": 0.6346,
      "step": 203800
    },
    {
      "epoch": 2.9026057240495513,
      "grad_norm": 0.5203239321708679,
      "learning_rate": 6.568213115532551e-06,
      "loss": 0.6344,
      "step": 203850
    },
    {
      "epoch": 2.9033176705111776,
      "grad_norm": 1.024416208267212,
      "learning_rate": 6.520727479937319e-06,
      "loss": 0.6812,
      "step": 203900
    },
    {
      "epoch": 2.9040296169728035,
      "grad_norm": 0.7638204097747803,
      "learning_rate": 6.473241844342087e-06,
      "loss": 0.5968,
      "step": 203950
    },
    {
      "epoch": 2.90474156343443,
      "grad_norm": 0.5255268216133118,
      "learning_rate": 6.425756208746854e-06,
      "loss": 0.6798,
      "step": 204000
    },
    {
      "epoch": 2.9054535098960557,
      "grad_norm": 1.3751518726348877,
      "learning_rate": 6.378270573151622e-06,
      "loss": 0.6343,
      "step": 204050
    },
    {
      "epoch": 2.906165456357682,
      "grad_norm": 0.48821696639060974,
      "learning_rate": 6.33078493755639e-06,
      "loss": 0.6076,
      "step": 204100
    },
    {
      "epoch": 2.906877402819308,
      "grad_norm": 0.32547229528427124,
      "learning_rate": 6.283299301961157e-06,
      "loss": 0.6464,
      "step": 204150
    },
    {
      "epoch": 2.907589349280934,
      "grad_norm": 0.6197628378868103,
      "learning_rate": 6.235813666365924e-06,
      "loss": 0.6696,
      "step": 204200
    },
    {
      "epoch": 2.90830129574256,
      "grad_norm": 0.5650162696838379,
      "learning_rate": 6.188328030770692e-06,
      "loss": 0.6486,
      "step": 204250
    },
    {
      "epoch": 2.9090132422041863,
      "grad_norm": 0.683994710445404,
      "learning_rate": 6.14084239517546e-06,
      "loss": 0.6714,
      "step": 204300
    },
    {
      "epoch": 2.909725188665812,
      "grad_norm": 0.6069648861885071,
      "learning_rate": 6.093356759580227e-06,
      "loss": 0.645,
      "step": 204350
    },
    {
      "epoch": 2.9104371351274385,
      "grad_norm": 0.6950828433036804,
      "learning_rate": 6.045871123984995e-06,
      "loss": 0.647,
      "step": 204400
    },
    {
      "epoch": 2.9111490815890644,
      "grad_norm": 0.7053789496421814,
      "learning_rate": 5.999335201101667e-06,
      "loss": 0.5985,
      "step": 204450
    },
    {
      "epoch": 2.9118610280506907,
      "grad_norm": 0.568390965461731,
      "learning_rate": 5.951849565506435e-06,
      "loss": 0.6141,
      "step": 204500
    },
    {
      "epoch": 2.9125729745123166,
      "grad_norm": 0.530561089515686,
      "learning_rate": 5.904363929911202e-06,
      "loss": 0.6308,
      "step": 204550
    },
    {
      "epoch": 2.913284920973943,
      "grad_norm": 0.9049892425537109,
      "learning_rate": 5.8568782943159696e-06,
      "loss": 0.6681,
      "step": 204600
    },
    {
      "epoch": 2.9139968674355687,
      "grad_norm": 0.831148624420166,
      "learning_rate": 5.809392658720737e-06,
      "loss": 0.6026,
      "step": 204650
    },
    {
      "epoch": 2.914708813897195,
      "grad_norm": 0.6962618827819824,
      "learning_rate": 5.7619070231255045e-06,
      "loss": 0.6279,
      "step": 204700
    },
    {
      "epoch": 2.915420760358821,
      "grad_norm": 0.48192185163497925,
      "learning_rate": 5.714421387530272e-06,
      "loss": 0.6195,
      "step": 204750
    },
    {
      "epoch": 2.9161327068204472,
      "grad_norm": 0.5227051377296448,
      "learning_rate": 5.66693575193504e-06,
      "loss": 0.6322,
      "step": 204800
    },
    {
      "epoch": 2.916844653282073,
      "grad_norm": 0.5913362503051758,
      "learning_rate": 5.619450116339808e-06,
      "loss": 0.6228,
      "step": 204850
    },
    {
      "epoch": 2.9175565997436994,
      "grad_norm": 0.5064236521720886,
      "learning_rate": 5.571964480744575e-06,
      "loss": 0.6105,
      "step": 204900
    },
    {
      "epoch": 2.9182685462053253,
      "grad_norm": 0.7611932158470154,
      "learning_rate": 5.5244788451493425e-06,
      "loss": 0.6155,
      "step": 204950
    },
    {
      "epoch": 2.9189804926669516,
      "grad_norm": 0.46929386258125305,
      "learning_rate": 5.47699320955411e-06,
      "loss": 0.6267,
      "step": 205000
    },
    {
      "epoch": 2.9196924391285775,
      "grad_norm": 0.5503391027450562,
      "learning_rate": 5.429507573958878e-06,
      "loss": 0.6147,
      "step": 205050
    },
    {
      "epoch": 2.9204043855902038,
      "grad_norm": 0.5072486996650696,
      "learning_rate": 5.382021938363646e-06,
      "loss": 0.6663,
      "step": 205100
    },
    {
      "epoch": 2.9211163320518296,
      "grad_norm": 0.6821479797363281,
      "learning_rate": 5.334536302768413e-06,
      "loss": 0.5843,
      "step": 205150
    },
    {
      "epoch": 2.921828278513456,
      "grad_norm": 0.7801592946052551,
      "learning_rate": 5.287050667173181e-06,
      "loss": 0.6454,
      "step": 205200
    },
    {
      "epoch": 2.922540224975082,
      "grad_norm": 0.5545981526374817,
      "learning_rate": 5.239565031577948e-06,
      "loss": 0.6614,
      "step": 205250
    },
    {
      "epoch": 2.923252171436708,
      "grad_norm": 0.5871694087982178,
      "learning_rate": 5.1920793959827155e-06,
      "loss": 0.5688,
      "step": 205300
    },
    {
      "epoch": 2.923964117898334,
      "grad_norm": 0.9265743494033813,
      "learning_rate": 5.144593760387483e-06,
      "loss": 0.6459,
      "step": 205350
    },
    {
      "epoch": 2.92467606435996,
      "grad_norm": 0.6376886963844299,
      "learning_rate": 5.09710812479225e-06,
      "loss": 0.6136,
      "step": 205400
    },
    {
      "epoch": 2.925388010821586,
      "grad_norm": 0.4162102937698364,
      "learning_rate": 5.049622489197018e-06,
      "loss": 0.6304,
      "step": 205450
    },
    {
      "epoch": 2.9260999572832125,
      "grad_norm": 0.5286183953285217,
      "learning_rate": 5.002136853601785e-06,
      "loss": 0.6634,
      "step": 205500
    },
    {
      "epoch": 2.9268119037448384,
      "grad_norm": 0.41191989183425903,
      "learning_rate": 4.9546512180065535e-06,
      "loss": 0.6258,
      "step": 205550
    },
    {
      "epoch": 2.927523850206464,
      "grad_norm": 0.5767222046852112,
      "learning_rate": 4.907165582411321e-06,
      "loss": 0.6088,
      "step": 205600
    },
    {
      "epoch": 2.9282357966680905,
      "grad_norm": 0.7868939638137817,
      "learning_rate": 4.8596799468160884e-06,
      "loss": 0.6351,
      "step": 205650
    },
    {
      "epoch": 2.928947743129717,
      "grad_norm": 0.861396849155426,
      "learning_rate": 4.812194311220856e-06,
      "loss": 0.6631,
      "step": 205700
    },
    {
      "epoch": 2.9296596895913427,
      "grad_norm": 0.8575684428215027,
      "learning_rate": 4.764708675625623e-06,
      "loss": 0.7174,
      "step": 205750
    },
    {
      "epoch": 2.9303716360529686,
      "grad_norm": 0.3169953227043152,
      "learning_rate": 4.717223040030392e-06,
      "loss": 0.5999,
      "step": 205800
    },
    {
      "epoch": 2.931083582514595,
      "grad_norm": 0.9896042346954346,
      "learning_rate": 4.669737404435159e-06,
      "loss": 0.6622,
      "step": 205850
    },
    {
      "epoch": 2.931795528976221,
      "grad_norm": 0.710689127445221,
      "learning_rate": 4.6222517688399265e-06,
      "loss": 0.666,
      "step": 205900
    },
    {
      "epoch": 2.932507475437847,
      "grad_norm": 0.6071831583976746,
      "learning_rate": 4.574766133244694e-06,
      "loss": 0.6008,
      "step": 205950
    },
    {
      "epoch": 2.933219421899473,
      "grad_norm": 0.5276618003845215,
      "learning_rate": 4.5272804976494605e-06,
      "loss": 0.6407,
      "step": 206000
    },
    {
      "epoch": 2.9339313683610992,
      "grad_norm": 0.5847288370132446,
      "learning_rate": 4.479794862054229e-06,
      "loss": 0.6278,
      "step": 206050
    },
    {
      "epoch": 2.9346433148227256,
      "grad_norm": 0.3389630615711212,
      "learning_rate": 4.432309226458996e-06,
      "loss": 0.6351,
      "step": 206100
    },
    {
      "epoch": 2.9353552612843514,
      "grad_norm": 0.5712683200836182,
      "learning_rate": 4.384823590863764e-06,
      "loss": 0.6177,
      "step": 206150
    },
    {
      "epoch": 2.9360672077459773,
      "grad_norm": 0.7407124638557434,
      "learning_rate": 4.337337955268531e-06,
      "loss": 0.6911,
      "step": 206200
    },
    {
      "epoch": 2.9367791542076036,
      "grad_norm": 0.7717207670211792,
      "learning_rate": 4.289852319673299e-06,
      "loss": 0.6162,
      "step": 206250
    },
    {
      "epoch": 2.93749110066923,
      "grad_norm": 0.6729637980461121,
      "learning_rate": 4.242366684078067e-06,
      "loss": 0.5803,
      "step": 206300
    },
    {
      "epoch": 2.938203047130856,
      "grad_norm": 0.565156102180481,
      "learning_rate": 4.194881048482834e-06,
      "loss": 0.6157,
      "step": 206350
    },
    {
      "epoch": 2.9389149935924817,
      "grad_norm": 0.5365068316459656,
      "learning_rate": 4.147395412887602e-06,
      "loss": 0.6255,
      "step": 206400
    },
    {
      "epoch": 2.939626940054108,
      "grad_norm": 0.40947413444519043,
      "learning_rate": 4.099909777292369e-06,
      "loss": 0.519,
      "step": 206450
    },
    {
      "epoch": 2.9403388865157343,
      "grad_norm": 0.6920062899589539,
      "learning_rate": 4.053373854409041e-06,
      "loss": 0.6208,
      "step": 206500
    },
    {
      "epoch": 2.94105083297736,
      "grad_norm": 0.3312746584415436,
      "learning_rate": 4.005888218813808e-06,
      "loss": 0.6372,
      "step": 206550
    },
    {
      "epoch": 2.941762779438986,
      "grad_norm": 0.6943464875221252,
      "learning_rate": 3.958402583218577e-06,
      "loss": 0.6711,
      "step": 206600
    },
    {
      "epoch": 2.9424747259006123,
      "grad_norm": 0.445220410823822,
      "learning_rate": 3.910916947623344e-06,
      "loss": 0.6418,
      "step": 206650
    },
    {
      "epoch": 2.9431866723622386,
      "grad_norm": 0.8227643966674805,
      "learning_rate": 3.8634313120281116e-06,
      "loss": 0.6399,
      "step": 206700
    },
    {
      "epoch": 2.9438986188238645,
      "grad_norm": 0.8837552666664124,
      "learning_rate": 3.815945676432879e-06,
      "loss": 0.5766,
      "step": 206750
    },
    {
      "epoch": 2.9446105652854904,
      "grad_norm": 0.9208734035491943,
      "learning_rate": 3.7694097535495516e-06,
      "loss": 0.7157,
      "step": 206800
    },
    {
      "epoch": 2.9453225117471167,
      "grad_norm": 0.6997995376586914,
      "learning_rate": 3.7219241179543186e-06,
      "loss": 0.5752,
      "step": 206850
    },
    {
      "epoch": 2.9460344582087425,
      "grad_norm": 0.7290354371070862,
      "learning_rate": 3.674438482359087e-06,
      "loss": 0.6334,
      "step": 206900
    },
    {
      "epoch": 2.946746404670369,
      "grad_norm": 0.6468088030815125,
      "learning_rate": 3.6269528467638543e-06,
      "loss": 0.6499,
      "step": 206950
    },
    {
      "epoch": 2.9474583511319947,
      "grad_norm": 0.5403168201446533,
      "learning_rate": 3.5794672111686218e-06,
      "loss": 0.6155,
      "step": 207000
    },
    {
      "epoch": 2.948170297593621,
      "grad_norm": 0.4252982437610626,
      "learning_rate": 3.531981575573389e-06,
      "loss": 0.6464,
      "step": 207050
    },
    {
      "epoch": 2.948882244055247,
      "grad_norm": 0.5610276460647583,
      "learning_rate": 3.4844959399781567e-06,
      "loss": 0.6314,
      "step": 207100
    },
    {
      "epoch": 2.949594190516873,
      "grad_norm": 0.9380115866661072,
      "learning_rate": 3.4370103043829245e-06,
      "loss": 0.6283,
      "step": 207150
    },
    {
      "epoch": 2.950306136978499,
      "grad_norm": 0.7378990650177002,
      "learning_rate": 3.389524668787692e-06,
      "loss": 0.6716,
      "step": 207200
    },
    {
      "epoch": 2.9510180834401254,
      "grad_norm": 0.9070291519165039,
      "learning_rate": 3.3420390331924594e-06,
      "loss": 0.5917,
      "step": 207250
    },
    {
      "epoch": 2.9517300299017513,
      "grad_norm": 0.6593654751777649,
      "learning_rate": 3.294553397597227e-06,
      "loss": 0.594,
      "step": 207300
    },
    {
      "epoch": 2.9524419763633776,
      "grad_norm": 0.595261812210083,
      "learning_rate": 3.2470677620019943e-06,
      "loss": 0.6052,
      "step": 207350
    },
    {
      "epoch": 2.9531539228250034,
      "grad_norm": 0.5030296444892883,
      "learning_rate": 3.1995821264067626e-06,
      "loss": 0.5663,
      "step": 207400
    },
    {
      "epoch": 2.9538658692866298,
      "grad_norm": 0.3961820900440216,
      "learning_rate": 3.15209649081153e-06,
      "loss": 0.5867,
      "step": 207450
    },
    {
      "epoch": 2.9545778157482556,
      "grad_norm": 0.5248478055000305,
      "learning_rate": 3.104610855216297e-06,
      "loss": 0.6164,
      "step": 207500
    },
    {
      "epoch": 2.955289762209882,
      "grad_norm": 0.5086911916732788,
      "learning_rate": 3.0571252196210645e-06,
      "loss": 0.6211,
      "step": 207550
    },
    {
      "epoch": 2.956001708671508,
      "grad_norm": 0.38734170794487,
      "learning_rate": 3.0096395840258324e-06,
      "loss": 0.6445,
      "step": 207600
    },
    {
      "epoch": 2.956713655133134,
      "grad_norm": 0.7284554243087769,
      "learning_rate": 2.9621539484306e-06,
      "loss": 0.5991,
      "step": 207650
    },
    {
      "epoch": 2.95742560159476,
      "grad_norm": 0.4486541152000427,
      "learning_rate": 2.9146683128353677e-06,
      "loss": 0.6087,
      "step": 207700
    },
    {
      "epoch": 2.9581375480563863,
      "grad_norm": 0.5910063982009888,
      "learning_rate": 2.867182677240135e-06,
      "loss": 0.5762,
      "step": 207750
    },
    {
      "epoch": 2.958849494518012,
      "grad_norm": 0.6184402704238892,
      "learning_rate": 2.8196970416449026e-06,
      "loss": 0.6618,
      "step": 207800
    },
    {
      "epoch": 2.9595614409796385,
      "grad_norm": 0.5160276889801025,
      "learning_rate": 2.77221140604967e-06,
      "loss": 0.6092,
      "step": 207850
    },
    {
      "epoch": 2.9602733874412643,
      "grad_norm": 0.4840772747993469,
      "learning_rate": 2.7247257704544374e-06,
      "loss": 0.6485,
      "step": 207900
    },
    {
      "epoch": 2.9609853339028906,
      "grad_norm": 0.8014069199562073,
      "learning_rate": 2.6772401348592053e-06,
      "loss": 0.6665,
      "step": 207950
    },
    {
      "epoch": 2.9616972803645165,
      "grad_norm": 0.7193541526794434,
      "learning_rate": 2.6297544992639728e-06,
      "loss": 0.6314,
      "step": 208000
    },
    {
      "epoch": 2.962409226826143,
      "grad_norm": 0.39698925614356995,
      "learning_rate": 2.58226886366874e-06,
      "loss": 0.5784,
      "step": 208050
    },
    {
      "epoch": 2.9631211732877687,
      "grad_norm": 0.27863001823425293,
      "learning_rate": 2.534783228073508e-06,
      "loss": 0.6529,
      "step": 208100
    },
    {
      "epoch": 2.963833119749395,
      "grad_norm": 0.611189603805542,
      "learning_rate": 2.4872975924782755e-06,
      "loss": 0.6677,
      "step": 208150
    },
    {
      "epoch": 2.964545066211021,
      "grad_norm": 0.4885648787021637,
      "learning_rate": 2.439811956883043e-06,
      "loss": 0.5856,
      "step": 208200
    },
    {
      "epoch": 2.965257012672647,
      "grad_norm": 0.43262389302253723,
      "learning_rate": 2.3923263212878104e-06,
      "loss": 0.6604,
      "step": 208250
    },
    {
      "epoch": 2.965968959134273,
      "grad_norm": 0.4979122579097748,
      "learning_rate": 2.344840685692578e-06,
      "loss": 0.6732,
      "step": 208300
    },
    {
      "epoch": 2.966680905595899,
      "grad_norm": 0.48608630895614624,
      "learning_rate": 2.2973550500973457e-06,
      "loss": 0.6003,
      "step": 208350
    },
    {
      "epoch": 2.9673928520575252,
      "grad_norm": 1.0108911991119385,
      "learning_rate": 2.249869414502113e-06,
      "loss": 0.5734,
      "step": 208400
    },
    {
      "epoch": 2.9681047985191515,
      "grad_norm": 0.503483772277832,
      "learning_rate": 2.202383778906881e-06,
      "loss": 0.6237,
      "step": 208450
    },
    {
      "epoch": 2.9688167449807774,
      "grad_norm": 0.8252903819084167,
      "learning_rate": 2.1548981433116485e-06,
      "loss": 0.6414,
      "step": 208500
    },
    {
      "epoch": 2.9695286914424033,
      "grad_norm": 0.48607105016708374,
      "learning_rate": 2.107412507716416e-06,
      "loss": 0.6267,
      "step": 208550
    },
    {
      "epoch": 2.9702406379040296,
      "grad_norm": 0.6026849150657654,
      "learning_rate": 2.0599268721211833e-06,
      "loss": 0.6606,
      "step": 208600
    },
    {
      "epoch": 2.970952584365656,
      "grad_norm": 0.43243861198425293,
      "learning_rate": 2.0124412365259508e-06,
      "loss": 0.6184,
      "step": 208650
    },
    {
      "epoch": 2.9716645308272818,
      "grad_norm": 0.5711882710456848,
      "learning_rate": 1.9649556009307187e-06,
      "loss": 0.6375,
      "step": 208700
    },
    {
      "epoch": 2.9723764772889076,
      "grad_norm": 0.556114912033081,
      "learning_rate": 1.917469965335486e-06,
      "loss": 0.6377,
      "step": 208750
    },
    {
      "epoch": 2.973088423750534,
      "grad_norm": 0.668244481086731,
      "learning_rate": 1.8699843297402538e-06,
      "loss": 0.5892,
      "step": 208800
    },
    {
      "epoch": 2.9738003702121603,
      "grad_norm": 0.4081600606441498,
      "learning_rate": 1.8224986941450212e-06,
      "loss": 0.5365,
      "step": 208850
    },
    {
      "epoch": 2.974512316673786,
      "grad_norm": 0.6062107682228088,
      "learning_rate": 1.7750130585497886e-06,
      "loss": 0.6447,
      "step": 208900
    },
    {
      "epoch": 2.975224263135412,
      "grad_norm": 0.5832828879356384,
      "learning_rate": 1.7275274229545565e-06,
      "loss": 0.6373,
      "step": 208950
    },
    {
      "epoch": 2.9759362095970383,
      "grad_norm": 0.9180141091346741,
      "learning_rate": 1.6800417873593237e-06,
      "loss": 0.6877,
      "step": 209000
    },
    {
      "epoch": 2.9766481560586646,
      "grad_norm": 1.1057673692703247,
      "learning_rate": 1.6325561517640916e-06,
      "loss": 0.6811,
      "step": 209050
    },
    {
      "epoch": 2.9773601025202905,
      "grad_norm": 0.5356001853942871,
      "learning_rate": 1.585070516168859e-06,
      "loss": 0.5912,
      "step": 209100
    },
    {
      "epoch": 2.9780720489819164,
      "grad_norm": 0.45290282368659973,
      "learning_rate": 1.5375848805736265e-06,
      "loss": 0.6119,
      "step": 209150
    },
    {
      "epoch": 2.9787839954435427,
      "grad_norm": 0.39237576723098755,
      "learning_rate": 1.4900992449783941e-06,
      "loss": 0.6641,
      "step": 209200
    },
    {
      "epoch": 2.979495941905169,
      "grad_norm": 0.5667783617973328,
      "learning_rate": 1.4426136093831616e-06,
      "loss": 0.6877,
      "step": 209250
    },
    {
      "epoch": 2.980207888366795,
      "grad_norm": 0.5659281611442566,
      "learning_rate": 1.3951279737879292e-06,
      "loss": 0.6018,
      "step": 209300
    },
    {
      "epoch": 2.9809198348284207,
      "grad_norm": 0.6350353360176086,
      "learning_rate": 1.3476423381926967e-06,
      "loss": 0.5567,
      "step": 209350
    },
    {
      "epoch": 2.981631781290047,
      "grad_norm": 0.9301673769950867,
      "learning_rate": 1.3001567025974643e-06,
      "loss": 0.5953,
      "step": 209400
    },
    {
      "epoch": 2.9823437277516733,
      "grad_norm": 0.6814505457878113,
      "learning_rate": 1.252671067002232e-06,
      "loss": 0.5906,
      "step": 209450
    },
    {
      "epoch": 2.983055674213299,
      "grad_norm": 0.48279494047164917,
      "learning_rate": 1.2051854314069994e-06,
      "loss": 0.6469,
      "step": 209500
    },
    {
      "epoch": 2.983767620674925,
      "grad_norm": 0.7476565837860107,
      "learning_rate": 1.1576997958117669e-06,
      "loss": 0.63,
      "step": 209550
    },
    {
      "epoch": 2.9844795671365514,
      "grad_norm": 0.5689148306846619,
      "learning_rate": 1.1102141602165345e-06,
      "loss": 0.6302,
      "step": 209600
    },
    {
      "epoch": 2.9851915135981777,
      "grad_norm": 0.7648857831954956,
      "learning_rate": 1.0627285246213022e-06,
      "loss": 0.6439,
      "step": 209650
    },
    {
      "epoch": 2.9859034600598036,
      "grad_norm": 0.47866320610046387,
      "learning_rate": 1.0152428890260696e-06,
      "loss": 0.625,
      "step": 209700
    },
    {
      "epoch": 2.9866154065214294,
      "grad_norm": 0.7881286144256592,
      "learning_rate": 9.677572534308373e-07,
      "loss": 0.6147,
      "step": 209750
    },
    {
      "epoch": 2.9873273529830557,
      "grad_norm": 0.6520687341690063,
      "learning_rate": 9.202716178356047e-07,
      "loss": 0.6624,
      "step": 209800
    },
    {
      "epoch": 2.9880392994446816,
      "grad_norm": 0.5570541024208069,
      "learning_rate": 8.727859822403723e-07,
      "loss": 0.671,
      "step": 209850
    },
    {
      "epoch": 2.988751245906308,
      "grad_norm": 1.212803602218628,
      "learning_rate": 8.253003466451398e-07,
      "loss": 0.6212,
      "step": 209900
    },
    {
      "epoch": 2.989463192367934,
      "grad_norm": 0.6282405853271484,
      "learning_rate": 7.778147110499075e-07,
      "loss": 0.6358,
      "step": 209950
    },
    {
      "epoch": 2.99017513882956,
      "grad_norm": 0.4421674609184265,
      "learning_rate": 7.303290754546749e-07,
      "loss": 0.6557,
      "step": 210000
    },
    {
      "epoch": 2.990887085291186,
      "grad_norm": 0.6374204754829407,
      "learning_rate": 6.828434398594426e-07,
      "loss": 0.6322,
      "step": 210050
    },
    {
      "epoch": 2.9915990317528123,
      "grad_norm": 0.5945158004760742,
      "learning_rate": 6.353578042642101e-07,
      "loss": 0.6524,
      "step": 210100
    },
    {
      "epoch": 2.992310978214438,
      "grad_norm": 0.6940098404884338,
      "learning_rate": 5.878721686689777e-07,
      "loss": 0.6144,
      "step": 210150
    },
    {
      "epoch": 2.9930229246760645,
      "grad_norm": 0.5838534235954285,
      "learning_rate": 5.403865330737452e-07,
      "loss": 0.6071,
      "step": 210200
    },
    {
      "epoch": 2.9937348711376903,
      "grad_norm": 0.6481245160102844,
      "learning_rate": 4.929008974785128e-07,
      "loss": 0.7021,
      "step": 210250
    },
    {
      "epoch": 2.9944468175993166,
      "grad_norm": 0.5606469511985779,
      "learning_rate": 4.4541526188328033e-07,
      "loss": 0.5947,
      "step": 210300
    },
    {
      "epoch": 2.9951587640609425,
      "grad_norm": 0.6148092150688171,
      "learning_rate": 3.979296262880479e-07,
      "loss": 0.6102,
      "step": 210350
    },
    {
      "epoch": 2.995870710522569,
      "grad_norm": 0.6346043944358826,
      "learning_rate": 3.5044399069281543e-07,
      "loss": 0.6351,
      "step": 210400
    },
    {
      "epoch": 2.9965826569841947,
      "grad_norm": 0.5946089625358582,
      "learning_rate": 3.02958355097583e-07,
      "loss": 0.6558,
      "step": 210450
    },
    {
      "epoch": 2.997294603445821,
      "grad_norm": 0.711331844329834,
      "learning_rate": 2.5547271950235053e-07,
      "loss": 0.5915,
      "step": 210500
    }
  ],
  "logging_steps": 50,
  "max_steps": 210690,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.99791334391808e+18,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
